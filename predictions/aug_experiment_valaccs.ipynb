{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fveKC1NzY8lL"
   },
   "source": [
    "# Set Up\n",
    "\n",
    "Note: af_dir and disk_data_dir are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/efficientnet\n",
      "  Cloning https://github.com/qubvel/efficientnet to /tmp/pip-req-build-v1cudxij\n",
      "  Running command git clone -q https://github.com/qubvel/efficientnet /tmp/pip-req-build-v1cudxij\n",
      "Requirement already satisfied (use --upgrade to upgrade): efficientnet==1.1.0 from git+https://github.com/qubvel/efficientnet in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.19.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (2.10.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.5.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (3.2.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.4)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (7.1.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.8.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2020.6.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.0) (4.4.2)\n",
      "Building wheels for collected packages: efficientnet\n",
      "  Building wheel for efficientnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet: filename=efficientnet-1.1.0-py3-none-any.whl size=18397 sha256=3f4686dc6c66908e8ad80716bc6279717580da3016c288e28a6d0fb4f3b8abb6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sv83pi4m/wheels/11/69/85/814d64d694c96db0eef17b718042d644a1e54f113920481920\n",
      "Successfully built efficientnet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math # required for prediction conversion\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "af_dir = '../../all_faces_bucket/'\n",
    "disk_data_dir = '../../forensics_split/'\n",
    "!pip install git+https://github.com/qubvel/efficientnet\n",
    "import efficientnet.tfkeras\n",
    "# !sudo kill -9 PID # clear GPU memory where 9 is PID number\n",
    "# !watch -n0.1 nvidia-smi\n",
    "\n",
    "model_to_load = 'config8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "**Note: Save time by only running the cells with functions but leaving the others -- they are there for you to see the intermediate steps.**\n",
    "\n",
    "## Predict Labels for Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrsAPTF9Y8m_"
   },
   "outputs": [],
   "source": [
    "# Need to load model to CPU if GPU is busy training\n",
    "# with tf.device('/cpu:0'):\n",
    "#     loaded_model = load_model(af_dir + 'trained_models/saved_models/' + architecture + '_model.h5')\n",
    "\n",
    "def get_model(model):\n",
    "    '''Loads one of the saved models based on specified architecture\n",
    "    (vgg, resnet50, mobilenet or xception)'''\n",
    "    \n",
    "    return load_model(af_dir + 'trained_models/saved_models/' + model + '.h5')\n",
    "\n",
    "def get_multidim_predictions(model):\n",
    "    '''Takes in a loaded model and outputs filenames and multi-dimensional\n",
    "    predictions for each class.\n",
    "    \n",
    "    Works by initiating an instance of ImageDataGenerator which is used for\n",
    "    flow_from_directory method.'''\n",
    "    # normalise and centre test data\n",
    "    datagen = ImageDataGenerator(samplewise_std_normalization=True, samplewise_center=True)\n",
    "    generator = datagen.flow_from_directory(disk_data_dir + 'validation', target_size=(224, 224),\n",
    "                                            shuffle = False, batch_size=1)\n",
    "    filenames = generator.filenames\n",
    "    nb_samples = len(filenames)\n",
    "    generator.reset() # figure out this \n",
    "    predictions = model.predict(generator, steps = nb_samples, verbose=1, workers=8)\n",
    "    \n",
    "    return filenames, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywxwKFvuY8nc"
   },
   "outputs": [],
   "source": [
    "def get_image_predictions(arr, soft=True):\n",
    "    '''Obtains image predictions.\n",
    "    soft: a true value returns probabilities as opposed to hard predictions.'''\n",
    "\n",
    "    if soft:\n",
    "        # probability of belonging to fake (second) class,\n",
    "        # hence return second value for each element in the list\n",
    "        return [el[1] for el in arr]\n",
    "    # returns a list of 0's and 1's\n",
    "    return np.argmax(arr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Information Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(filenames):\n",
    "    index = range(len(filenames))\n",
    "    df = pd.DataFrame(index = index, columns = ['method', 'video', 'image', 'test/train', 'true label',\n",
    "                                                  'probability', 'predicted label', 'acc'])\n",
    "    df = df.fillna(0)\n",
    "    methods = [el[el.find('/')+1: el.find('_')] for el in filenames]\n",
    "    video_numbers = [el[el.find('_')+1: el.rfind('_')] for el in filenames]\n",
    "    # video_numbers = [re.search(\"_(.*?)\\_\", el).group(1) for el in filenames] # older version -- does not include second video name for fake videos\n",
    "    image_numbers = [el[el.find('_')+1: el.find('.')][4:] for el in filenames]\n",
    "    true_labels = [0 if el[0] == 'a' else 1 for el in filenames]\n",
    "    \n",
    "    df['method'] = methods\n",
    "    df['video'] =  video_numbers\n",
    "    df['image'] =  image_numbers\n",
    "    df['true label'] = true_labels\n",
    "    df['test/train'] = ['test']*len(filenames)\n",
    "    df['probability'] = predictions\n",
    "    df['predicted label'] = ['-']*len(filenames)\n",
    "    df['acc'] = ['-']*len(filenames)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to Video Predictions\n",
    "\n",
    "Next cell contains three functions which are all options for combining image predictions and converting those into a single video prediction. Fraction method considers only a fraction of higest image predictions (useful when only a fraction of video has been manipulated), ConfidentStrategy takes a mean of a subset of predictions depending on popularity of each of the two classes, and Transform makes each prediction more extreme (pushed towards 0.0 or 1.0 depending on its original value) before taking a mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_of_a_fraction(lst, fraction = 1.0):\n",
    "    '''Takes in a list and outputs a mean of the fraction of the largest\n",
    "    elements for that list (by default (fraction is 1) == consider all predictions)\n",
    "        \n",
    "    if fraction equals to 1, the output is simply a mean.\n",
    "        \n",
    "    This mimics considering only the top third highest probabilities for\n",
    "    images for a given video to classify that video. The main idea is\n",
    "    that if a given video has only a fraction of it being manipulated\n",
    "    (unknown information) then it's likely to be wrongly classfied as original\n",
    "    if we average all associated probabilities, however, if we take only a\n",
    "    certain number of highest proabilities that will be much more representative\n",
    "    and overall robust.'''\n",
    "        \n",
    "    sorted_lst = sorted(lst)[::-1] if type(lst) == list else [lst]\n",
    "    sliced_lst = sorted_lst[0:math.ceil(len(lst)*fraction)] if fraction != (1 or 1.0) else sorted_lst\n",
    "    return np.mean(sliced_lst)\n",
    "\n",
    "def get_mean_with_confident_strategy(lst, fraction = 0.75, t = 0.5):\n",
    "    '''Confident strategy is implemented from first-place solution in DFDC.\n",
    "    \n",
    "    The main idea is that if there are a lot of predictions for one class,\n",
    "    then the average is taken of those predictions only. If that's not the\n",
    "    case, then a simple mean is outputted.\n",
    "    \n",
    "    Inputs:\n",
    "    1. list of predictions (converted to a list if it's a single prediction)\n",
    "    2. fraction -- (between 0 and 1) what fraction of the list should predict\n",
    "    the same class for other predictions to be disregarded when taking a mean\n",
    "    3. t -- threshold cutoff value between two classes (note: whole notebook\n",
    "    is structured for a binary classification problem only)'''\n",
    "    \n",
    "    lst = np.array(lst)\n",
    "    num_pred = len(lst)\n",
    "    num_fakes = np.count_nonzero(lst >= t)\n",
    "    num_authentic = num_pred - num_fakes\n",
    "\n",
    "    # if number of fakes is more that 75% of all predictions \n",
    "    if num_fakes > int(num_pred * fraction):\n",
    "        # take predictions which are greater than threshold and average them\n",
    "        return np.mean(lst[lst >= t])\n",
    "\n",
    "    # else if number of predictions below threshold value t is more that 75%\n",
    "    # of all predictions\n",
    "    elif num_authentic > int(num_pred * fraction):\n",
    "        # take these predictions and return their mean\n",
    "        return np.mean(lst[lst < t])\n",
    "  \n",
    "    else: # simple mean\n",
    "        return np.mean(lst)\n",
    "    \n",
    "def get_mean_of_transformed_predictions(lst):\n",
    "    '''Takes a list of predictions, transforms them by individually\n",
    "    pushing the values away from the centre (0.5) closer towards the\n",
    "    extremes (0.0 and 1.0). The visualisation is included below.\n",
    "    \n",
    "    Returns a mean of transformed predictions.'''\n",
    "\n",
    "    if type(lst) != list: lst = [lst]\n",
    "    weights = np.power([abs(el -0.5) for el in lst], 1.0) + 1e-4\n",
    "    return float((lst * weights).sum() / weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(prob, t = 0.15):\n",
    "    if prob == 0.5:\n",
    "        return 0.5\n",
    "    elif prob <= t:\n",
    "        return 0.0\n",
    "    elif prob >= (1-t):\n",
    "        return 1.0\n",
    "    elif prob > t and prob < 0.5:\n",
    "        return 0.5 - np.power(abs(prob-0.5), 0.65)\n",
    "    elif prob < (1-t) and prob > 0.5:\n",
    "        return 0.5 + np.power(abs(prob-0.5), 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(df, threshold = 0.5, option = None, fraction = 0.33):\n",
    "    ''' Takes in a dataframe, regroups by videos (collecting all predictions\n",
    "    for a video in one nested list) then (optionally modifies by one of the three\n",
    "    methods) returning a mean prediction for each video. Lastly, an accuracy\n",
    "    column is filled by comparing a true label with the predicted one (a mean\n",
    "    probability is convered into a label subject to threshold value).\n",
    "    \n",
    "    Inputs:\n",
    "    1. df -- dataframe\n",
    "    2. threshold -- cutoff probability value between classes (by default 0.5)\n",
    "    3. option -- (by default None) choices are 'transform', 'confident strategy'\n",
    "    or 'fraction'; correspond to possible list manipulations\n",
    "    \n",
    "    Note: if you feed option = 'fraction', then you need to also specify fraction\n",
    "    value (1.0 means a simple mean, 0.33 means taking top third, et cetera)\n",
    "    \n",
    "    if option is not speficied or not among choices then a simple mean is calculated\n",
    "    4. fraction (= 0.33) -- value for 'fraction' option '''\n",
    "\n",
    "    # regroup based on method, video title, and test/train category\n",
    "    df = df.groupby(['method', 'video','test/train', 'true label', 'predicted label'])\\\n",
    "                    ['probability'].apply(list).reset_index()\n",
    "    \n",
    "    collected_labels_pred = list(df['probability']) # get the nested list\n",
    "    \n",
    "    # next, we apply one of the three methods to get means\n",
    "    \n",
    "    if option == 'transform':\n",
    "        mean_labels_pred = [get_mean_of_transformed_predictions(el) for el in collected_labels_pred]\n",
    "        \n",
    "    elif option == 'confident strategy':\n",
    "        mean_labels_pred = [get_mean_with_confident_strategy(el) for el in collected_labels_pred]\n",
    "    \n",
    "    elif option == 'fraction':\n",
    "        mean_labels_pred = [get_mean_of_a_fraction(el, fraction) for el in collected_labels_pred]\n",
    "        \n",
    "    else: # if no option is chosen (or not from the list), output a simple mean per video\n",
    "        mean_labels_pred = [np.mean(el) for el in collected_labels_pred]\n",
    "\n",
    "    labels_pred = [0 if el <= threshold else 1 for el in mean_labels_pred]\n",
    "    df['predicted label'] = labels_pred\n",
    "\n",
    "    # produce accuraacy values for each video (0 if classification is wrong and\n",
    "    # 1 if classicification is correct)\n",
    "    df['acc'] = [1 if df['true label'][i] == df['predicted label'][i]\n",
    "                            else 0 for i in range(len(df['true label']))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(df):\n",
    "    acc_per_method = df.groupby(['test/train', 'method'])['acc'].mean()\n",
    "    acc_total = df.groupby(['test/train'])['acc'].mean()\n",
    "    #display(acc_per_method)\n",
    "    #display(acc_total)\n",
    "    return acc_per_method, acc_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridMask\n",
      "Found 21291 images belonging to 2 classes.\n",
      "21291/21291 [==============================] - 129s 6ms/step\n",
      "transform: 0.991429\n",
      "confident strategy: 0.991429\n",
      "fraction: 0.991429\n",
      "None: 0.991429\n",
      "\n",
      "\n",
      "GridMask-Alternative\n",
      "Found 21291 images belonging to 2 classes.\n",
      "21291/21291 [==============================] - 130s 6ms/step\n",
      "transform: 0.992857\n",
      "confident strategy: 0.992857\n",
      "fraction: 0.992857\n",
      "None: 0.992857\n",
      "\n",
      "\n",
      "FaceMask\n",
      "Found 21291 images belonging to 2 classes.\n",
      "21291/21291 [==============================] - 134s 6ms/step\n",
      "transform: 0.992857\n",
      "confident strategy: 0.992857\n",
      "fraction: 0.990000\n",
      "None: 0.992857\n",
      "\n",
      "\n",
      "FacialArtifacts\n",
      "Found 21291 images belonging to 2 classes.\n",
      "21291/21291 [==============================] - 133s 6ms/step\n",
      "transform: 0.991429\n",
      "confident strategy: 0.991429\n",
      "fraction: 0.990000\n",
      "None: 0.991429\n",
      "\n",
      "\n",
      "FacialArtifacts-Alternative\n",
      "Found 21291 images belonging to 2 classes.\n",
      "21291/21291 [==============================] - 134s 6ms/step\n",
      "transform: 0.992857\n",
      "confident strategy: 0.992857\n",
      "fraction: 0.990000\n",
      "None: 0.992857\n",
      "\n",
      "\n",
      "No information-dropping augmentation\n",
      "Found 21291 images belonging to 2 classes.\n",
      "21291/21291 [==============================] - 131s 6ms/step\n",
      "transform: 0.992857\n",
      "confident strategy: 0.992857\n",
      "fraction: 0.991429\n",
      "None: 0.992857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_config_nums = [2025, 2026, 2027, 2028, 2029, 2030]\n",
    "\n",
    "for num in all_config_nums:\n",
    "    \n",
    "    if num == 2025: aug = 'GridMask'\n",
    "    elif num == 2026: aug = 'GridMask-Alternative'\n",
    "    elif num == 2027: aug = 'FaceMask'\n",
    "    elif num == 2028: aug = 'FacialArtifacts'\n",
    "    elif num == 2029: aug = 'FacialArtifacts-Alternative'\n",
    "    elif num == 2030: aug = 'No information-dropping augmentation'\n",
    "    print(aug)\n",
    "    loaded_model = get_model('config' + str(num))\n",
    "    filenames, multidim_predictions = get_multidim_predictions(loaded_model)\n",
    "    predictions = get_image_predictions(multidim_predictions, soft=True)\n",
    "    data = build_dataframe(filenames)\n",
    "    #print('\\n')\n",
    "    for option in ['transform', 'confident strategy', 'fraction', 'None']:\n",
    "        #print('#'*38)\n",
    "        new = convert_predictions(data, option = option)\n",
    "        acc_per_method, acc_total = show_accuracy(new)\n",
    "        print(option + ':', \"%.6f\" % acc_total['test'])\n",
    "        #print('#'*38)\n",
    "        #if option != 'None': print('\\n')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "prediction_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
