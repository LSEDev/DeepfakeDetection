{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Model: \"simple_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 603,778\n",
      "Trainable params: 603,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (300)\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "print('hello')\n",
    "\n",
    "t_dense1 = Dense(1024, activation='relu')(inputs)\n",
    "t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
    "t_do = Dropout(0.3)(t_dense3)\n",
    "predictions = Dense(2, activation= 'softmax')(t_do)\n",
    "model = Model(inputs=inputs, outputs=predictions, name = 'simple_model')\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate= 0.002, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import radialProfile\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    data_aug_train = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            fill_mode='nearest',\n",
    "            brightness_range=None,\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "       #     preprocessing_function=deepaug.joint_function,\n",
    "            data_format=None,)\n",
    "\n",
    "    data_aug_val = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "            \n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = data_aug_train.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    val_data = data_aug_val.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "def featurevectorise(img):\n",
    "    #img = color.rgb2gray(img)\n",
    "    img = rgb2gray(img)\n",
    "    h = int(img.shape[1]/3)\n",
    "    w = int(img.shape[2]/3)\n",
    "    img = img[0,h:-h,w:-w]\n",
    "\n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    fshift += epsilon\n",
    "\n",
    "\n",
    "    magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "    # Calculate the azimuthally averaged 1D power spectrum\n",
    "    psd1D = radialProfile.azimuthalAverage(magnitude_spectrum)\n",
    "    points = np.linspace(0,N,num=psd1D.size) # coordinates of a\n",
    "    xi = np.linspace(0,N,num=N) # coordinates for interpolation\n",
    "\n",
    "    interpolated = griddata(points,psd1D,xi,method='cubic')\n",
    "    interpolated /= interpolated[0]\n",
    "    \n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113928 images belonging to 2 classes.\n",
      "Found 21291 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "train_data, val_data = augment_data('../ff-alldata/home/jupyter/forensics_split', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 102.12793135643005 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "data= {}\n",
    "epsilon = 1e-8\n",
    "N = 300\n",
    "y = []\n",
    "error = []\n",
    "\n",
    "\n",
    "number_iter = 5000\n",
    "val_length = 500\n",
    "\n",
    "psd1D_total = np.zeros([number_iter, N])\n",
    "label_total = np.zeros([number_iter, 2])\n",
    "\n",
    "X_test = np.zeros([val_length, N])\n",
    "Y_test = np.zeros([val_length, 2])\n",
    "cont = 0\n",
    "\n",
    "for image in train_data:\n",
    "    psd1D_total[cont,:] = featurevectorise(image[0])             \n",
    "    label_total[cont,:] = image[1]\n",
    "    cont+=1\n",
    "    if cont == number_iter:\n",
    "        break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "(500, 300)\n",
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "cont2 = 0\n",
    "for image in val_data:\n",
    "    X_test[cont2,:] = featurevectorise(image[0])             \n",
    "    Y_test[cont2,:] = image[1]\n",
    "    cont2+=1\n",
    "    if cont2 == val_length:\n",
    "        break\n",
    "print(psd1D_total.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure class weights function corresponds to these class indices: {'authentic': 0, 'fake': 1}\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights\n",
    "class_weights = calculate_class_weights(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = psd1D_total\n",
    "Y_train = label_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 0s 73us/sample - loss: 0.9449 - accuracy: 0.6276 - val_loss: 1.2035 - val_accuracy: 0.6880\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 0s 81us/sample - loss: 0.9150 - accuracy: 0.6284 - val_loss: 1.5145 - val_accuracy: 0.6500\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.9377 - accuracy: 0.6042 - val_loss: 1.5116 - val_accuracy: 0.6100\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.9272 - accuracy: 0.6162 - val_loss: 1.2476 - val_accuracy: 0.6880\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 0s 83us/sample - loss: 0.9178 - accuracy: 0.6324 - val_loss: 1.5645 - val_accuracy: 0.6500\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.9338 - accuracy: 0.6010 - val_loss: 1.4277 - val_accuracy: 0.6680\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 0s 76us/sample - loss: 0.9362 - accuracy: 0.6132 - val_loss: 1.2864 - val_accuracy: 0.6500\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 0s 75us/sample - loss: 0.9286 - accuracy: 0.6224 - val_loss: 1.4330 - val_accuracy: 0.6720\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 0s 76us/sample - loss: 0.9278 - accuracy: 0.6198 - val_loss: 1.3372 - val_accuracy: 0.6840\n",
      "Epoch 10/50\n",
      "5000/5000 [==============================] - 0s 70us/sample - loss: 0.9205 - accuracy: 0.6116 - val_loss: 1.5239 - val_accuracy: 0.6540\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 0s 64us/sample - loss: 0.9290 - accuracy: 0.6108 - val_loss: 1.3671 - val_accuracy: 0.6760\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 0s 72us/sample - loss: 0.9104 - accuracy: 0.6404 - val_loss: 1.3412 - val_accuracy: 0.6540\n",
      "Epoch 13/50\n",
      "5000/5000 [==============================] - 0s 81us/sample - loss: 0.9173 - accuracy: 0.6306 - val_loss: 1.1073 - val_accuracy: 0.6480\n",
      "Epoch 14/50\n",
      "5000/5000 [==============================] - 0s 86us/sample - loss: 0.9432 - accuracy: 0.6006 - val_loss: 1.3961 - val_accuracy: 0.7020\n",
      "Epoch 15/50\n",
      "5000/5000 [==============================] - 0s 75us/sample - loss: 0.9283 - accuracy: 0.6058 - val_loss: 1.4036 - val_accuracy: 0.6160\n",
      "Epoch 16/50\n",
      "5000/5000 [==============================] - 0s 78us/sample - loss: 0.9182 - accuracy: 0.6208 - val_loss: 1.5204 - val_accuracy: 0.6940\n",
      "Epoch 17/50\n",
      "5000/5000 [==============================] - 0s 72us/sample - loss: 0.9145 - accuracy: 0.6334 - val_loss: 1.4866 - val_accuracy: 0.6160\n",
      "Epoch 18/50\n",
      "5000/5000 [==============================] - 0s 79us/sample - loss: 0.9083 - accuracy: 0.6402 - val_loss: 1.4621 - val_accuracy: 0.6460\n",
      "Epoch 19/50\n",
      "5000/5000 [==============================] - 0s 82us/sample - loss: 0.9197 - accuracy: 0.6354 - val_loss: 1.4749 - val_accuracy: 0.6260\n",
      "Epoch 20/50\n",
      "5000/5000 [==============================] - 0s 68us/sample - loss: 0.9214 - accuracy: 0.6298 - val_loss: 1.5182 - val_accuracy: 0.5960\n",
      "Epoch 21/50\n",
      "5000/5000 [==============================] - 0s 67us/sample - loss: 0.9225 - accuracy: 0.6404 - val_loss: 1.1785 - val_accuracy: 0.6340\n",
      "Epoch 22/50\n",
      "5000/5000 [==============================] - 0s 63us/sample - loss: 0.9224 - accuracy: 0.6102 - val_loss: 1.2420 - val_accuracy: 0.6680\n",
      "Epoch 23/50\n",
      "5000/5000 [==============================] - 0s 68us/sample - loss: 0.9294 - accuracy: 0.6012 - val_loss: 1.0800 - val_accuracy: 0.6640\n",
      "Epoch 24/50\n",
      "5000/5000 [==============================] - 0s 78us/sample - loss: 0.9265 - accuracy: 0.6160 - val_loss: 1.1474 - val_accuracy: 0.7080\n",
      "Epoch 25/50\n",
      "5000/5000 [==============================] - 0s 83us/sample - loss: 0.9136 - accuracy: 0.6322 - val_loss: 1.4533 - val_accuracy: 0.6760\n",
      "Epoch 26/50\n",
      "5000/5000 [==============================] - 0s 93us/sample - loss: 0.9132 - accuracy: 0.6404 - val_loss: 1.2455 - val_accuracy: 0.6740\n",
      "Epoch 27/50\n",
      "5000/5000 [==============================] - 0s 79us/sample - loss: 0.9091 - accuracy: 0.6318 - val_loss: 1.1606 - val_accuracy: 0.6560\n",
      "Epoch 28/50\n",
      "5000/5000 [==============================] - 0s 75us/sample - loss: 0.9025 - accuracy: 0.6338 - val_loss: 1.6095 - val_accuracy: 0.6840\n",
      "Epoch 29/50\n",
      "5000/5000 [==============================] - 0s 90us/sample - loss: 0.9049 - accuracy: 0.6372 - val_loss: 1.4970 - val_accuracy: 0.6980\n",
      "Epoch 30/50\n",
      "5000/5000 [==============================] - 0s 72us/sample - loss: 0.9091 - accuracy: 0.6366 - val_loss: 1.3197 - val_accuracy: 0.7200\n",
      "Epoch 31/50\n",
      "5000/5000 [==============================] - 0s 71us/sample - loss: 0.9048 - accuracy: 0.6270 - val_loss: 1.2587 - val_accuracy: 0.6800\n",
      "Epoch 32/50\n",
      "5000/5000 [==============================] - 0s 77us/sample - loss: 0.9130 - accuracy: 0.6470 - val_loss: 1.4181 - val_accuracy: 0.6660\n",
      "Epoch 33/50\n",
      "5000/5000 [==============================] - 0s 73us/sample - loss: 0.9041 - accuracy: 0.6356 - val_loss: 1.4888 - val_accuracy: 0.7000\n",
      "Epoch 34/50\n",
      "5000/5000 [==============================] - 0s 76us/sample - loss: 0.9123 - accuracy: 0.6210 - val_loss: 1.1244 - val_accuracy: 0.6980\n",
      "Epoch 35/50\n",
      "5000/5000 [==============================] - 0s 76us/sample - loss: 0.9204 - accuracy: 0.6138 - val_loss: 1.2527 - val_accuracy: 0.6500\n",
      "Epoch 36/50\n",
      "5000/5000 [==============================] - 0s 79us/sample - loss: 0.9132 - accuracy: 0.6252 - val_loss: 1.5403 - val_accuracy: 0.6900\n",
      "Epoch 37/50\n",
      "5000/5000 [==============================] - 0s 70us/sample - loss: 0.9174 - accuracy: 0.6166 - val_loss: 1.6396 - val_accuracy: 0.7120\n",
      "Epoch 38/50\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.9173 - accuracy: 0.6420 - val_loss: 1.3613 - val_accuracy: 0.6920\n",
      "Epoch 39/50\n",
      "5000/5000 [==============================] - 0s 91us/sample - loss: 0.9050 - accuracy: 0.6344 - val_loss: 1.5783 - val_accuracy: 0.6640\n",
      "Epoch 40/50\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.9118 - accuracy: 0.6100 - val_loss: 1.7564 - val_accuracy: 0.6940\n",
      "Epoch 41/50\n",
      "5000/5000 [==============================] - 0s 69us/sample - loss: 0.9072 - accuracy: 0.6464 - val_loss: 1.4145 - val_accuracy: 0.6880\n",
      "Epoch 42/50\n",
      "5000/5000 [==============================] - 0s 87us/sample - loss: 0.9089 - accuracy: 0.6382 - val_loss: 1.7229 - val_accuracy: 0.6900\n",
      "Epoch 43/50\n",
      "5000/5000 [==============================] - 0s 75us/sample - loss: 0.8958 - accuracy: 0.6436 - val_loss: 1.4227 - val_accuracy: 0.6700\n",
      "Epoch 44/50\n",
      "5000/5000 [==============================] - 0s 72us/sample - loss: 0.9233 - accuracy: 0.6212 - val_loss: 1.4024 - val_accuracy: 0.6460\n",
      "Epoch 45/50\n",
      "5000/5000 [==============================] - 0s 75us/sample - loss: 0.9164 - accuracy: 0.6140 - val_loss: 1.4717 - val_accuracy: 0.6780\n",
      "Epoch 46/50\n",
      "5000/5000 [==============================] - 0s 85us/sample - loss: 0.9078 - accuracy: 0.6316 - val_loss: 1.3832 - val_accuracy: 0.6560\n",
      "Epoch 47/50\n",
      "5000/5000 [==============================] - 0s 66us/sample - loss: 0.9029 - accuracy: 0.6350 - val_loss: 1.7107 - val_accuracy: 0.6900\n",
      "Epoch 48/50\n",
      "5000/5000 [==============================] - 0s 64us/sample - loss: 0.9261 - accuracy: 0.6320 - val_loss: 1.5867 - val_accuracy: 0.6940\n",
      "Epoch 49/50\n",
      "5000/5000 [==============================] - 0s 68us/sample - loss: 0.9133 - accuracy: 0.6320 - val_loss: 1.3803 - val_accuracy: 0.6500\n",
      "Epoch 50/50\n",
      "5000/5000 [==============================] - 0s 66us/sample - loss: 0.9106 - accuracy: 0.6258 - val_loss: 1.3307 - val_accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f28e06401d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "#X_train = psd1D_total\n",
    "Y_train = label_total\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          class_weight = class_weights,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
