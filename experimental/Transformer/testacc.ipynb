{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import TransformCode as tc\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "#!pip install git+https://github.com/qubvel/efficientnet\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'transformer',\n",
       " 'epochs': 50,\n",
       " 'batch_size': 4,\n",
       " 'learning_rate_type': 'constant',\n",
       " 'learning_rate': 0.0001,\n",
       " 'patience': 10,\n",
       " 'weight_initialisation': 'imagenet',\n",
       " 'optimiser': 'adam',\n",
       " 'momentum': 0.99,\n",
       " 'nesterov': 'False',\n",
       " 'label_smoothing': 0.01,\n",
       " 'dropout': 0.5,\n",
       " 'target_size': 224,\n",
       " 'class_weights': 'True',\n",
       " 'warmup_epochs': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify config file\n",
    "config_number=1021\n",
    "config_file='config{}'.format(config_number)\n",
    "def obtain_configs(number):\n",
    "    '''Extracts hyperparameters from config file given the config file number.'''\n",
    "    with open('../configs/config{}.json'.format(number)) as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = obtain_configs(config_number)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['authentic', 'fake']\n",
      "Total data: 2 classes for 700 files for train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import VideoFrameGenerator\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[-1] for i in glob.glob('../restructured-all-faces/home/jupyter/restructured_data/test/*')]\n",
    "classes.sort() # actually already within source code\n",
    "print(classes)\n",
    "# some global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 20\n",
    "BS = 4\n",
    "# pattern to get videos and classes\n",
    "\n",
    "glob_pattern_test = '../restructured-all-faces/home/jupyter/restructured_data/test/{classname}/*'\n",
    "\n",
    "#glob_pattern='videos/{classname}/*.avi'\n",
    "# for data augmentation\n",
    "data_aug_test = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "test_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_test,\n",
    "        nb_frames=NBFRAME,\n",
    "    #    split=0, \n",
    "        shuffle=True,\n",
    "        batch_size=BS,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture = 'lstm',frozen_base = True):\n",
    "    \n",
    "    frames = 20\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    embed_dim = 1280\n",
    "    num_heads = 8\n",
    "    ff_dim = 512\n",
    "    \n",
    "    # embed_dim is Embedding size for each token\n",
    "    # num_heads is number of attention heads\n",
    "    # ff_dim is hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "        \n",
    "    #from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "    #conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "    #                    input_shape=(224,224,3))\n",
    "    \n",
    "    from efficientnet.tfkeras import EfficientNetB0\n",
    "    conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "            input_shape=(224,224,3))\n",
    "    cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "    cnn = tf.keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "    #cnn.trainable = False\n",
    "    \n",
    "    encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "        \n",
    "    if architecture == 'lstm':\n",
    "        \n",
    "        encoded_sequence = tf.keras.layers.LSTM(2048, dropout = dropout)(encoded_frames)\n",
    "        hidden_layer = tf.keras.layers.Dense(512, activation=\"relu\")(encoded_sequence)\n",
    "        hidden_layer2 = tf.keras.layers.Dense(128, activation=\"relu\")(hidden_layer)\n",
    "        dropoutlstm = tf.keras.layers.Dropout(dropout)(hidden_layer2)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(dropoutlstm)\n",
    "        model = Model([video], outputs)\n",
    "    \n",
    "        \n",
    "    if architecture == 'transformer':\n",
    "        \n",
    "        transformer_block1 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block1(encoded_frames)\n",
    "        transformer_block2 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block2(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model([video], outputs)\n",
    "                \n",
    "    if frozen_base:\n",
    "        cnn.trainable = False # freeze the convolutional base\n",
    "        \n",
    "    else: \n",
    "        cnn.trainable = True    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      epsilon=1e-08,\n",
    "                      schedule_decay=0.004)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"]) \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 20, 1280)          4049564   \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_1 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               655872    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 20,524,062\n",
      "Trainable params: 20,482,046\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "with strategy.scope(): # Allows for parallel GPUs\n",
    "    model = build_model(0, params['learning_rate'], params['architecture'], frozen_base = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n"
     ]
    }
   ],
   "source": [
    "path_to_weights = \"../all_faces_bucket/trained_models/weights/{}\".format(config_file)\n",
    "# get all the weights file names in a list\n",
    "if os.path.exists(path_to_weights):\n",
    "    all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "# If there is at least one file\n",
    "    if len(all_weights) >= 1:\n",
    "        # Use weights from highest val acc\n",
    "        model.load_weights(path_to_weights + '/' + 'highest_val_acc.hdf5')\n",
    "        print('Weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 31s 174ms/step - accuracy: 0.9886 - loss: 0.0561\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test_data,\n",
    "                           # class_weight=class_weights,\n",
    "                          #  initial_epoch=0,\n",
    "                            verbose=1,\n",
    "                            max_queue_size=100,                # maximum size for the generator queue\n",
    "                            workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "                            use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
