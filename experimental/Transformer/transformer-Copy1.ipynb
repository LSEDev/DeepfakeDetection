{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "# https://github.com/tensorflow/tensor2tensor# See available GPU RAM \n",
    "!nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !htop # cpu threads and if they're all working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "\n",
    "import TransformCode as tc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = tc.TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model_text = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 200, 32)           646400    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 200, 32)           6464      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 653,566\n",
      "Trainable params: 653,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [3], we do not fine-tune the network. \n",
    "The 2048-dimensional feature vec- tors after the last pooling layers are then used as the sequen- tial LSTM input.\n",
    "\n",
    "The key challenge that we need to address is the de- sign of a model to recursively process a sequence in a mean- ingful manner. For this problem, we resort to the use of a 2048-wide LSTM unit with 0.5 chance of dropout, which is capable to do exactly what we need. More particularly, during training, our LSTM model takes a sequence of 2048- dimensional ImageNet feature vectors. The LSTM is followed by a 512 fully-connected layer with 0.5 chance of dropout. Then classic dropout, no auxiliary loss functions are necessary (but might be interesting right).\n",
    "\n",
    "The optimizer is set to Adam[23] for end-to-endtraining of the complete model with a learning rate of 1e−5 and decay of 1e−6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#history = model.fit(\n",
    "#    x_train, y_train, batch_size=32, epochs=4, validation_data=(x_val, y_val)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture = 'lstm'):\n",
    "    frames = 10\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    \n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "    \n",
    "    \n",
    "    if architecture == 'lstm':\n",
    "        \n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                            input_shape=(224,224,3))\n",
    "    \n",
    "        cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "        cnn = keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "        cnn.trainable = False\n",
    "    \n",
    "        encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "        \n",
    "        encoded_sequence = tf.keras.layers.LSTM(256)(encoded_frames)\n",
    "        hidden_layer = Dense(128, activation=\"relu\")(encoded_sequence)\n",
    "        outputs = Dense(2, activation=\"softmax\")(hidden_layer)\n",
    "        model = Model([video], outputs)\n",
    "    \n",
    "        \n",
    "    if architecture == 'transformer':\n",
    "        \n",
    "        embedding_layer = tc.TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        x = embedding_layer(cnn.output)\n",
    "        transformer_block = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model([video], outputs)\n",
    "                \n",
    "        \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      epsilon=1e-08,\n",
    "                      schedule_decay=0.004)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"binary_accuracy\"]) \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweaked_ImageGenerator_v2 import ImageDataGenerator\n",
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "\n",
    "#     # Classes give the folders storing the two different categories\n",
    "#     train_data = datagen.flow_from_directory(directory + '/train',\n",
    "#                                              target_size=(224,224), batch_size = batch,\n",
    "#                                             frames_per_step=15, shuffle = True)\n",
    "    \n",
    "#     val_data = datagen.flow_from_directory(directory + '/validation',\n",
    "#                                              target_size=(224,224), batch_size = batch,\n",
    "#                                           frames_per_step=15, shuffle = True)\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen.flow_from_directory(directory + '/train/authentic',\n",
    "                                             target_size=(224,224), batch_size = batch,classes=['Original_001','Original_002'],\n",
    "                                            frames_per_step=15, shuffle = True)\n",
    "    \n",
    "    val_data = datagen.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch,\n",
    "                                          frames_per_step=15, shuffle = True)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78 images belonging to 2 classes.\n",
      "Found 21292 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch = 4\n",
    "train_data = augment_data('../restructured-all-faces/home/jupyter/restructured_data', batch)\n",
    "# train_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 15, 224, 224, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i,j in enumerate(train_data):\n",
    "#     print(j)\n",
    "#     if i==5:\n",
    "#         break\n",
    "\n",
    "# b=[]\n",
    "# c=0\n",
    "# for i in train_data:\n",
    "#     for j in i:\n",
    "#         b.append(j)\n",
    "#         c+=1\n",
    "#         if c==4:\n",
    "#             break\n",
    "#     break\n",
    "    \n",
    "# # bb = np.array(b)\n",
    "\n",
    "# np.array(b).shape\n",
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 15, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "c=0\n",
    "for i in train_data:\n",
    "    b.append(i[0])\n",
    "    c+=1\n",
    "    if c==4:\n",
    "        break\n",
    "print(b[0].shape)\n",
    "np.array(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 15, 224, 224, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 15, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 60, 15, 224, 224, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b=[]\n",
    "# c=0\n",
    "# for i in train_data:\n",
    "#     b.append(i[0])\n",
    "#     c+=1\n",
    "#     if c==4:\n",
    "#         break\n",
    "print(b[0].shape)\n",
    "np.array(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113929 images belonging to 2 classes.\n",
      "Found 21291 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "t,v = augment_data('../restructured_data', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'categorical'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.class_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 1280)          2257984   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               1573888   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,865,026\n",
      "Trainable params: 1,607,042\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "with strategy.scope(): # Everything that creates variables should be under the strategy scope.\n",
    "    model = build_model(0.1,0.002)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113929 images belonging to 2 classes.\n",
      "Found 21291 images belonging to 2 classes.\n",
      "Ensure class weights function corresponds to these class indices: {'authentic': 0, 'fake': 1}\n"
     ]
    }
   ],
   "source": [
    "batch = 4\n",
    "\n",
    "train_data, val_data = augment_data('../restructured_data', batch)\n",
    "\n",
    "class_weights = calculate_class_weights(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/28482 [..............................] - ETA: 185:01:43"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [60,2] vs. [90,2]\n\t [[node metrics/binary_accuracy/Equal (defined at /opt/conda/lib/python3.7/threading.py:926) ]]\n\t [[Identity_4/_58]]\n  (1) Invalid argument:  Incompatible shapes: [60,2] vs. [90,2]\n\t [[node metrics/binary_accuracy/Equal (defined at /opt/conda/lib/python3.7/threading.py:926) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_23600]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node metrics/binary_accuracy/Equal:\n cond_1/Identity_1 (defined at <ipython-input-7-fe90236a01c6>:11)\n\nInput Source operations connected to node metrics/binary_accuracy/Equal:\n cond_1/Identity_1 (defined at <ipython-input-7-fe90236a01c6>:11)\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fe90236a01c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# maximum size for the generator queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0;31m# maximum number of processes to spin up when using process-based threading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m               use_multiprocessing=False)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [60,2] vs. [90,2]\n\t [[node metrics/binary_accuracy/Equal (defined at /opt/conda/lib/python3.7/threading.py:926) ]]\n\t [[Identity_4/_58]]\n  (1) Invalid argument:  Incompatible shapes: [60,2] vs. [90,2]\n\t [[node metrics/binary_accuracy/Equal (defined at /opt/conda/lib/python3.7/threading.py:926) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_23600]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node metrics/binary_accuracy/Equal:\n cond_1/Identity_1 (defined at <ipython-input-7-fe90236a01c6>:11)\n\nInput Source operations connected to node metrics/binary_accuracy/Equal:\n cond_1/Identity_1 (defined at <ipython-input-7-fe90236a01c6>:11)\n\nFunction call stack:\ndistributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses],\n",
    "              verbose=1,\n",
    "              max_queue_size=30,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_from_best_weights(dropout, lr_rate, architecture):\n",
    "    '''Takes the latest saved weights and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/' + architecture + '_model.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/' + architecture):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/' + architecture)\n",
    "\n",
    "    # Save weights - below saves every epoch where there is improvement\n",
    "    # filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/highest_val_acc.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/' + architecture + \".csv\",\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=7)\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    #load_model_weights(model, architecture)\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses, checkpoint, csv_logger, es],\n",
    "              verbose=1,\n",
    "              max_queue_size=30,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Everything that creates variables should be under the strategy scope.\n",
    "        model = build_model(dropout, lr_rate, architecture)\n",
    "    print(model.summary())\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(dropout = 0.1, lr_rate = 0.0002, batch = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
