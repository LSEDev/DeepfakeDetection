{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "# https://github.com/tensorflow/tensor2tensor# See available GPU RAM \n",
    "!nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !htop # cpu threads and if they're all working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "\n",
    "import TransformCode as tc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 20000  # Only consider the top 20k words\n",
    "#maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "#(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "#print(len(x_train), \"Training sequences\")\n",
    "#print(len(x_val), \"Validation sequences\")\n",
    "#x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_dim = 32  # Embedding size for each token\n",
    "#num_heads = 4  # Number of attention heads\n",
    "#ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "#\n",
    "#inputs = layers.Input(shape=(maxlen,))\n",
    "#embedding_layer = tc.TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "#x = embedding_layer(inputs)\n",
    "#transformer_block = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "#x = transformer_block(x)\n",
    "#x = layers.GlobalAveragePooling1D()(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "#x = layers.Dense(20, activation=\"relu\")(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "#outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "#\n",
    "#model_text = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [3], we do not fine-tune the network. \n",
    "The 2048-dimensional feature vec- tors after the last pooling layers are then used as the sequen- tial LSTM input.\n",
    "\n",
    "The key challenge that we need to address is the de- sign of a model to recursively process a sequence in a mean- ingful manner. For this problem, we resort to the use of a 2048-wide LSTM unit with 0.5 chance of dropout, which is capable to do exactly what we need. More particularly, during training, our LSTM model takes a sequence of 2048- dimensional ImageNet feature vectors. The LSTM is followed by a 512 fully-connected layer with 0.5 chance of dropout. Then classic dropout, no auxiliary loss functions are necessary (but might be interesting right).\n",
    "\n",
    "The optimizer is set to Adam[23] for end-to-endtraining of the complete model with a learning rate of 1e−5 and decay of 1e−6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#history = model.fit(\n",
    "#    x_train, y_train, batch_size=32, epochs=4, validation_data=(x_val, y_val)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture):\n",
    "    frames = 20\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "    from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "    conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                            input_shape=(224,224,3))\n",
    "    \n",
    "    cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "    cnn = keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "    cnn.trainable = False\n",
    "    \n",
    "    encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "    encoded_sequence = tf.keras.layers.LSTM(256)(encoded_frames)\n",
    "    hidden_layer = Dense(1024, activation=\"relu\")(encoded_sequence)\n",
    "    outputs = Dense(2, activation=\"softmax\")(hidden_layer)\n",
    "    model = Model([video], outputs)\n",
    "    #0.02\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      epsilon=1e-08,\n",
    "                      schedule_decay=0.004)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"categorical_accuracy\"]) \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweaked_ImageGenerator_v2 import ImageDataGenerator\n",
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(224,224), batch_size = batch,\n",
    "                                            frames_per_step=20, shuffle = False)\n",
    "    \n",
    "    val_data = datagen.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch,\n",
    "                                          frames_per_step=20, shuffle = False)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../all_faces_disk/home/jupyter/forensics_split/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c2a266454a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../all_faces_disk/home/jupyter/forensics_split'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4fb1c7693ef3>\u001b[0m in \u001b[0;36maugment_data\u001b[0;34m(directory, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m     train_data = datagen.flow_from_directory(directory + '/train',\n\u001b[1;32m     38\u001b[0m                                              \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                             frames_per_step=20)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     val_data = datagen.flow_from_directory(directory + '/validation',\n",
      "\u001b[0;32m~/DeepFake-2019-20/experimental/Transformer/tweaked_ImageGenerator_v2.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, frames_per_step, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             follow_links=follow_links)\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;31m# my addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchange_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeepFake-2019-20/experimental/Transformer/tweaked_ImageGenerator_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, frames_per_step, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../all_faces_disk/home/jupyter/forensics_split/train'"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "with strategy.scope(): # Everything that creates variables should be under the strategy scope.\n",
    "    model = build_model(0.1,0.002, 'boeje')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "\n",
    "class_weights = calculate_class_weights(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses],\n",
    "              verbose=1,\n",
    "              max_queue_size=30,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_from_best_weights(dropout, lr_rate, architecture):\n",
    "    '''Takes the latest saved weights and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/' + architecture + '_model.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/' + architecture):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/' + architecture)\n",
    "\n",
    "    # Save weights - below saves every epoch where there is improvement\n",
    "    # filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/highest_val_acc.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/' + architecture + \".csv\",\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=7)\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    #load_model_weights(model, architecture)\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses, checkpoint, csv_logger, es],\n",
    "              verbose=1,\n",
    "              max_queue_size=30,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Everything that creates variables should be under the strategy scope.\n",
    "        model = build_model(dropout, lr_rate, architecture)\n",
    "    print(model.summary())\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(dropout = 0.1, lr_rate = 0.0002, batch = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
