{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Train-from-specified-config-files\" data-toc-modified-id=\"Train-from-specified-config-files-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Train from specified config files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-model\" data-toc-modified-id=\"Build-model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Build model</a></span></li><li><span><a href=\"#Augment-data\" data-toc-modified-id=\"Augment-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Augment data</a></span></li><li><span><a href=\"#Auxiliary-train-time-functions\" data-toc-modified-id=\"Auxiliary-train-time-functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Auxiliary train time functions</a></span></li><li><span><a href=\"#Training-functions\" data-toc-modified-id=\"Training-functions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Training functions</a></span></li><li><span><a href=\"#Unifying-Function\" data-toc-modified-id=\"Unifying-Function-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Unifying Function</a></span></li><li><span><a href=\"#Run-training\" data-toc-modified-id=\"Run-training-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Run training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from specified config files\n",
    "        -- Built for any deepfakes dataset with file structure as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import TransformCode as tc\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "\n",
    "!pip install git+https://github.com/qubvel/efficientnet\n",
    "!pip install imgaug\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify config file\n",
    "config_number=1011\n",
    "config_file='config{}'.format(config_number)\n",
    "def obtain_configs(number):\n",
    "    '''Extracts hyperparameters from config file given the config file number.'''\n",
    "    with open('../configs/config{}.json'.format(number)) as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = obtain_configs(config_number)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VideoFrameGenerator\n",
    "\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[-1] for i in glob.glob('../restructured-all-faces/home/jupyter/restructured_data/train/*')]\n",
    "classes.sort() # actually already within source code\n",
    "print(classes)\n",
    "\n",
    "# Global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 20\n",
    "BS = 8\n",
    "\n",
    "# Pattern to get videos and classes\n",
    "glob_pattern_train = '../restructured-all-faces/home/jupyter/restructured_data/train/{classname}/*'\n",
    "glob_pattern_val = '../restructured-all-faces/home/jupyter/restructured_data/validation/{classname}/*'\n",
    "\n",
    "# for data augmentation\n",
    "data_aug_train = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            brightness_range=None,\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            rescale=None,\n",
    "            preprocessing_function=deepaug.joint_function,\n",
    "            data_format=None,)\n",
    "\n",
    "data_aug_val = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture = 'lstm',frozen_base = True):\n",
    "    \n",
    "    frames = 20\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    embed_dim = 1280\n",
    "    num_heads = 8\n",
    "    ff_dim = 512\n",
    "    \n",
    "    # embed_dim is Embedding size for each token\n",
    "    # num_heads is number of attention heads\n",
    "    # ff_dim is hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "        \n",
    "    #from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "    #conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "    #                    input_shape=(224,224,3))\n",
    "    \n",
    "    from efficientnet.tfkeras import EfficientNetB0\n",
    "    conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "            input_shape=(224,224,3))\n",
    "    cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "    cnn = tf.keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "    #cnn.trainable = False\n",
    "    \n",
    "    encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "        \n",
    "    if architecture == 'lstm':\n",
    "        \n",
    "        encoded_sequence = tf.keras.layers.LSTM(2048, dropout = dropout)(encoded_frames)\n",
    "        hidden_layer = tf.keras.layers.Dense(512, activation=\"relu\")(encoded_sequence)\n",
    "        hidden_layer2 = tf.keras.layers.Dense(128, activation=\"relu\")(hidden_layer)\n",
    "        dropoutlstm = tf.keras.layers.Dropout(dropout)(hidden_layer2)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(dropoutlstm)\n",
    "        model = Model([video], outputs)\n",
    "    \n",
    "        \n",
    "    if architecture == 'transformer':\n",
    "        \n",
    "        transformer_block1 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block1(encoded_frames)\n",
    "        transformer_block2 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block2(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model([video], outputs)\n",
    "                \n",
    "    if frozen_base:\n",
    "        cnn.trainable = False # freeze the convolutional base\n",
    "        \n",
    "    else: \n",
    "        cnn.trainable = True    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      epsilon=1e-08,\n",
    "                      schedule_decay=0.004)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"]) \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(0.5,0.0001,'transformer',frozen_base = False)\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed.'''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/{}\".format(config_file)\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # If there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # Use weights from highest val acc\n",
    "            model.load_weights(path_to_weights + '/' + 'highest_val_acc.hdf5')\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture, frozen_base):\n",
    "    '''Takes the weights with the highest val accuracy and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    7. lr_rate: initial learning rate\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/{}'.format(config_file)):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/{}'.format(config_file))\n",
    "\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/{}/highest_val_acc.hdf5\".format(config_file)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/{}.csv'.format(config_file),\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=params['patience'])\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    if params['learning_rate_type']=='cosine_decay':\n",
    "        # Set learning rate config \n",
    "        sample_count = train_data.n # number of training samples\n",
    "        epochs = epochs # total epochs - affects total steps (and hence speed of decay)\n",
    "        warmup_epoch = params['warmup_epochs'] # number of warmup epochs\n",
    "        batch_size = train_data.batch_size\n",
    "        learning_rate_base = lr_rate\n",
    "        total_steps = int(epochs * sample_count / batch_size)\n",
    "\n",
    "        warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "\n",
    "        warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                            total_steps=total_steps,\n",
    "                                            warmup_learning_rate=0.0,\n",
    "                                            warmup_steps=warmup_steps,\n",
    "                                            hold_base_rate_steps=2,\n",
    "                                            verbose=0)\n",
    "\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es, warm_up_lr]\n",
    "        \n",
    "    elif params['learning_rate_type']=='constant':\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es]\n",
    "        \n",
    "    elif params['learning_rate_type']=='increasing':\n",
    "        pass\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs,\n",
    "              validation_data = val_data, \n",
    "              class_weight=class_weights,\n",
    "              callbacks=cb,\n",
    "              initial_epoch=0,                    # start training epoch - useful if continuing training\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')\n",
    "        \n",
    "    model.save_weights('../all_faces_bucket/trained_models/weights/{}/lastepoch.hdf5'.format(config_file)) \n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50, frozen_base=True):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    \n",
    "    # Create video frame generator\n",
    "    \n",
    "    train_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_train,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=batch,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_train)\n",
    "    \n",
    "    val_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_val,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=batch,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_val)\n",
    "    \n",
    "    class_weights = {0: 4, 1: 1}\n",
    "    \n",
    "    if params['class_weights']=='True':\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate)\n",
    "    else:\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, None, architecture, lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(dropout = params['dropout'], lr_rate = params['learning_rate'], \n",
    "             architecture = params['architecture'], batch = params['batch_size'],\n",
    "             epochs = params['epochs'], frozen_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to bucket from highest val acc weights\n",
    "save_model_from_best_weights(dropout = 0.3, lr_rate = 0.0001, architecture = 'transformer', frozen_base=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
