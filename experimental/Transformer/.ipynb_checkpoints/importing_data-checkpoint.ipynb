{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install tensorflow==2.1.0\n",
    "# This cell has the latest set up for AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "\n",
    "import TransformCode as tc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['authentic', 'fake']\n",
      "Total data: 2 classes for 3600 files for train\n",
      "Total data: 2 classes for 700 files for train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import VideoFrameGenerator\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[-1] for i in glob.glob('../restructured-all-faces/home/jupyter/restructured_data/train/*')]\n",
    "classes.sort() # actually already within source code\n",
    "print(classes)\n",
    "# some global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 20\n",
    "BS = 8\n",
    "# pattern to get videos and classes\n",
    "\n",
    "glob_pattern_train = '../restructured-all-faces/home/jupyter/restructured_data/train/{classname}/*'\n",
    "glob_pattern_val = '../restructured-all-faces/home/jupyter/restructured_data/validation/{classname}/*'\n",
    "\n",
    "#glob_pattern='videos/{classname}/*.avi'\n",
    "# for data augmentation\n",
    "data_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=8,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2)\n",
    "\n",
    "# Create video frame generator\n",
    "\n",
    "train_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_pattern_train,\n",
    "    nb_frames=NBFRAME,\n",
    "#    split=0, \n",
    "    shuffle=True,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_aug,\n",
    "    use_frame_cache=False)\n",
    "\n",
    "val_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_pattern_val,\n",
    "    nb_frames=NBFRAME,\n",
    "#    split=0, \n",
    "    shuffle=True,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_aug,\n",
    "    use_frame_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch out for USE_HEADER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import \\\n",
    "    img_to_array\n",
    "# Watch out for USE_HEADER!\n",
    "#img_to_array('../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_001/0000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3221   12  856 2893 2660 1487  713 1716 3171 1070  643   80 3391  212\n",
      " 1234 1794  855 3263 2810  930 2569 1345 1552  711 2865 2052  421 2728\n",
      " 1820 1317]\n",
      "2\n",
      "<bound method VideoFrameGenerator.on_epoch_end of <VideoFrameGenerator.VideoFrameGenerator object at 0x7fa526ac3dd0>>\n"
     ]
    }
   ],
   "source": [
    "#VideoFrameGenerator.utils.show_sample(train)\n",
    "print(train.indexes[0:30])\n",
    "# what are indexes?\n",
    "print(train.classes_count)\n",
    "print(train.on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_001/0000.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_001'\n",
    "word + '/' + (sorted(os.listdir(word))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_397', '../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_997', '../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_692', '../restructured-all-faces/home/jupyter/restructured_data/train/authentic/Original_627']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(train.files[0:4])\n",
    "#print(train._framecounters)\n",
    "print(train.nbframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below hopefully now relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture = 'lstm'):\n",
    "    frames = 20\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    \n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "    \n",
    "    \n",
    "    if architecture == 'lstm':\n",
    "        \n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                            input_shape=(224,224,3))\n",
    "    \n",
    "        cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "        cnn = tf.keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "        cnn.trainable = False\n",
    "    \n",
    "        encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "        \n",
    "        encoded_sequence = tf.keras.layers.LSTM(256)(encoded_frames)\n",
    "        hidden_layer = Dense(128, activation=\"relu\")(encoded_sequence)\n",
    "        outputs = Dense(2, activation=\"softmax\")(hidden_layer)\n",
    "        model = Model([video], outputs)\n",
    "    \n",
    "        \n",
    "    if architecture == 'transformer':\n",
    "        \n",
    "        embedding_layer = tc.TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        x = embedding_layer(cnn.output)\n",
    "        transformer_block = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model([video], outputs)\n",
    "                \n",
    "        \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      epsilon=1e-08,\n",
    "                      schedule_decay=0.004)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"binary_accuracy\"]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 20, 1280)          2257984   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               1573888   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,865,026\n",
      "Trainable params: 1,607,042\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "with strategy.scope(): # Everything that creates variables should be under the strategy scope.\n",
    "    model = build_model(0.1,0.002)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 3, 1: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "history = model.fit(train_data, epochs=epochs,# shuffle=True,\n",
    "            #  steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "           #   validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses],\n",
    "              verbose=1,\n",
    "              max_queue_size=30,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
