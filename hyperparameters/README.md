Two scripts:
1. ```hyper_utils.py``` creates the learning rate scheduler that uses a cosine decay strategy with warmup from 'SGDR: Stochastic Gradient Descent with Warm Restarts.'  
2. ```random_search.py``` generates configurations of hyperparameters to use when training the networks tested in the random search.
