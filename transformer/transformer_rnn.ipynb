{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Train-from-specified-config-files\" data-toc-modified-id=\"Train-from-specified-config-files-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Train from specified config files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-model\" data-toc-modified-id=\"Build-model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Build model</a></span></li><li><span><a href=\"#Augment-data\" data-toc-modified-id=\"Augment-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Augment data</a></span></li><li><span><a href=\"#Auxiliary-train-time-functions\" data-toc-modified-id=\"Auxiliary-train-time-functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Auxiliary train time functions</a></span></li><li><span><a href=\"#Training-functions\" data-toc-modified-id=\"Training-functions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Training functions</a></span></li><li><span><a href=\"#Unifying-Function\" data-toc-modified-id=\"Unifying-Function-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Unifying Function</a></span></li><li><span><a href=\"#Run-training\" data-toc-modified-id=\"Run-training-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Run training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from specified config files\n",
    "        -- Built for any deepfakes dataset with file structure as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/qubvel/efficientnet\n",
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import TransformCode as tc\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "import custom_aug as deepaug\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'transformer',\n",
       " 'epochs': 40,\n",
       " 'batch_size': 4,\n",
       " 'learning_rate_type': 'cosine_decay',\n",
       " 'learning_rate': 4e-05,\n",
       " 'patience': 20,\n",
       " 'weight_initialisation': 'imagenet',\n",
       " 'optimiser': 'adam',\n",
       " 'momentum': 0.99,\n",
       " 'nesterov': 'False',\n",
       " 'label_smoothing': 0.01,\n",
       " 'dropout': 0.3,\n",
       " 'target_size': 224,\n",
       " 'class_weights': 'True',\n",
       " 'warmup_epochs': 0,\n",
       " 'weight_decay': 2.5e-05}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify config file\n",
    "config_number=103111\n",
    "config_file='config{}'.format(config_number)\n",
    "def obtain_configs(number):\n",
    "    '''Extracts hyperparameters from config file given the config file number.'''\n",
    "    with open('../configs/config{}.json'.format(number)) as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = obtain_configs(config_number)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['authentic', 'fake']\n"
     ]
    }
   ],
   "source": [
    "import VideoFrameGenerator\n",
    "\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[-1] for i in glob.glob('../restructured-all-faces/home/jupyter/restructured_data/train/*')]\n",
    "classes.sort() # actually already within source code\n",
    "print(classes)\n",
    "\n",
    "# Global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 20\n",
    "\n",
    "# Pattern to get videos and classes\n",
    "glob_pattern_train = '../restructured-all-faces/home/jupyter/restructured_data/train/{classname}/*'\n",
    "glob_pattern_val = '../restructured-all-faces/home/jupyter/restructured_data/validation/{classname}/*'\n",
    "\n",
    "# for data augmentation\n",
    "data_aug_train = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            fill_mode='nearest',\n",
    "            brightness_range=None,\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            preprocessing_function=deepaug.joint_function,\n",
    "            data_format=None,)\n",
    "\n",
    "data_aug_val = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture = 'lstm',frozen_base = True):\n",
    "    \n",
    "    frames = 20\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    embed_dim = 1280\n",
    "    num_heads = 8\n",
    "    ff_dim = 512\n",
    "    \n",
    "    # embed_dim is Embedding size for each token\n",
    "    # num_heads is number of attention heads\n",
    "    # ff_dim is hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "        \n",
    "    #from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "    #conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "    #                    input_shape=(224,224,3))\n",
    "    \n",
    "    from efficientnet.tfkeras import EfficientNetB0\n",
    "    conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "            input_shape=(224,224,3))\n",
    "    cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "    cnn = tf.keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "    #cnn.trainable = False\n",
    "    \n",
    "    encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "        \n",
    "    if architecture == 'lstm':\n",
    "        \n",
    "        encoded_sequence = tf.keras.layers.LSTM(2048, dropout = dropout)(encoded_frames)\n",
    "        hidden_layer = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(encoded_sequence)\n",
    "        hidden_layer2 = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(hidden_layer)\n",
    "        dropoutlstm = tf.keras.layers.Dropout(dropout)(hidden_layer2)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(dropoutlstm)\n",
    "        model = Model([video], outputs)\n",
    "    \n",
    "        \n",
    "    if architecture == 'transformer':\n",
    "        \n",
    "        transformer_block1 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block1(encoded_frames)\n",
    "        transformer_block2 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block2(x)\n",
    "        transformer_block3 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block3(x)\n",
    "        transformer_block4 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block4(x)\n",
    "        transformer_block5 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block5(x)\n",
    "        transformer_block6 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block6(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(x)\n",
    "        x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model([video], outputs)\n",
    "                \n",
    "    if frozen_base:\n",
    "        cnn.trainable = False # freeze the convolutional base\n",
    "        \n",
    "    else: \n",
    "        cnn.trainable = True    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.98,\n",
    "                      epsilon=1e-09)\n",
    "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(label_smoothing=params['label_smoothing']),\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = [\"accuracy\"]) \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed.'''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/{}\".format(config_file)\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # If there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # Use weights from highest val acc\n",
    "            model.load_weights(path_to_weights + '/' + 'highest_val_acc.hdf5')\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture, frozen_base):\n",
    "    '''Takes the weights with the highest val accuracy and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    7. lr_rate: initial learning rate\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/{}'.format(config_file)):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/{}'.format(config_file))\n",
    "\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/{}/highest_val_acc.hdf5\".format(config_file)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/{}.csv'.format(config_file),\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=params['patience'])\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    if params['learning_rate_type']=='cosine_decay':\n",
    "        # Set learning rate config \n",
    "        sample_count = 3600 # number of training samples\n",
    "        epochs = epochs # total epochs - affects total steps (and hence speed of decay)\n",
    "        warmup_epoch = params['warmup_epochs'] # number of warmup epochs\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate_base = lr_rate\n",
    "        total_steps = int(epochs * sample_count / batch_size)\n",
    "\n",
    "        warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "\n",
    "        warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                            total_steps=total_steps,\n",
    "                                            warmup_learning_rate=0.0,\n",
    "                                            warmup_steps=warmup_steps,\n",
    "                                            hold_base_rate_steps=2,\n",
    "                                            verbose=0)\n",
    "\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es, warm_up_lr]\n",
    "        \n",
    "    elif params['learning_rate_type']=='constant':\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es]\n",
    "        \n",
    "    elif params['learning_rate_type']=='increasing':\n",
    "        pass\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs,\n",
    "              validation_data = val_data, \n",
    "              class_weight=class_weights,\n",
    "              callbacks=cb,\n",
    "              initial_epoch=0,                    # start training epoch - useful if continuing training\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')\n",
    "        \n",
    "    model.save_weights('../all_faces_bucket/trained_models/weights/{}/lastepoch.hdf5'.format(config_file)) \n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50, frozen_base=True):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    \n",
    "    # Create video frame generator\n",
    "    \n",
    "    train_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_train,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=batch,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_train)\n",
    "    \n",
    "    val_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_val,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=batch,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_val)\n",
    "    \n",
    "    class_weights = {0: 4, 1: 1}\n",
    "    \n",
    "    if params['class_weights']=='True':\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate)\n",
    "    else:\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, None, architecture, lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.99000\n",
      "900/900 [==============================] - 493s 548ms/step - accuracy: 0.9906 - loss: 0.1139 - val_accuracy: 0.9857 - val_loss: 0.1110\n",
      "Epoch 20/40\n",
      "\n",
      "Batch 17101: setting learning rate to 2.157100895590244e-05.\n",
      "  1/900 [..............................] - ETA: 0s - accuracy: 1.0000 - loss: 0.0684\n",
      "Batch 17102: setting learning rate to 2.156926891682464e-05.\n",
      "  2/900 [..............................] - ETA: 3:56 - accuracy: 1.0000 - loss: 0.0548\n",
      "Batch 17103: setting learning rate to 2.1567528865794852e-05.\n",
      "  3/900 [..............................] - ETA: 5:08 - accuracy: 1.0000 - loss: 0.0502\n",
      "Batch 17104: setting learning rate to 2.1565788802826315e-05.\n",
      "  4/900 [..............................] - ETA: 5:46 - accuracy: 1.0000 - loss: 0.0535\n",
      "Batch 17105: setting learning rate to 2.1564048727932292e-05.\n",
      "  5/900 [..............................] - ETA: 6:12 - accuracy: 1.0000 - loss: 0.0510\n",
      "Batch 17106: setting learning rate to 2.156230864112603e-05.\n",
      "  6/900 [..............................] - ETA: 6:26 - accuracy: 1.0000 - loss: 0.0534\n",
      "Batch 17107: setting learning rate to 2.156056854242079e-05.\n",
      "  7/900 [..............................] - ETA: 6:38 - accuracy: 1.0000 - loss: 0.0518\n",
      "Batch 17108: setting learning rate to 2.1558828431829825e-05.\n",
      "  8/900 [..............................] - ETA: 6:42 - accuracy: 1.0000 - loss: 0.0535\n",
      "Batch 17109: setting learning rate to 2.155708830936637e-05.\n",
      "  9/900 [..............................] - ETA: 6:47 - accuracy: 1.0000 - loss: 0.0520\n",
      "Batch 17110: setting learning rate to 2.1555348175043704e-05.\n",
      " 10/900 [..............................] - ETA: 6:49 - accuracy: 1.0000 - loss: 0.0511\n",
      "Batch 17111: setting learning rate to 2.155360802887506e-05.\n",
      " 11/900 [..............................] - ETA: 6:54 - accuracy: 1.0000 - loss: 0.0524\n",
      "Batch 17112: setting learning rate to 2.1551867870873705e-05.\n",
      " 12/900 [..............................] - ETA: 6:54 - accuracy: 0.9583 - loss: 0.9319\n",
      "Batch 17113: setting learning rate to 2.1550127701052883e-05.\n",
      " 13/900 [..............................] - ETA: 6:58 - accuracy: 0.9615 - loss: 0.8652\n",
      "Batch 17114: setting learning rate to 2.1548387519425858e-05.\n",
      " 14/900 [..............................] - ETA: 6:59 - accuracy: 0.9643 - loss: 0.8063\n",
      "Batch 17115: setting learning rate to 2.1546647326005868e-05.\n",
      " 15/900 [..............................] - ETA: 6:59 - accuracy: 0.9667 - loss: 0.7554\n",
      "Batch 17116: setting learning rate to 2.1544907120806187e-05.\n",
      " 16/900 [..............................] - ETA: 6:59 - accuracy: 0.9688 - loss: 0.7125\n",
      "Batch 17117: setting learning rate to 2.154316690384005e-05.\n",
      " 17/900 [..............................] - ETA: 7:02 - accuracy: 0.9706 - loss: 0.6763\n",
      "Batch 17118: setting learning rate to 2.1541426675120724e-05.\n",
      " 18/900 [..............................] - ETA: 7:02 - accuracy: 0.9722 - loss: 0.6441\n",
      "Batch 17119: setting learning rate to 2.1539686434661453e-05.\n",
      " 19/900 [..............................] - ETA: 7:03 - accuracy: 0.9737 - loss: 0.6138\n",
      "Batch 17120: setting learning rate to 2.1537946182475507e-05.\n",
      " 20/900 [..............................] - ETA: 7:04 - accuracy: 0.9750 - loss: 0.5857\n",
      "Batch 17121: setting learning rate to 2.1536205918576117e-05.\n",
      " 21/900 [..............................] - ETA: 7:04 - accuracy: 0.9762 - loss: 0.5612\n",
      "Batch 17122: setting learning rate to 2.1534465642976557e-05.\n",
      " 22/900 [..............................] - ETA: 7:06 - accuracy: 0.9773 - loss: 0.5400\n",
      "Batch 17123: setting learning rate to 2.1532725355690066e-05.\n",
      " 23/900 [..............................] - ETA: 7:06 - accuracy: 0.9783 - loss: 0.5195\n",
      "Batch 17124: setting learning rate to 2.1530985056729916e-05.\n",
      " 24/900 [..............................] - ETA: 7:06 - accuracy: 0.9792 - loss: 0.5012\n",
      "Batch 17125: setting learning rate to 2.152924474610935e-05.\n",
      " 25/900 [..............................] - ETA: 7:09 - accuracy: 0.9800 - loss: 0.4857\n",
      "Batch 17126: setting learning rate to 2.152750442384162e-05.\n",
      " 26/900 [..............................] - ETA: 7:09 - accuracy: 0.9808 - loss: 0.4697\n",
      "Batch 17127: setting learning rate to 2.1525764089939996e-05.\n",
      " 27/900 [..............................] - ETA: 7:09 - accuracy: 0.9815 - loss: 0.4541\n",
      "Batch 17128: setting learning rate to 2.1524023744417713e-05.\n",
      " 28/900 [..............................] - ETA: 7:09 - accuracy: 0.9821 - loss: 0.4408\n",
      "Batch 17129: setting learning rate to 2.1522283387288043e-05.\n",
      " 29/900 [..............................] - ETA: 7:08 - accuracy: 0.9828 - loss: 0.4274\n",
      "Batch 17130: setting learning rate to 2.1520543018564227e-05.\n",
      " 30/900 [>.............................] - ETA: 7:09 - accuracy: 0.9833 - loss: 0.4152\n",
      "Batch 17131: setting learning rate to 2.151880263825954e-05.\n",
      " 31/900 [>.............................] - ETA: 7:09 - accuracy: 0.9839 - loss: 0.4037\n",
      "Batch 17132: setting learning rate to 2.151706224638721e-05.\n",
      " 32/900 [>.............................] - ETA: 7:09 - accuracy: 0.9844 - loss: 0.3940\n",
      "Batch 17133: setting learning rate to 2.1515321842960516e-05.\n",
      " 33/900 [>.............................] - ETA: 7:09 - accuracy: 0.9848 - loss: 0.3840\n",
      "Batch 17134: setting learning rate to 2.15135814279927e-05.\n",
      " 34/900 [>.............................] - ETA: 7:08 - accuracy: 0.9853 - loss: 0.3742\n",
      "Batch 17135: setting learning rate to 2.1511841001497024e-05.\n",
      " 35/900 [>.............................] - ETA: 7:09 - accuracy: 0.9857 - loss: 0.3652\n",
      "Batch 17136: setting learning rate to 2.1510100563486733e-05.\n",
      " 36/900 [>.............................] - ETA: 7:08 - accuracy: 0.9861 - loss: 0.3574\n",
      "Batch 17137: setting learning rate to 2.1508360113975103e-05.\n",
      " 37/900 [>.............................] - ETA: 7:08 - accuracy: 0.9865 - loss: 0.3498\n",
      "Batch 17138: setting learning rate to 2.150661965297537e-05.\n",
      " 38/900 [>.............................] - ETA: 7:08 - accuracy: 0.9868 - loss: 0.3421\n",
      "Batch 17139: setting learning rate to 2.1504879180500794e-05.\n",
      " 39/900 [>.............................] - ETA: 7:08 - accuracy: 0.9872 - loss: 0.3349\n",
      "Batch 17140: setting learning rate to 2.1503138696564637e-05.\n",
      " 40/900 [>.............................] - ETA: 7:08 - accuracy: 0.9875 - loss: 0.3277\n",
      "Batch 17141: setting learning rate to 2.1501398201180156e-05.\n",
      " 41/900 [>.............................] - ETA: 7:08 - accuracy: 0.9878 - loss: 0.3208\n",
      "Batch 17142: setting learning rate to 2.1499657694360597e-05.\n",
      " 42/900 [>.............................] - ETA: 7:07 - accuracy: 0.9881 - loss: 0.3150\n",
      "Batch 17143: setting learning rate to 2.149791717611923e-05.\n",
      " 43/900 [>.............................] - ETA: 7:07 - accuracy: 0.9884 - loss: 0.3093\n",
      "Batch 17144: setting learning rate to 2.1496176646469298e-05.\n",
      " 44/900 [>.............................] - ETA: 7:07 - accuracy: 0.9886 - loss: 0.3033\n",
      "Batch 17145: setting learning rate to 2.1494436105424062e-05.\n",
      " 45/900 [>.............................] - ETA: 7:07 - accuracy: 0.9889 - loss: 0.2978\n",
      "Batch 17146: setting learning rate to 2.1492695552996788e-05.\n",
      " 46/900 [>.............................] - ETA: 7:07 - accuracy: 0.9837 - loss: 0.3853\n",
      "Batch 17147: setting learning rate to 2.1490954989200718e-05.\n",
      " 47/900 [>.............................] - ETA: 7:06 - accuracy: 0.9840 - loss: 0.3790\n",
      "Batch 17148: setting learning rate to 2.1489214414049114e-05.\n",
      " 48/900 [>.............................] - ETA: 7:06 - accuracy: 0.9844 - loss: 0.3723\n",
      "Batch 17149: setting learning rate to 2.148747382755523e-05.\n",
      " 49/900 [>.............................] - ETA: 7:06 - accuracy: 0.9847 - loss: 0.3661\n",
      "Batch 17150: setting learning rate to 2.1485733229732334e-05.\n",
      " 50/900 [>.............................] - ETA: 7:05 - accuracy: 0.9850 - loss: 0.3601\n",
      "Batch 17151: setting learning rate to 2.1483992620593676e-05.\n",
      " 51/900 [>.............................] - ETA: 7:05 - accuracy: 0.9853 - loss: 0.3547\n",
      "Batch 17152: setting learning rate to 2.1482252000152508e-05.\n",
      " 52/900 [>.............................] - ETA: 7:05 - accuracy: 0.9856 - loss: 0.3492\n",
      "Batch 17153: setting learning rate to 2.148051136842209e-05.\n",
      " 53/900 [>.............................] - ETA: 7:04 - accuracy: 0.9858 - loss: 0.3437\n",
      "Batch 17154: setting learning rate to 2.147877072541569e-05.\n",
      " 54/900 [>.............................] - ETA: 7:04 - accuracy: 0.9861 - loss: 0.3390\n",
      "Batch 17155: setting learning rate to 2.1477030071146542e-05.\n",
      " 55/900 [>.............................] - ETA: 7:04 - accuracy: 0.9864 - loss: 0.3339\n",
      "Batch 17156: setting learning rate to 2.147528940562792e-05.\n",
      " 56/900 [>.............................] - ETA: 7:04 - accuracy: 0.9866 - loss: 0.3290\n",
      "Batch 17157: setting learning rate to 2.147354872887308e-05.\n",
      " 57/900 [>.............................] - ETA: 7:03 - accuracy: 0.9868 - loss: 0.3242\n",
      "Batch 17158: setting learning rate to 2.1471808040895285e-05.\n",
      " 58/900 [>.............................] - ETA: 7:03 - accuracy: 0.9871 - loss: 0.3203\n",
      "Batch 17159: setting learning rate to 2.1470067341707774e-05.\n",
      " 59/900 [>.............................] - ETA: 7:03 - accuracy: 0.9873 - loss: 0.3163\n",
      "Batch 17160: setting learning rate to 2.1468326631323824e-05.\n",
      " 60/900 [=>............................] - ETA: 7:03 - accuracy: 0.9875 - loss: 0.3127\n",
      "Batch 17161: setting learning rate to 2.146658590975668e-05.\n",
      " 61/900 [=>............................] - ETA: 7:02 - accuracy: 0.9877 - loss: 0.3096\n",
      "Batch 17162: setting learning rate to 2.1464845177019608e-05.\n",
      " 62/900 [=>............................] - ETA: 7:02 - accuracy: 0.9879 - loss: 0.3059\n",
      "Batch 17163: setting learning rate to 2.146310443312586e-05.\n",
      " 63/900 [=>............................] - ETA: 7:02 - accuracy: 0.9881 - loss: 0.3022\n",
      "Batch 17164: setting learning rate to 2.1461363678088695e-05.\n",
      " 64/900 [=>............................] - ETA: 7:01 - accuracy: 0.9883 - loss: 0.2989\n",
      "Batch 17165: setting learning rate to 2.1459622911921377e-05.\n",
      " 65/900 [=>............................] - ETA: 7:01 - accuracy: 0.9885 - loss: 0.2955\n",
      "Batch 17166: setting learning rate to 2.1457882134637155e-05.\n",
      " 66/900 [=>............................] - ETA: 7:01 - accuracy: 0.9886 - loss: 0.2925\n",
      "Batch 17167: setting learning rate to 2.1456141346249293e-05.\n",
      " 67/900 [=>............................] - ETA: 7:00 - accuracy: 0.9888 - loss: 0.2895\n",
      "Batch 17168: setting learning rate to 2.1454400546771056e-05.\n",
      " 68/900 [=>............................] - ETA: 7:00 - accuracy: 0.9890 - loss: 0.2864\n",
      "Batch 17169: setting learning rate to 2.1452659736215688e-05.\n",
      " 69/900 [=>............................] - ETA: 6:59 - accuracy: 0.9891 - loss: 0.2831\n",
      "Batch 17170: setting learning rate to 2.1450918914596446e-05.\n",
      " 70/900 [=>............................] - ETA: 6:59 - accuracy: 0.9893 - loss: 0.2798\n",
      "Batch 17171: setting learning rate to 2.1449178081926613e-05.\n",
      " 71/900 [=>............................] - ETA: 6:58 - accuracy: 0.9894 - loss: 0.2770\n",
      "Batch 17172: setting learning rate to 2.1447437238219423e-05.\n",
      " 72/900 [=>............................] - ETA: 6:58 - accuracy: 0.9896 - loss: 0.2743\n",
      "Batch 17173: setting learning rate to 2.1445696383488142e-05.\n",
      " 73/900 [=>............................] - ETA: 6:57 - accuracy: 0.9897 - loss: 0.2719\n",
      "Batch 17174: setting learning rate to 2.144395551774603e-05.\n",
      " 74/900 [=>............................] - ETA: 6:57 - accuracy: 0.9899 - loss: 0.2689\n",
      "Batch 17175: setting learning rate to 2.1442214641006353e-05.\n",
      " 75/900 [=>............................] - ETA: 6:57 - accuracy: 0.9900 - loss: 0.2660\n",
      "Batch 17176: setting learning rate to 2.1440473753282353e-05.\n",
      " 76/900 [=>............................] - ETA: 6:56 - accuracy: 0.9901 - loss: 0.2632\n",
      "Batch 17177: setting learning rate to 2.1438732854587308e-05.\n",
      " 77/900 [=>............................] - ETA: 6:56 - accuracy: 0.9903 - loss: 0.2604\n",
      "Batch 17178: setting learning rate to 2.1436991944934466e-05.\n",
      " 78/900 [=>............................] - ETA: 6:55 - accuracy: 0.9904 - loss: 0.2577\n",
      "Batch 17179: setting learning rate to 2.1435251024337087e-05.\n",
      " 79/900 [=>............................] - ETA: 6:55 - accuracy: 0.9905 - loss: 0.2556\n",
      "Batch 17180: setting learning rate to 2.143351009280843e-05.\n",
      " 80/900 [=>............................] - ETA: 6:54 - accuracy: 0.9906 - loss: 0.2533\n",
      "Batch 17181: setting learning rate to 2.1431769150361757e-05.\n",
      " 81/900 [=>............................] - ETA: 6:53 - accuracy: 0.9907 - loss: 0.2508\n",
      "Batch 17182: setting learning rate to 2.1430028197010332e-05.\n",
      " 82/900 [=>............................] - ETA: 6:53 - accuracy: 0.9909 - loss: 0.2483\n",
      "Batch 17183: setting learning rate to 2.14282872327674e-05.\n",
      " 83/900 [=>............................] - ETA: 6:53 - accuracy: 0.9910 - loss: 0.2461\n",
      "Batch 17184: setting learning rate to 2.1426546257646238e-05.\n",
      " 84/900 [=>............................] - ETA: 6:52 - accuracy: 0.9911 - loss: 0.2440\n",
      "Batch 17185: setting learning rate to 2.142480527166009e-05.\n",
      " 85/900 [=>............................] - ETA: 6:52 - accuracy: 0.9912 - loss: 0.2420\n",
      "Batch 17186: setting learning rate to 2.142306427482223e-05.\n",
      " 86/900 [=>............................] - ETA: 6:51 - accuracy: 0.9913 - loss: 0.2397\n",
      "Batch 17187: setting learning rate to 2.1421323267145908e-05.\n",
      " 87/900 [=>............................] - ETA: 6:51 - accuracy: 0.9885 - loss: 0.2574\n",
      "Batch 17188: setting learning rate to 2.14195822486444e-05.\n",
      " 88/900 [=>............................] - ETA: 6:50 - accuracy: 0.9886 - loss: 0.2553\n",
      "Batch 17189: setting learning rate to 2.141784121933094e-05.\n",
      " 89/900 [=>............................] - ETA: 6:50 - accuracy: 0.9888 - loss: 0.2536\n",
      "Batch 17190: setting learning rate to 2.1416100179218805e-05.\n",
      " 90/900 [==>...........................] - ETA: 6:49 - accuracy: 0.9889 - loss: 0.2516\n",
      "Batch 17191: setting learning rate to 2.141435912832125e-05.\n",
      " 91/900 [==>...........................] - ETA: 6:49 - accuracy: 0.9890 - loss: 0.2496\n",
      "Batch 17192: setting learning rate to 2.1412618066651544e-05.\n",
      " 92/900 [==>...........................] - ETA: 6:48 - accuracy: 0.9891 - loss: 0.2480\n",
      "Batch 17193: setting learning rate to 2.1410876994222933e-05.\n",
      " 93/900 [==>...........................] - ETA: 6:48 - accuracy: 0.9892 - loss: 0.2462\n",
      "Batch 17194: setting learning rate to 2.140913591104869e-05.\n",
      " 94/900 [==>...........................] - ETA: 6:47 - accuracy: 0.9894 - loss: 0.2446\n",
      "Batch 17195: setting learning rate to 2.1407394817142074e-05.\n",
      " 95/900 [==>...........................] - ETA: 6:47 - accuracy: 0.9895 - loss: 0.2432\n",
      "Batch 17196: setting learning rate to 2.1405653712516338e-05.\n",
      " 96/900 [==>...........................] - ETA: 6:46 - accuracy: 0.9896 - loss: 0.2417\n",
      "Batch 17197: setting learning rate to 2.1403912597184743e-05.\n",
      " 97/900 [==>...........................] - ETA: 6:46 - accuracy: 0.9897 - loss: 0.2403\n",
      "Batch 17198: setting learning rate to 2.1402171471160563e-05.\n",
      " 98/900 [==>...........................] - ETA: 6:45 - accuracy: 0.9898 - loss: 0.2388\n",
      "Batch 17199: setting learning rate to 2.1400430334457046e-05.\n",
      " 99/900 [==>...........................] - ETA: 6:45 - accuracy: 0.9899 - loss: 0.2373\n",
      "Batch 17200: setting learning rate to 2.1398689187087452e-05.\n",
      "100/900 [==>...........................] - ETA: 6:45 - accuracy: 0.9900 - loss: 0.2355\n",
      "Batch 17201: setting learning rate to 2.1396948029065054e-05.\n",
      "101/900 [==>...........................] - ETA: 6:44 - accuracy: 0.9901 - loss: 0.2337\n",
      "Batch 17202: setting learning rate to 2.1395206860403106e-05.\n",
      "102/900 [==>...........................] - ETA: 6:44 - accuracy: 0.9902 - loss: 0.2322\n",
      "Batch 17203: setting learning rate to 2.1393465681114865e-05.\n",
      "103/900 [==>...........................] - ETA: 6:43 - accuracy: 0.9879 - loss: 0.2371\n",
      "Batch 17204: setting learning rate to 2.1391724491213603e-05.\n",
      "104/900 [==>...........................] - ETA: 6:43 - accuracy: 0.9880 - loss: 0.2353\n",
      "Batch 17205: setting learning rate to 2.1389983290712574e-05.\n",
      "105/900 [==>...........................] - ETA: 6:42 - accuracy: 0.9881 - loss: 0.2338\n",
      "Batch 17206: setting learning rate to 2.1388242079625035e-05.\n",
      "106/900 [==>...........................] - ETA: 6:42 - accuracy: 0.9882 - loss: 0.2320\n",
      "Batch 17207: setting learning rate to 2.138650085796426e-05.\n",
      "107/900 [==>...........................] - ETA: 6:41 - accuracy: 0.9883 - loss: 0.2303\n",
      "Batch 17208: setting learning rate to 2.13847596257435e-05.\n",
      "108/900 [==>...........................] - ETA: 6:41 - accuracy: 0.9884 - loss: 0.2287\n",
      "Batch 17209: setting learning rate to 2.1383018382976024e-05.\n",
      "109/900 [==>...........................] - ETA: 6:41 - accuracy: 0.9885 - loss: 0.2273\n",
      "Batch 17210: setting learning rate to 2.138127712967508e-05.\n",
      "110/900 [==>...........................] - ETA: 6:41 - accuracy: 0.9886 - loss: 0.2257\n",
      "Batch 17211: setting learning rate to 2.137953586585395e-05.\n",
      "111/900 [==>...........................] - ETA: 6:40 - accuracy: 0.9887 - loss: 0.2245\n",
      "Batch 17212: setting learning rate to 2.1377794591525886e-05.\n",
      "112/900 [==>...........................] - ETA: 6:39 - accuracy: 0.9888 - loss: 0.2231\n",
      "Batch 17213: setting learning rate to 2.137605330670415e-05.\n",
      "113/900 [==>...........................] - ETA: 6:39 - accuracy: 0.9889 - loss: 0.2215\n",
      "Batch 17214: setting learning rate to 2.1374312011402e-05.\n",
      "114/900 [==>...........................] - ETA: 6:39 - accuracy: 0.9890 - loss: 0.2199\n",
      "Batch 17215: setting learning rate to 2.1372570705632713e-05.\n",
      "115/900 [==>...........................] - ETA: 6:38 - accuracy: 0.9891 - loss: 0.2186\n",
      "Batch 17216: setting learning rate to 2.1370829389409528e-05.\n",
      "116/900 [==>...........................] - ETA: 6:38 - accuracy: 0.9892 - loss: 0.2175\n",
      "Batch 17217: setting learning rate to 2.1369088062745726e-05.\n",
      "117/900 [==>...........................] - ETA: 6:37 - accuracy: 0.9893 - loss: 0.2160\n",
      "Batch 17218: setting learning rate to 2.136734672565456e-05.\n",
      "118/900 [==>...........................] - ETA: 6:37 - accuracy: 0.9894 - loss: 0.2148\n",
      "Batch 17219: setting learning rate to 2.13656053781493e-05.\n",
      "119/900 [==>...........................] - ETA: 6:36 - accuracy: 0.9895 - loss: 0.2136\n",
      "Batch 17220: setting learning rate to 2.1363864020243205e-05.\n",
      "120/900 [===>..........................] - ETA: 6:36 - accuracy: 0.9896 - loss: 0.2124\n",
      "Batch 17221: setting learning rate to 2.1362122651949533e-05.\n",
      "121/900 [===>..........................] - ETA: 6:36 - accuracy: 0.9897 - loss: 0.2112\n",
      "Batch 17222: setting learning rate to 2.1360381273281558e-05.\n",
      "122/900 [===>..........................] - ETA: 6:35 - accuracy: 0.9898 - loss: 0.2100\n",
      "Batch 17223: setting learning rate to 2.135863988425253e-05.\n",
      "123/900 [===>..........................] - ETA: 6:35 - accuracy: 0.9898 - loss: 0.2088\n",
      "Batch 17224: setting learning rate to 2.135689848487572e-05.\n",
      "124/900 [===>..........................] - ETA: 6:34 - accuracy: 0.9899 - loss: 0.2074\n",
      "Batch 17225: setting learning rate to 2.1355157075164393e-05.\n",
      "125/900 [===>..........................] - ETA: 6:34 - accuracy: 0.9900 - loss: 0.2061\n",
      "Batch 17226: setting learning rate to 2.1353415655131804e-05.\n",
      "126/900 [===>..........................] - ETA: 6:33 - accuracy: 0.9901 - loss: 0.2049\n",
      "Batch 17227: setting learning rate to 2.1351674224791217e-05.\n",
      "127/900 [===>..........................] - ETA: 6:33 - accuracy: 0.9902 - loss: 0.2038\n",
      "Batch 17228: setting learning rate to 2.1349932784155904e-05.\n",
      "128/900 [===>..........................] - ETA: 6:32 - accuracy: 0.9902 - loss: 0.2027\n",
      "Batch 17229: setting learning rate to 2.1348191333239118e-05.\n",
      "129/900 [===>..........................] - ETA: 6:31 - accuracy: 0.9903 - loss: 0.2016\n",
      "Batch 17230: setting learning rate to 2.1346449872054135e-05.\n",
      "130/900 [===>..........................] - ETA: 6:31 - accuracy: 0.9904 - loss: 0.2006\n",
      "Batch 17231: setting learning rate to 2.13447084006142e-05.\n",
      "131/900 [===>..........................] - ETA: 6:30 - accuracy: 0.9905 - loss: 0.1994\n",
      "Batch 17232: setting learning rate to 2.1342966918932596e-05.\n",
      "132/900 [===>..........................] - ETA: 6:29 - accuracy: 0.9905 - loss: 0.1984\n",
      "Batch 17233: setting learning rate to 2.1341225427022567e-05.\n",
      "133/900 [===>..........................] - ETA: 6:29 - accuracy: 0.9906 - loss: 0.1972\n",
      "Batch 17234: setting learning rate to 2.1339483924897398e-05.\n",
      "134/900 [===>..........................] - ETA: 6:28 - accuracy: 0.9907 - loss: 0.1962\n",
      "Batch 17235: setting learning rate to 2.133774241257034e-05.\n",
      "135/900 [===>..........................] - ETA: 6:27 - accuracy: 0.9907 - loss: 0.1958\n",
      "Batch 17236: setting learning rate to 2.133600089005465e-05.\n",
      "136/900 [===>..........................] - ETA: 6:27 - accuracy: 0.9908 - loss: 0.1947\n",
      "Batch 17237: setting learning rate to 2.1334259357363608e-05.\n",
      "137/900 [===>..........................] - ETA: 6:26 - accuracy: 0.9909 - loss: 0.1938\n",
      "Batch 17238: setting learning rate to 2.1332517814510466e-05.\n",
      "138/900 [===>..........................] - ETA: 6:26 - accuracy: 0.9909 - loss: 0.1930\n",
      "Batch 17239: setting learning rate to 2.13307762615085e-05.\n",
      "139/900 [===>..........................] - ETA: 6:25 - accuracy: 0.9892 - loss: 0.1996\n",
      "Batch 17240: setting learning rate to 2.1329034698370962e-05.\n",
      "140/900 [===>..........................] - ETA: 6:24 - accuracy: 0.9893 - loss: 0.1989\n",
      "Batch 17241: setting learning rate to 2.1327293125111126e-05.\n",
      "141/900 [===>..........................] - ETA: 6:24 - accuracy: 0.9894 - loss: 0.1979\n",
      "Batch 17242: setting learning rate to 2.132555154174225e-05.\n",
      "142/900 [===>..........................] - ETA: 6:23 - accuracy: 0.9877 - loss: 0.2386\n",
      "Batch 17243: setting learning rate to 2.13238099482776e-05.\n",
      "143/900 [===>..........................] - ETA: 6:23 - accuracy: 0.9878 - loss: 0.2376\n",
      "Batch 17244: setting learning rate to 2.132206834473044e-05.\n",
      "144/900 [===>..........................] - ETA: 6:22 - accuracy: 0.9878 - loss: 0.2365\n",
      "Batch 17245: setting learning rate to 2.1320326731114033e-05.\n",
      "145/900 [===>..........................] - ETA: 6:21 - accuracy: 0.9862 - loss: 0.2388\n",
      "Batch 17246: setting learning rate to 2.131858510744165e-05.\n",
      "146/900 [===>..........................] - ETA: 6:21 - accuracy: 0.9863 - loss: 0.2374\n",
      "Batch 17247: setting learning rate to 2.1316843473726547e-05.\n",
      "147/900 [===>..........................] - ETA: 6:20 - accuracy: 0.9864 - loss: 0.2361\n",
      "Batch 17248: setting learning rate to 2.131510182998199e-05.\n",
      "148/900 [===>..........................] - ETA: 6:20 - accuracy: 0.9865 - loss: 0.2349\n",
      "Batch 17249: setting learning rate to 2.131336017622126e-05.\n",
      "149/900 [===>..........................] - ETA: 6:19 - accuracy: 0.9866 - loss: 0.2338\n",
      "Batch 17250: setting learning rate to 2.1311618512457593e-05.\n",
      "150/900 [====>.........................] - ETA: 6:18 - accuracy: 0.9867 - loss: 0.2327\n",
      "Batch 17251: setting learning rate to 2.130987683870428e-05.\n",
      "151/900 [====>.........................] - ETA: 6:18 - accuracy: 0.9868 - loss: 0.2315\n",
      "Batch 17252: setting learning rate to 2.1308135154974577e-05.\n",
      "152/900 [====>.........................] - ETA: 6:17 - accuracy: 0.9868 - loss: 0.2303\n",
      "Batch 17253: setting learning rate to 2.1306393461281747e-05.\n",
      "153/900 [====>.........................] - ETA: 6:17 - accuracy: 0.9869 - loss: 0.2294\n",
      "Batch 17254: setting learning rate to 2.130465175763905e-05.\n",
      "154/900 [====>.........................] - ETA: 6:16 - accuracy: 0.9870 - loss: 0.2284\n",
      "Batch 17255: setting learning rate to 2.130291004405976e-05.\n",
      "155/900 [====>.........................] - ETA: 6:16 - accuracy: 0.9871 - loss: 0.2277\n",
      "Batch 17256: setting learning rate to 2.1301168320557143e-05.\n",
      "156/900 [====>.........................] - ETA: 6:15 - accuracy: 0.9872 - loss: 0.2266\n",
      "Batch 17257: setting learning rate to 2.1299426587144454e-05.\n",
      "157/900 [====>.........................] - ETA: 6:14 - accuracy: 0.9873 - loss: 0.2259\n",
      "Batch 17258: setting learning rate to 2.129768484383498e-05.\n",
      "158/900 [====>.........................] - ETA: 6:14 - accuracy: 0.9873 - loss: 0.2250\n",
      "Batch 17259: setting learning rate to 2.1295943090641963e-05.\n",
      "159/900 [====>.........................] - ETA: 6:13 - accuracy: 0.9874 - loss: 0.2241\n",
      "Batch 17260: setting learning rate to 2.1294201327578682e-05.\n",
      "160/900 [====>.........................] - ETA: 6:13 - accuracy: 0.9875 - loss: 0.2233\n",
      "Batch 17261: setting learning rate to 2.129245955465839e-05.\n",
      "161/900 [====>.........................] - ETA: 6:12 - accuracy: 0.9876 - loss: 0.2224\n",
      "Batch 17262: setting learning rate to 2.129071777189437e-05.\n",
      "162/900 [====>.........................] - ETA: 6:11 - accuracy: 0.9877 - loss: 0.2213\n",
      "Batch 17263: setting learning rate to 2.128897597929988e-05.\n",
      "163/900 [====>.........................] - ETA: 6:11 - accuracy: 0.9862 - loss: 0.2244\n",
      "Batch 17264: setting learning rate to 2.128723417688819e-05.\n",
      "164/900 [====>.........................] - ETA: 6:10 - accuracy: 0.9848 - loss: 0.2285\n",
      "Batch 17265: setting learning rate to 2.1285492364672552e-05.\n",
      "165/900 [====>.........................] - ETA: 6:10 - accuracy: 0.9833 - loss: 0.2304\n",
      "Batch 17266: setting learning rate to 2.128375054266625e-05.\n",
      "166/900 [====>.........................] - ETA: 6:09 - accuracy: 0.9834 - loss: 0.2293\n",
      "Batch 17267: setting learning rate to 2.1282008710882537e-05.\n",
      "167/900 [====>.........................] - ETA: 6:08 - accuracy: 0.9835 - loss: 0.2283\n",
      "Batch 17268: setting learning rate to 2.128026686933469e-05.\n",
      "168/900 [====>.........................] - ETA: 6:08 - accuracy: 0.9836 - loss: 0.2274\n",
      "Batch 17269: setting learning rate to 2.1278525018035965e-05.\n",
      "169/900 [====>.........................] - ETA: 6:07 - accuracy: 0.9837 - loss: 0.2264\n",
      "Batch 17270: setting learning rate to 2.1276783156999637e-05.\n",
      "170/900 [====>.........................] - ETA: 6:07 - accuracy: 0.9838 - loss: 0.2255\n",
      "Batch 17271: setting learning rate to 2.1275041286238963e-05.\n",
      "171/900 [====>.........................] - ETA: 6:06 - accuracy: 0.9839 - loss: 0.2244\n",
      "Batch 17272: setting learning rate to 2.127329940576722e-05.\n",
      "172/900 [====>.........................] - ETA: 6:06 - accuracy: 0.9840 - loss: 0.2236\n",
      "Batch 17273: setting learning rate to 2.1271557515597666e-05.\n",
      "173/900 [====>.........................] - ETA: 6:05 - accuracy: 0.9841 - loss: 0.2228\n",
      "Batch 17274: setting learning rate to 2.1269815615743573e-05.\n",
      "174/900 [====>.........................] - ETA: 6:04 - accuracy: 0.9842 - loss: 0.2219\n",
      "Batch 17275: setting learning rate to 2.126807370621821e-05.\n",
      "175/900 [====>.........................] - ETA: 6:04 - accuracy: 0.9829 - loss: 0.2236\n",
      "Batch 17276: setting learning rate to 2.1266331787034837e-05.\n",
      "176/900 [====>.........................] - ETA: 6:03 - accuracy: 0.9815 - loss: 0.2247\n",
      "Batch 17277: setting learning rate to 2.1264589858206728e-05.\n",
      "177/900 [====>.........................] - ETA: 6:03 - accuracy: 0.9816 - loss: 0.2240\n",
      "Batch 17278: setting learning rate to 2.126284791974714e-05.\n",
      "178/900 [====>.........................] - ETA: 6:02 - accuracy: 0.9817 - loss: 0.2230\n",
      "Batch 17279: setting learning rate to 2.126110597166936e-05.\n",
      "179/900 [====>.........................] - ETA: 6:02 - accuracy: 0.9818 - loss: 0.2222\n",
      "Batch 17280: setting learning rate to 2.1259364013986628e-05.\n",
      "180/900 [=====>........................] - ETA: 6:01 - accuracy: 0.9819 - loss: 0.2215\n",
      "Batch 17281: setting learning rate to 2.125762204671223e-05.\n",
      "181/900 [=====>........................] - ETA: 6:01 - accuracy: 0.9820 - loss: 0.2205\n",
      "Batch 17282: setting learning rate to 2.125588006985942e-05.\n",
      "182/900 [=====>........................] - ETA: 6:00 - accuracy: 0.9821 - loss: 0.2197\n",
      "Batch 17283: setting learning rate to 2.125413808344149e-05.\n",
      "183/900 [=====>........................] - ETA: 5:59 - accuracy: 0.9822 - loss: 0.2187\n",
      "Batch 17284: setting learning rate to 2.1252396087471675e-05.\n",
      "184/900 [=====>........................] - ETA: 5:59 - accuracy: 0.9823 - loss: 0.2180\n",
      "Batch 17285: setting learning rate to 2.125065408196327e-05.\n",
      "185/900 [=====>........................] - ETA: 5:58 - accuracy: 0.9824 - loss: 0.2175\n",
      "Batch 17286: setting learning rate to 2.1248912066929527e-05.\n",
      "186/900 [=====>........................] - ETA: 5:58 - accuracy: 0.9825 - loss: 0.2165\n",
      "Batch 17287: setting learning rate to 2.124717004238372e-05.\n",
      "187/900 [=====>........................] - ETA: 5:57 - accuracy: 0.9826 - loss: 0.2156\n",
      "Batch 17288: setting learning rate to 2.124542800833911e-05.\n",
      "188/900 [=====>........................] - ETA: 5:57 - accuracy: 0.9827 - loss: 0.2148\n",
      "Batch 17289: setting learning rate to 2.124368596480897e-05.\n",
      "189/900 [=====>........................] - ETA: 5:56 - accuracy: 0.9828 - loss: 0.2148\n",
      "Batch 17290: setting learning rate to 2.124194391180657e-05.\n",
      "190/900 [=====>........................] - ETA: 5:56 - accuracy: 0.9829 - loss: 0.2140\n",
      "Batch 17291: setting learning rate to 2.1240201849345174e-05.\n",
      "191/900 [=====>........................] - ETA: 5:55 - accuracy: 0.9830 - loss: 0.2132\n",
      "Batch 17292: setting learning rate to 2.1238459777438048e-05.\n",
      "192/900 [=====>........................] - ETA: 5:54 - accuracy: 0.9831 - loss: 0.2125\n",
      "Batch 17293: setting learning rate to 2.123671769609847e-05.\n",
      "193/900 [=====>........................] - ETA: 5:54 - accuracy: 0.9832 - loss: 0.2117\n",
      "Batch 17294: setting learning rate to 2.12349756053397e-05.\n",
      "194/900 [=====>........................] - ETA: 5:53 - accuracy: 0.9832 - loss: 0.2109\n",
      "Batch 17295: setting learning rate to 2.1233233505175002e-05.\n",
      "195/900 [=====>........................] - ETA: 5:53 - accuracy: 0.9833 - loss: 0.2101\n",
      "Batch 17296: setting learning rate to 2.123149139561766e-05.\n",
      "196/900 [=====>........................] - ETA: 5:52 - accuracy: 0.9834 - loss: 0.2093\n",
      "Batch 17297: setting learning rate to 2.1229749276680925e-05.\n",
      "197/900 [=====>........................] - ETA: 5:52 - accuracy: 0.9835 - loss: 0.2087\n",
      "Batch 17298: setting learning rate to 2.1228007148378075e-05.\n",
      "198/900 [=====>........................] - ETA: 5:51 - accuracy: 0.9836 - loss: 0.2078\n",
      "Batch 17299: setting learning rate to 2.1226265010722375e-05.\n",
      "199/900 [=====>........................] - ETA: 5:51 - accuracy: 0.9837 - loss: 0.2070\n",
      "Batch 17300: setting learning rate to 2.1224522863727103e-05.\n",
      "200/900 [=====>........................] - ETA: 5:50 - accuracy: 0.9837 - loss: 0.2062\n",
      "Batch 17301: setting learning rate to 2.122278070740551e-05.\n",
      "201/900 [=====>........................] - ETA: 5:49 - accuracy: 0.9838 - loss: 0.2056\n",
      "Batch 17302: setting learning rate to 2.122103854177088e-05.\n",
      "202/900 [=====>........................] - ETA: 5:49 - accuracy: 0.9839 - loss: 0.2049\n",
      "Batch 17303: setting learning rate to 2.1219296366836476e-05.\n",
      "203/900 [=====>........................] - ETA: 5:48 - accuracy: 0.9840 - loss: 0.2042\n",
      "Batch 17304: setting learning rate to 2.1217554182615566e-05.\n",
      "204/900 [=====>........................] - ETA: 5:48 - accuracy: 0.9841 - loss: 0.2037\n",
      "Batch 17305: setting learning rate to 2.121581198912142e-05.\n",
      "205/900 [=====>........................] - ETA: 5:47 - accuracy: 0.9841 - loss: 0.2030\n",
      "Batch 17306: setting learning rate to 2.121406978636731e-05.\n",
      "206/900 [=====>........................] - ETA: 5:47 - accuracy: 0.9842 - loss: 0.2023\n",
      "Batch 17307: setting learning rate to 2.12123275743665e-05.\n",
      "207/900 [=====>........................] - ETA: 5:46 - accuracy: 0.9843 - loss: 0.2017\n",
      "Batch 17308: setting learning rate to 2.121058535313226e-05.\n",
      "208/900 [=====>........................] - ETA: 5:45 - accuracy: 0.9844 - loss: 0.2010\n",
      "Batch 17309: setting learning rate to 2.1208843122677862e-05.\n",
      "209/900 [=====>........................] - ETA: 5:45 - accuracy: 0.9844 - loss: 0.2003\n",
      "Batch 17310: setting learning rate to 2.1207100883016576e-05.\n",
      "210/900 [======>.......................] - ETA: 5:44 - accuracy: 0.9845 - loss: 0.1996\n",
      "Batch 17311: setting learning rate to 2.120535863416167e-05.\n",
      "211/900 [======>.......................] - ETA: 5:44 - accuracy: 0.9846 - loss: 0.1988\n",
      "Batch 17312: setting learning rate to 2.1203616376126412e-05.\n",
      "212/900 [======>.......................] - ETA: 5:43 - accuracy: 0.9835 - loss: 0.2018\n",
      "Batch 17313: setting learning rate to 2.1201874108924073e-05.\n",
      "213/900 [======>.......................] - ETA: 5:43 - accuracy: 0.9836 - loss: 0.2010\n",
      "Batch 17314: setting learning rate to 2.1200131832567924e-05.\n",
      "214/900 [======>.......................] - ETA: 5:42 - accuracy: 0.9836 - loss: 0.2004\n",
      "Batch 17315: setting learning rate to 2.119838954707123e-05.\n",
      "215/900 [======>.......................] - ETA: 5:42 - accuracy: 0.9837 - loss: 0.1998\n",
      "Batch 17316: setting learning rate to 2.119664725244726e-05.\n",
      "216/900 [======>.......................] - ETA: 5:41 - accuracy: 0.9838 - loss: 0.1991\n",
      "Batch 17317: setting learning rate to 2.1194904948709295e-05.\n",
      "217/900 [======>.......................] - ETA: 5:41 - accuracy: 0.9839 - loss: 0.1985\n",
      "Batch 17318: setting learning rate to 2.1193162635870593e-05.\n",
      "218/900 [======>.......................] - ETA: 5:40 - accuracy: 0.9839 - loss: 0.1979\n",
      "Batch 17319: setting learning rate to 2.1191420313944427e-05.\n",
      "219/900 [======>.......................] - ETA: 5:39 - accuracy: 0.9840 - loss: 0.1972\n",
      "Batch 17320: setting learning rate to 2.1189677982944078e-05.\n",
      "220/900 [======>.......................] - ETA: 5:39 - accuracy: 0.9841 - loss: 0.1966\n",
      "Batch 17321: setting learning rate to 2.1187935642882794e-05.\n",
      "221/900 [======>.......................] - ETA: 5:38 - accuracy: 0.9842 - loss: 0.1960\n",
      "Batch 17322: setting learning rate to 2.1186193293773864e-05.\n",
      "222/900 [======>.......................] - ETA: 5:38 - accuracy: 0.9842 - loss: 0.1953\n",
      "Batch 17323: setting learning rate to 2.118445093563055e-05.\n",
      "223/900 [======>.......................] - ETA: 5:37 - accuracy: 0.9843 - loss: 0.1947\n",
      "Batch 17324: setting learning rate to 2.1182708568466123e-05.\n",
      "224/900 [======>.......................] - ETA: 5:37 - accuracy: 0.9844 - loss: 0.1941\n",
      "Batch 17325: setting learning rate to 2.1180966192293856e-05.\n",
      "225/900 [======>.......................] - ETA: 5:36 - accuracy: 0.9844 - loss: 0.1935\n",
      "Batch 17326: setting learning rate to 2.1179223807127018e-05.\n",
      "226/900 [======>.......................] - ETA: 5:36 - accuracy: 0.9845 - loss: 0.1928\n",
      "Batch 17327: setting learning rate to 2.117748141297888e-05.\n",
      "227/900 [======>.......................] - ETA: 5:35 - accuracy: 0.9835 - loss: 0.1933\n",
      "Batch 17328: setting learning rate to 2.1175739009862707e-05.\n",
      "228/900 [======>.......................] - ETA: 5:34 - accuracy: 0.9836 - loss: 0.1928\n",
      "Batch 17329: setting learning rate to 2.1173996597791778e-05.\n",
      "229/900 [======>.......................] - ETA: 5:34 - accuracy: 0.9836 - loss: 0.1922\n",
      "Batch 17330: setting learning rate to 2.1172254176779362e-05.\n",
      "230/900 [======>.......................] - ETA: 5:33 - accuracy: 0.9837 - loss: 0.1918\n",
      "Batch 17331: setting learning rate to 2.1170511746838724e-05.\n",
      "231/900 [======>.......................] - ETA: 5:33 - accuracy: 0.9838 - loss: 0.1914\n",
      "Batch 17332: setting learning rate to 2.1168769307983143e-05.\n",
      "232/900 [======>.......................] - ETA: 5:32 - accuracy: 0.9838 - loss: 0.1908\n",
      "Batch 17333: setting learning rate to 2.116702686022588e-05.\n",
      "233/900 [======>.......................] - ETA: 5:32 - accuracy: 0.9839 - loss: 0.1904\n",
      "Batch 17334: setting learning rate to 2.116528440358022e-05.\n",
      "234/900 [======>.......................] - ETA: 5:31 - accuracy: 0.9840 - loss: 0.1898\n",
      "Batch 17335: setting learning rate to 2.1163541938059414e-05.\n",
      "235/900 [======>.......................] - ETA: 5:31 - accuracy: 0.9840 - loss: 0.1892\n",
      "Batch 17336: setting learning rate to 2.1161799463676754e-05.\n",
      "236/900 [======>.......................] - ETA: 5:30 - accuracy: 0.9831 - loss: 0.1899\n",
      "Batch 17337: setting learning rate to 2.1160056980445502e-05.\n",
      "237/900 [======>.......................] - ETA: 5:30 - accuracy: 0.9831 - loss: 0.1894\n",
      "Batch 17338: setting learning rate to 2.1158314488378928e-05.\n",
      "238/900 [======>.......................] - ETA: 5:29 - accuracy: 0.9832 - loss: 0.1889\n",
      "Batch 17339: setting learning rate to 2.11565719874903e-05.\n",
      "239/900 [======>.......................] - ETA: 5:29 - accuracy: 0.9833 - loss: 0.1884\n",
      "Batch 17340: setting learning rate to 2.11548294777929e-05.\n",
      "240/900 [=======>......................] - ETA: 5:28 - accuracy: 0.9833 - loss: 0.1878\n",
      "Batch 17341: setting learning rate to 2.115308695929999e-05.\n",
      "241/900 [=======>......................] - ETA: 5:28 - accuracy: 0.9834 - loss: 0.1872\n",
      "Batch 17342: setting learning rate to 2.1151344432024844e-05.\n",
      "242/900 [=======>......................] - ETA: 5:27 - accuracy: 0.9835 - loss: 0.1867\n",
      "Batch 17343: setting learning rate to 2.1149601895980733e-05.\n",
      "243/900 [=======>......................] - ETA: 5:26 - accuracy: 0.9835 - loss: 0.1861\n",
      "Batch 17344: setting learning rate to 2.1147859351180936e-05.\n",
      "244/900 [=======>......................] - ETA: 5:26 - accuracy: 0.9836 - loss: 0.1855\n",
      "Batch 17345: setting learning rate to 2.1146116797638717e-05.\n",
      "245/900 [=======>......................] - ETA: 5:25 - accuracy: 0.9837 - loss: 0.1870\n",
      "Batch 17346: setting learning rate to 2.1144374235367348e-05.\n",
      "246/900 [=======>......................] - ETA: 5:25 - accuracy: 0.9837 - loss: 0.1864\n",
      "Batch 17347: setting learning rate to 2.1142631664380106e-05.\n",
      "247/900 [=======>......................] - ETA: 5:24 - accuracy: 0.9838 - loss: 0.1860\n",
      "Batch 17348: setting learning rate to 2.114088908469025e-05.\n",
      "248/900 [=======>......................] - ETA: 5:24 - accuracy: 0.9839 - loss: 0.1878\n",
      "Batch 17349: setting learning rate to 2.1139146496311066e-05.\n",
      "249/900 [=======>......................] - ETA: 5:23 - accuracy: 0.9839 - loss: 0.1872\n",
      "Batch 17350: setting learning rate to 2.1137403899255822e-05.\n",
      "250/900 [=======>......................] - ETA: 5:23 - accuracy: 0.9840 - loss: 0.1868\n",
      "Batch 17351: setting learning rate to 2.1135661293537796e-05.\n",
      "251/900 [=======>......................] - ETA: 5:22 - accuracy: 0.9841 - loss: 0.1863\n",
      "Batch 17352: setting learning rate to 2.113391867917024e-05.\n",
      "252/900 [=======>......................] - ETA: 5:22 - accuracy: 0.9841 - loss: 0.1859\n",
      "Batch 17353: setting learning rate to 2.113217605616645e-05.\n",
      "253/900 [=======>......................] - ETA: 5:21 - accuracy: 0.9842 - loss: 0.1855\n",
      "Batch 17354: setting learning rate to 2.113043342453969e-05.\n",
      "254/900 [=======>......................] - ETA: 5:21 - accuracy: 0.9843 - loss: 0.1850\n",
      "Batch 17355: setting learning rate to 2.1128690784303228e-05.\n",
      "255/900 [=======>......................] - ETA: 5:20 - accuracy: 0.9843 - loss: 0.1844\n",
      "Batch 17356: setting learning rate to 2.1126948135470334e-05.\n",
      "256/900 [=======>......................] - ETA: 5:20 - accuracy: 0.9844 - loss: 0.1840\n",
      "Batch 17357: setting learning rate to 2.1125205478054295e-05.\n",
      "257/900 [=======>......................] - ETA: 5:19 - accuracy: 0.9844 - loss: 0.1836\n",
      "Batch 17358: setting learning rate to 2.1123462812068364e-05.\n",
      "258/900 [=======>......................] - ETA: 5:19 - accuracy: 0.9845 - loss: 0.1832\n",
      "Batch 17359: setting learning rate to 2.112172013752583e-05.\n",
      "259/900 [=======>......................] - ETA: 5:18 - accuracy: 0.9846 - loss: 0.1828\n",
      "Batch 17360: setting learning rate to 2.1119977454439954e-05.\n",
      "260/900 [=======>......................] - ETA: 5:18 - accuracy: 0.9846 - loss: 0.1823\n",
      "Batch 17361: setting learning rate to 2.1118234762824025e-05.\n",
      "261/900 [=======>......................] - ETA: 5:17 - accuracy: 0.9847 - loss: 0.1818\n",
      "Batch 17362: setting learning rate to 2.111649206269129e-05.\n",
      "262/900 [=======>......................] - ETA: 5:17 - accuracy: 0.9847 - loss: 0.1812\n",
      "Batch 17363: setting learning rate to 2.1114749354055046e-05.\n",
      "263/900 [=======>......................] - ETA: 5:16 - accuracy: 0.9848 - loss: 0.1807\n",
      "Batch 17364: setting learning rate to 2.1113006636928558e-05.\n",
      "264/900 [=======>......................] - ETA: 5:16 - accuracy: 0.9848 - loss: 0.1803\n",
      "Batch 17365: setting learning rate to 2.1111263911325097e-05.\n",
      "265/900 [=======>......................] - ETA: 5:15 - accuracy: 0.9849 - loss: 0.1799\n",
      "Batch 17366: setting learning rate to 2.110952117725793e-05.\n",
      "266/900 [=======>......................] - ETA: 5:14 - accuracy: 0.9850 - loss: 0.1795\n",
      "Batch 17367: setting learning rate to 2.1107778434740346e-05.\n",
      "267/900 [=======>......................] - ETA: 5:14 - accuracy: 0.9850 - loss: 0.1791\n",
      "Batch 17368: setting learning rate to 2.1106035683785605e-05.\n",
      "268/900 [=======>......................] - ETA: 5:13 - accuracy: 0.9851 - loss: 0.1786\n",
      "Batch 17369: setting learning rate to 2.1104292924406983e-05.\n",
      "269/900 [=======>......................] - ETA: 5:13 - accuracy: 0.9851 - loss: 0.1782\n",
      "Batch 17370: setting learning rate to 2.1102550156617757e-05.\n",
      "270/900 [========>.....................] - ETA: 5:12 - accuracy: 0.9852 - loss: 0.1778\n",
      "Batch 17371: setting learning rate to 2.11008073804312e-05.\n",
      "271/900 [========>.....................] - ETA: 5:12 - accuracy: 0.9852 - loss: 0.1774\n",
      "Batch 17372: setting learning rate to 2.1099064595860584e-05.\n",
      "272/900 [========>.....................] - ETA: 5:11 - accuracy: 0.9853 - loss: 0.1769\n",
      "Batch 17373: setting learning rate to 2.1097321802919176e-05.\n",
      "273/900 [========>.....................] - ETA: 5:11 - accuracy: 0.9853 - loss: 0.1764\n",
      "Batch 17374: setting learning rate to 2.1095579001620267e-05.\n",
      "274/900 [========>.....................] - ETA: 5:10 - accuracy: 0.9854 - loss: 0.1760\n",
      "Batch 17375: setting learning rate to 2.1093836191977114e-05.\n",
      "275/900 [========>.....................] - ETA: 5:10 - accuracy: 0.9855 - loss: 0.1755\n",
      "Batch 17376: setting learning rate to 2.109209337400299e-05.\n",
      "276/900 [========>.....................] - ETA: 5:09 - accuracy: 0.9855 - loss: 0.1751\n",
      "Batch 17377: setting learning rate to 2.109035054771118e-05.\n",
      "277/900 [========>.....................] - ETA: 5:09 - accuracy: 0.9856 - loss: 0.1747\n",
      "Batch 17378: setting learning rate to 2.108860771311496e-05.\n",
      "278/900 [========>.....................] - ETA: 5:08 - accuracy: 0.9856 - loss: 0.1742\n",
      "Batch 17379: setting learning rate to 2.1086864870227587e-05.\n",
      "279/900 [========>.....................] - ETA: 5:08 - accuracy: 0.9857 - loss: 0.1737\n",
      "Batch 17380: setting learning rate to 2.108512201906235e-05.\n",
      "280/900 [========>.....................] - ETA: 5:07 - accuracy: 0.9857 - loss: 0.1734\n",
      "Batch 17381: setting learning rate to 2.1083379159632522e-05.\n",
      "281/900 [========>.....................] - ETA: 5:07 - accuracy: 0.9858 - loss: 0.1730\n",
      "Batch 17382: setting learning rate to 2.108163629195137e-05.\n",
      "282/900 [========>.....................] - ETA: 5:06 - accuracy: 0.9858 - loss: 0.1728\n",
      "Batch 17383: setting learning rate to 2.1079893416032164e-05.\n",
      "283/900 [========>.....................] - ETA: 5:05 - accuracy: 0.9859 - loss: 0.1723\n",
      "Batch 17384: setting learning rate to 2.1078150531888194e-05.\n",
      "284/900 [========>.....................] - ETA: 5:05 - accuracy: 0.9850 - loss: 0.1731\n",
      "Batch 17385: setting learning rate to 2.1076407639532724e-05.\n",
      "285/900 [========>.....................] - ETA: 5:04 - accuracy: 0.9851 - loss: 0.1727\n",
      "Batch 17386: setting learning rate to 2.1074664738979026e-05.\n",
      "286/900 [========>.....................] - ETA: 5:04 - accuracy: 0.9851 - loss: 0.1724\n",
      "Batch 17387: setting learning rate to 2.107292183024038e-05.\n",
      "287/900 [========>.....................] - ETA: 5:03 - accuracy: 0.9852 - loss: 0.1720\n",
      "Batch 17388: setting learning rate to 2.1071178913330062e-05.\n",
      "288/900 [========>.....................] - ETA: 5:03 - accuracy: 0.9852 - loss: 0.1715\n",
      "Batch 17389: setting learning rate to 2.1069435988261343e-05.\n",
      "289/900 [========>.....................] - ETA: 5:02 - accuracy: 0.9844 - loss: 0.1763\n",
      "Batch 17390: setting learning rate to 2.1067693055047495e-05.\n",
      "290/900 [========>.....................] - ETA: 5:02 - accuracy: 0.9845 - loss: 0.1758\n",
      "Batch 17391: setting learning rate to 2.1065950113701804e-05.\n",
      "291/900 [========>.....................] - ETA: 5:01 - accuracy: 0.9845 - loss: 0.1755\n",
      "Batch 17392: setting learning rate to 2.106420716423753e-05.\n",
      "292/900 [========>.....................] - ETA: 5:01 - accuracy: 0.9846 - loss: 0.1750\n",
      "Batch 17393: setting learning rate to 2.1062464206667957e-05.\n",
      "293/900 [========>.....................] - ETA: 5:00 - accuracy: 0.9846 - loss: 0.1746\n",
      "Batch 17394: setting learning rate to 2.1060721241006353e-05.\n",
      "294/900 [========>.....................] - ETA: 5:00 - accuracy: 0.9847 - loss: 0.1742\n",
      "Batch 17395: setting learning rate to 2.1058978267266003e-05.\n",
      "295/900 [========>.....................] - ETA: 4:59 - accuracy: 0.9847 - loss: 0.1739\n",
      "Batch 17396: setting learning rate to 2.105723528546017e-05.\n",
      "296/900 [========>.....................] - ETA: 4:59 - accuracy: 0.9848 - loss: 0.1735\n",
      "Batch 17397: setting learning rate to 2.105549229560214e-05.\n",
      "297/900 [========>.....................] - ETA: 4:58 - accuracy: 0.9848 - loss: 0.1732\n",
      "Batch 17398: setting learning rate to 2.1053749297705185e-05.\n",
      "298/900 [========>.....................] - ETA: 4:58 - accuracy: 0.9849 - loss: 0.1729\n",
      "Batch 17399: setting learning rate to 2.1052006291782577e-05.\n",
      "299/900 [========>.....................] - ETA: 4:57 - accuracy: 0.9849 - loss: 0.1725\n",
      "Batch 17400: setting learning rate to 2.1050263277847586e-05.\n",
      "300/900 [=========>....................] - ETA: 4:57 - accuracy: 0.9850 - loss: 0.1722\n",
      "Batch 17401: setting learning rate to 2.1048520255913507e-05.\n",
      "301/900 [=========>....................] - ETA: 4:56 - accuracy: 0.9850 - loss: 0.1719\n",
      "Batch 17402: setting learning rate to 2.1046777225993588e-05.\n",
      "302/900 [=========>....................] - ETA: 4:56 - accuracy: 0.9851 - loss: 0.1716\n",
      "Batch 17403: setting learning rate to 2.1045034188101127e-05.\n",
      "303/900 [=========>....................] - ETA: 4:55 - accuracy: 0.9843 - loss: 0.1720\n",
      "Batch 17404: setting learning rate to 2.1043291142249386e-05.\n",
      "304/900 [=========>....................] - ETA: 4:55 - accuracy: 0.9844 - loss: 0.1716\n",
      "Batch 17405: setting learning rate to 2.1041548088451652e-05.\n",
      "305/900 [=========>....................] - ETA: 4:54 - accuracy: 0.9844 - loss: 0.1712\n",
      "Batch 17406: setting learning rate to 2.103980502672119e-05.\n",
      "306/900 [=========>....................] - ETA: 4:54 - accuracy: 0.9845 - loss: 0.1709\n",
      "Batch 17407: setting learning rate to 2.103806195707128e-05.\n",
      "307/900 [=========>....................] - ETA: 4:53 - accuracy: 0.9845 - loss: 0.1704\n",
      "Batch 17408: setting learning rate to 2.1036318879515204e-05.\n",
      "308/900 [=========>....................] - ETA: 4:53 - accuracy: 0.9846 - loss: 0.1700\n",
      "Batch 17409: setting learning rate to 2.103457579406622e-05.\n",
      "309/900 [=========>....................] - ETA: 4:52 - accuracy: 0.9846 - loss: 0.1697\n",
      "Batch 17410: setting learning rate to 2.1032832700737625e-05.\n",
      "310/900 [=========>....................] - ETA: 4:51 - accuracy: 0.9847 - loss: 0.1692\n",
      "Batch 17411: setting learning rate to 2.103108959954268e-05.\n",
      "311/900 [=========>....................] - ETA: 4:51 - accuracy: 0.9847 - loss: 0.1689\n",
      "Batch 17412: setting learning rate to 2.1029346490494677e-05.\n",
      "312/900 [=========>....................] - ETA: 4:50 - accuracy: 0.9848 - loss: 0.1685\n",
      "Batch 17413: setting learning rate to 2.1027603373606866e-05.\n",
      "313/900 [=========>....................] - ETA: 4:50 - accuracy: 0.9848 - loss: 0.1681\n",
      "Batch 17414: setting learning rate to 2.1025860248892546e-05.\n",
      "314/900 [=========>....................] - ETA: 4:49 - accuracy: 0.9849 - loss: 0.1678\n",
      "Batch 17415: setting learning rate to 2.1024117116364984e-05.\n",
      "315/900 [=========>....................] - ETA: 4:49 - accuracy: 0.9849 - loss: 0.1675\n",
      "Batch 17416: setting learning rate to 2.1022373976037458e-05.\n",
      "316/900 [=========>....................] - ETA: 4:48 - accuracy: 0.9850 - loss: 0.1671\n",
      "Batch 17417: setting learning rate to 2.1020630827923242e-05.\n",
      "317/900 [=========>....................] - ETA: 4:48 - accuracy: 0.9850 - loss: 0.1667\n",
      "Batch 17418: setting learning rate to 2.101888767203562e-05.\n",
      "318/900 [=========>....................] - ETA: 4:47 - accuracy: 0.9851 - loss: 0.1665\n",
      "Batch 17419: setting learning rate to 2.1017144508387853e-05.\n",
      "319/900 [=========>....................] - ETA: 4:47 - accuracy: 0.9851 - loss: 0.1661\n",
      "Batch 17420: setting learning rate to 2.1015401336993232e-05.\n",
      "320/900 [=========>....................] - ETA: 4:46 - accuracy: 0.9852 - loss: 0.1658\n",
      "Batch 17421: setting learning rate to 2.1013658157865024e-05.\n",
      "321/900 [=========>....................] - ETA: 4:46 - accuracy: 0.9852 - loss: 0.1654\n",
      "Batch 17422: setting learning rate to 2.1011914971016517e-05.\n",
      "322/900 [=========>....................] - ETA: 4:45 - accuracy: 0.9852 - loss: 0.1650\n",
      "Batch 17423: setting learning rate to 2.1010171776460976e-05.\n",
      "323/900 [=========>....................] - ETA: 4:45 - accuracy: 0.9845 - loss: 0.1653\n",
      "Batch 17424: setting learning rate to 2.1008428574211682e-05.\n",
      "324/900 [=========>....................] - ETA: 4:44 - accuracy: 0.9846 - loss: 0.1649\n",
      "Batch 17425: setting learning rate to 2.100668536428192e-05.\n",
      "325/900 [=========>....................] - ETA: 4:44 - accuracy: 0.9846 - loss: 0.1646\n",
      "Batch 17426: setting learning rate to 2.100494214668495e-05.\n",
      "326/900 [=========>....................] - ETA: 4:43 - accuracy: 0.9847 - loss: 0.1643\n",
      "Batch 17427: setting learning rate to 2.100319892143406e-05.\n",
      "327/900 [=========>....................] - ETA: 4:43 - accuracy: 0.9847 - loss: 0.1640\n",
      "Batch 17428: setting learning rate to 2.100145568854252e-05.\n",
      "328/900 [=========>....................] - ETA: 4:42 - accuracy: 0.9848 - loss: 0.1637\n",
      "Batch 17429: setting learning rate to 2.099971244802362e-05.\n",
      "329/900 [=========>....................] - ETA: 4:42 - accuracy: 0.9848 - loss: 0.1634\n",
      "Batch 17430: setting learning rate to 2.099796919989062e-05.\n",
      "330/900 [==========>...................] - ETA: 4:41 - accuracy: 0.9848 - loss: 0.1631\n",
      "Batch 17431: setting learning rate to 2.099622594415681e-05.\n",
      "331/900 [==========>...................] - ETA: 4:41 - accuracy: 0.9849 - loss: 0.1627\n",
      "Batch 17432: setting learning rate to 2.0994482680835465e-05.\n",
      "332/900 [==========>...................] - ETA: 4:40 - accuracy: 0.9849 - loss: 0.1624\n",
      "Batch 17433: setting learning rate to 2.0992739409939857e-05.\n",
      "333/900 [==========>...................] - ETA: 4:40 - accuracy: 0.9850 - loss: 0.1622\n",
      "Batch 17434: setting learning rate to 2.099099613148326e-05.\n",
      "334/900 [==========>...................] - ETA: 4:39 - accuracy: 0.9850 - loss: 0.1620\n",
      "Batch 17435: setting learning rate to 2.0989252845478968e-05.\n",
      "335/900 [==========>...................] - ETA: 4:39 - accuracy: 0.9851 - loss: 0.1617\n",
      "Batch 17436: setting learning rate to 2.0987509551940238e-05.\n",
      "336/900 [==========>...................] - ETA: 4:38 - accuracy: 0.9851 - loss: 0.1614\n",
      "Batch 17437: setting learning rate to 2.0985766250880366e-05.\n",
      "337/900 [==========>...................] - ETA: 4:38 - accuracy: 0.9852 - loss: 0.1611\n",
      "Batch 17438: setting learning rate to 2.0984022942312608e-05.\n",
      "338/900 [==========>...................] - ETA: 4:37 - accuracy: 0.9852 - loss: 0.1608\n",
      "Batch 17439: setting learning rate to 2.098227962625027e-05.\n",
      "339/900 [==========>...................] - ETA: 4:37 - accuracy: 0.9853 - loss: 0.1605\n",
      "Batch 17440: setting learning rate to 2.09805363027066e-05.\n",
      "340/900 [==========>...................] - ETA: 4:36 - accuracy: 0.9853 - loss: 0.1603\n",
      "Batch 17441: setting learning rate to 2.09787929716949e-05.\n",
      "341/900 [==========>...................] - ETA: 4:36 - accuracy: 0.9853 - loss: 0.1600\n",
      "Batch 17442: setting learning rate to 2.097704963322843e-05.\n",
      "342/900 [==========>...................] - ETA: 4:35 - accuracy: 0.9854 - loss: 0.1598\n",
      "Batch 17443: setting learning rate to 2.0975306287320473e-05.\n",
      "343/900 [==========>...................] - ETA: 4:35 - accuracy: 0.9854 - loss: 0.1596\n",
      "Batch 17444: setting learning rate to 2.0973562933984316e-05.\n",
      "344/900 [==========>...................] - ETA: 4:34 - accuracy: 0.9855 - loss: 0.1593\n",
      "Batch 17445: setting learning rate to 2.0971819573233227e-05.\n",
      "345/900 [==========>...................] - ETA: 4:33 - accuracy: 0.9855 - loss: 0.1591\n",
      "Batch 17446: setting learning rate to 2.0970076205080482e-05.\n",
      "346/900 [==========>...................] - ETA: 4:33 - accuracy: 0.9855 - loss: 0.1588\n",
      "Batch 17447: setting learning rate to 2.0968332829539367e-05.\n",
      "347/900 [==========>...................] - ETA: 4:32 - accuracy: 0.9856 - loss: 0.1586\n",
      "Batch 17448: setting learning rate to 2.0966589446623153e-05.\n",
      "348/900 [==========>...................] - ETA: 4:32 - accuracy: 0.9856 - loss: 0.1583\n",
      "Batch 17449: setting learning rate to 2.096484605634513e-05.\n",
      "349/900 [==========>...................] - ETA: 4:31 - accuracy: 0.9857 - loss: 0.1582\n",
      "Batch 17450: setting learning rate to 2.0963102658718558e-05.\n",
      "350/900 [==========>...................] - ETA: 4:31 - accuracy: 0.9857 - loss: 0.1580\n",
      "Batch 17451: setting learning rate to 2.0961359253756726e-05.\n",
      "351/900 [==========>...................] - ETA: 4:30 - accuracy: 0.9858 - loss: 0.1577\n",
      "Batch 17452: setting learning rate to 2.095961584147292e-05.\n",
      "352/900 [==========>...................] - ETA: 4:30 - accuracy: 0.9858 - loss: 0.1574\n",
      "Batch 17453: setting learning rate to 2.09578724218804e-05.\n",
      "353/900 [==========>...................] - ETA: 4:29 - accuracy: 0.9858 - loss: 0.1572\n",
      "Batch 17454: setting learning rate to 2.0956128994992457e-05.\n",
      "354/900 [==========>...................] - ETA: 4:29 - accuracy: 0.9859 - loss: 0.1568\n",
      "Batch 17455: setting learning rate to 2.095438556082236e-05.\n",
      "355/900 [==========>...................] - ETA: 4:28 - accuracy: 0.9859 - loss: 0.1565\n",
      "Batch 17456: setting learning rate to 2.0952642119383407e-05.\n",
      "356/900 [==========>...................] - ETA: 4:28 - accuracy: 0.9860 - loss: 0.1562\n",
      "Batch 17457: setting learning rate to 2.0950898670688854e-05.\n",
      "357/900 [==========>...................] - ETA: 4:27 - accuracy: 0.9860 - loss: 0.1559\n",
      "Batch 17458: setting learning rate to 2.094915521475199e-05.\n",
      "358/900 [==========>...................] - ETA: 4:27 - accuracy: 0.9860 - loss: 0.1556\n",
      "Batch 17459: setting learning rate to 2.0947411751586095e-05.\n",
      "359/900 [==========>...................] - ETA: 4:26 - accuracy: 0.9861 - loss: 0.1553\n",
      "Batch 17460: setting learning rate to 2.094566828120444e-05.\n",
      "360/900 [===========>..................] - ETA: 4:26 - accuracy: 0.9861 - loss: 0.1551\n",
      "Batch 17461: setting learning rate to 2.0943924803620313e-05.\n",
      "361/900 [===========>..................] - ETA: 4:25 - accuracy: 0.9861 - loss: 0.1548\n",
      "Batch 17462: setting learning rate to 2.094218131884699e-05.\n",
      "362/900 [===========>..................] - ETA: 4:25 - accuracy: 0.9862 - loss: 0.1545\n",
      "Batch 17463: setting learning rate to 2.0940437826897746e-05.\n",
      "363/900 [===========>..................] - ETA: 4:24 - accuracy: 0.9862 - loss: 0.1543\n",
      "Batch 17464: setting learning rate to 2.0938694327785864e-05.\n",
      "364/900 [===========>..................] - ETA: 4:24 - accuracy: 0.9863 - loss: 0.1540\n",
      "Batch 17465: setting learning rate to 2.093695082152462e-05.\n",
      "365/900 [===========>..................] - ETA: 4:23 - accuracy: 0.9863 - loss: 0.1537\n",
      "Batch 17466: setting learning rate to 2.0935207308127298e-05.\n",
      "366/900 [===========>..................] - ETA: 4:23 - accuracy: 0.9863 - loss: 0.1534\n",
      "Batch 17467: setting learning rate to 2.093346378760717e-05.\n",
      "367/900 [===========>..................] - ETA: 4:22 - accuracy: 0.9864 - loss: 0.1532\n",
      "Batch 17468: setting learning rate to 2.0931720259977518e-05.\n",
      "368/900 [===========>..................] - ETA: 4:22 - accuracy: 0.9864 - loss: 0.1529\n",
      "Batch 17469: setting learning rate to 2.0929976725251628e-05.\n",
      "369/900 [===========>..................] - ETA: 4:21 - accuracy: 0.9864 - loss: 0.1526\n",
      "Batch 17470: setting learning rate to 2.0928233183442767e-05.\n",
      "370/900 [===========>..................] - ETA: 4:21 - accuracy: 0.9865 - loss: 0.1523\n",
      "Batch 17471: setting learning rate to 2.0926489634564223e-05.\n",
      "371/900 [===========>..................] - ETA: 4:20 - accuracy: 0.9858 - loss: 0.1526\n",
      "Batch 17472: setting learning rate to 2.0924746078629275e-05.\n",
      "372/900 [===========>..................] - ETA: 4:20 - accuracy: 0.9859 - loss: 0.1523\n",
      "Batch 17473: setting learning rate to 2.09230025156512e-05.\n",
      "373/900 [===========>..................] - ETA: 4:19 - accuracy: 0.9859 - loss: 0.1520\n",
      "Batch 17474: setting learning rate to 2.092125894564327e-05.\n",
      "374/900 [===========>..................] - ETA: 4:19 - accuracy: 0.9853 - loss: 0.1551\n",
      "Batch 17475: setting learning rate to 2.091951536861878e-05.\n",
      "375/900 [===========>..................] - ETA: 4:18 - accuracy: 0.9853 - loss: 0.1548\n",
      "Batch 17476: setting learning rate to 2.0917771784591002e-05.\n",
      "376/900 [===========>..................] - ETA: 4:18 - accuracy: 0.9854 - loss: 0.1549\n",
      "Batch 17477: setting learning rate to 2.0916028193573216e-05.\n",
      "377/900 [===========>..................] - ETA: 4:17 - accuracy: 0.9854 - loss: 0.1547\n",
      "Batch 17478: setting learning rate to 2.0914284595578695e-05.\n",
      "378/900 [===========>..................] - ETA: 4:17 - accuracy: 0.9854 - loss: 0.1545\n",
      "Batch 17479: setting learning rate to 2.0912540990620732e-05.\n",
      "379/900 [===========>..................] - ETA: 4:16 - accuracy: 0.9855 - loss: 0.1542\n",
      "Batch 17480: setting learning rate to 2.0910797378712596e-05.\n",
      "380/900 [===========>..................] - ETA: 4:16 - accuracy: 0.9855 - loss: 0.1540\n",
      "Batch 17481: setting learning rate to 2.090905375986757e-05.\n",
      "381/900 [===========>..................] - ETA: 4:15 - accuracy: 0.9856 - loss: 0.1538\n",
      "Batch 17482: setting learning rate to 2.0907310134098932e-05.\n",
      "382/900 [===========>..................] - ETA: 4:15 - accuracy: 0.9856 - loss: 0.1535\n",
      "Batch 17483: setting learning rate to 2.0905566501419973e-05.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bf9acd252a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m run_training(dropout = params['dropout'], lr_rate = params['learning_rate'], \n\u001b[1;32m      2\u001b[0m              \u001b[0marchitecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'architecture'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m              epochs = params['epochs'], frozen_base=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-09a2616518cb>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(dropout, lr_rate, architecture, batch, epochs, frozen_base)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bf9b1f58b987>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, val_data, epochs, class_weights, architecture, lr_rate)\u001b[0m\n\u001b[1;32m     78\u001b[0m               \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# maximum size for the generator queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0;31m# maximum number of processes to spin up when using process-based threading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m               use_multiprocessing=False)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Make a saved models folder if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_training(dropout = params['dropout'], lr_rate = params['learning_rate'], \n",
    "             architecture = params['architecture'], batch = params['batch_size'],\n",
    "             epochs = params['epochs'], frozen_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 20, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 1280)          4049564   \n",
      "_________________________________________________________________\n",
      "transformer_block_6 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_7 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_8 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_9 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_10 (Transf (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_11 (Transf (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 512)               655872    \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 52,029,470\n",
      "Trainable params: 51,987,454\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Save model to bucket from highest val acc weights\n",
    "save_model_from_best_weights(dropout = 0.3, lr_rate = 0.00004, architecture = 'transformer', frozen_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
