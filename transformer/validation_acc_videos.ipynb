{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding validation accuracy for temporal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import TransformCode as tc\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data generators\n",
    "The parameters are specified manually as they are consistent for each config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['authentic', 'fake']\n",
      "Total data: 2 classes for 700 files for train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import VideoFrameGenerator\n",
    "\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[-1] for i in glob.glob('../restructured-all-faces/home/jupyter/restructured_data/validation/*')]\n",
    "classes.sort() \n",
    "print(classes)\n",
    "\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 20\n",
    "BS = 4\n",
    "\n",
    "glob_pattern_test = '../restructured-all-faces/home/jupyter/restructured_data/validation/{classname}/*'\n",
    "\n",
    "data_aug_test = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "test_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_test,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=BS,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_dir = '../all_faces_bucket/'\n",
    "configname = 'config1011'\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'TransformerBlock': tc.TransformerBlock, 'MultiHeadSelfAttention' : tc.MultiHeadSelfAttention}):\n",
    "    model = tf.keras.models.load_model(af_dir + 'trained_models/saved_models/' + configname + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the evaluation\n",
    "Instead of predicting on the validation set, the model is evaluated directly as this model does not require frame predictions to be transferred to video predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(test_data,\n",
    "                           # class_weight=class_weights,\n",
    "                          #  initial_epoch=0,\n",
    "                            verbose=1,\n",
    "                            max_queue_size=100,                # maximum size for the generator queue\n",
    "                            workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "                            use_multiprocessing=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
