{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from specified config files\n",
    "        -- Built for any deepfakes dataset with files structured as /train/{class}/{video_filename}/{frames_of_video}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/qubvel/efficientnet\n",
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import TransformCode as tc\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "import custom_aug as deepaug\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'transformer',\n",
       " 'epochs': 40,\n",
       " 'batch_size': 4,\n",
       " 'learning_rate_type': 'cosine_decay',\n",
       " 'learning_rate': 4e-05,\n",
       " 'patience': 20,\n",
       " 'weight_initialisation': 'imagenet',\n",
       " 'optimiser': 'adam',\n",
       " 'momentum': 0.99,\n",
       " 'nesterov': 'False',\n",
       " 'label_smoothing': 0.01,\n",
       " 'dropout': 0.3,\n",
       " 'target_size': 224,\n",
       " 'class_weights': 'True',\n",
       " 'warmup_epochs': 0,\n",
       " 'weight_decay': 2.5e-05}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify config file\n",
    "config_number=103111\n",
    "config_file='config{}'.format(config_number)\n",
    "def obtain_configs(number):\n",
    "    '''Extracts hyperparameters from config file given the config file number.'''\n",
    "    with open('../configs/config{}.json'.format(number)) as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = obtain_configs(config_number)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the video generators\n",
    "The above code is very similar to the other training the code. Below the custom video generator is imported and opened. For the inner workings of this generator, the source code is in ```videoFrameGenerator.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['authentic', 'fake']\n"
     ]
    }
   ],
   "source": [
    "import VideoFrameGenerator\n",
    "\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[-1] for i in glob.glob('../restructured-all-faces/home/jupyter/restructured_data/train/*')]\n",
    "classes.sort() # actually already within source code\n",
    "print(classes)\n",
    "\n",
    "# Global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 20\n",
    "\n",
    "# Pattern to get videos and classes\n",
    "glob_pattern_train = '../restructured-all-faces/home/jupyter/restructured_data/train/{classname}/*'\n",
    "glob_pattern_val = '../restructured-all-faces/home/jupyter/restructured_data/validation/{classname}/*'\n",
    "\n",
    "# for data augmentation\n",
    "data_aug_train = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            fill_mode='nearest',\n",
    "            brightness_range=None,\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            preprocessing_function=deepaug.joint_function,\n",
    "            data_format=None,)\n",
    "\n",
    "data_aug_val = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture = 'lstm',frozen_base = True):\n",
    "    \"\"\"This functions builds the model, with either lstm or transformer specified.\n",
    "    It first builds a CNN with an efficientnet convolutional base, and then creates a\n",
    "    TimeDistributed layer, which is large and heavy to process, containing the output of the final layer\n",
    "    of the headless CNN. This is then fed into either the\n",
    "    lstm or Transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    frames = 20\n",
    "    channels = 3\n",
    "    rows = 224\n",
    "    columns = 224\n",
    "    embed_dim = 1280\n",
    "    num_heads = 8\n",
    "    ff_dim = 512\n",
    "    \n",
    "    # embed_dim is Embedding size for each token\n",
    "    # num_heads is number of attention heads\n",
    "    # ff_dim is hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    video = tf.keras.layers.Input(shape=(frames,\n",
    "                         rows,\n",
    "                         columns,\n",
    "                         channels,))\n",
    "        \n",
    "    from efficientnet.tfkeras import EfficientNetB0\n",
    "    conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "            input_shape=(224,224,3))\n",
    "    cnn_out = GlobalAveragePooling2D()(conv_base.output)\n",
    "    cnn = tf.keras.Model(inputs=conv_base.input, outputs=cnn_out)\n",
    "    \n",
    "    encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)\n",
    "        \n",
    "    if architecture == 'lstm':\n",
    "        \n",
    "        encoded_sequence = tf.keras.layers.LSTM(2048, dropout = dropout)(encoded_frames)\n",
    "        hidden_layer = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(encoded_sequence)\n",
    "        hidden_layer2 = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(hidden_layer)\n",
    "        dropoutlstm = tf.keras.layers.Dropout(dropout)(hidden_layer2)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(dropoutlstm)\n",
    "        model = Model([video], outputs)\n",
    "    \n",
    "        \n",
    "    if architecture == 'transformer':\n",
    "        \n",
    "        transformer_block1 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block1(encoded_frames)\n",
    "        transformer_block2 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block2(x)\n",
    "        transformer_block3 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block3(x)\n",
    "        transformer_block4 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block4(x)\n",
    "        transformer_block5 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block5(x)\n",
    "        transformer_block6 = tc.TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        x = transformer_block6(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(x)\n",
    "        x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(params['weight_decay']))(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model([video], outputs)\n",
    "                \n",
    "    if frozen_base:\n",
    "        cnn.trainable = False # freeze the convolutional base\n",
    "        \n",
    "    else: \n",
    "        cnn.trainable = True    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr_rate,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.98,\n",
    "                      epsilon=1e-09)\n",
    "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(label_smoothing=params['label_smoothing']),\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = [\"accuracy\"]) \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed.'''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/{}\".format(config_file)\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # If there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # Use weights from highest val acc\n",
    "            model.load_weights(path_to_weights + '/' + 'highest_val_acc.hdf5')\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture, frozen_base):\n",
    "    '''Takes the weights with the highest val accuracy and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    7. lr_rate: initial learning rate\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/{}'.format(config_file)):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/{}'.format(config_file))\n",
    "\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/{}/highest_val_acc.hdf5\".format(config_file)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/{}.csv'.format(config_file),\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=params['patience'])\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    if params['learning_rate_type']=='cosine_decay':\n",
    "        # Set learning rate config \n",
    "        sample_count = 3600 # number of training samples\n",
    "        epochs = epochs # total epochs - affects total steps (and hence speed of decay)\n",
    "        warmup_epoch = params['warmup_epochs'] # number of warmup epochs\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate_base = lr_rate\n",
    "        total_steps = int(epochs * sample_count / batch_size)\n",
    "\n",
    "        warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "\n",
    "        warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                            total_steps=total_steps,\n",
    "                                            warmup_learning_rate=0.0,\n",
    "                                            warmup_steps=warmup_steps,\n",
    "                                            hold_base_rate_steps=2,\n",
    "                                            verbose=0)\n",
    "\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es, warm_up_lr]\n",
    "        \n",
    "    elif params['learning_rate_type']=='constant':\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es]\n",
    "        \n",
    "    elif params['learning_rate_type']=='increasing':\n",
    "        pass\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs,\n",
    "              validation_data = val_data, \n",
    "              class_weight=class_weights,\n",
    "              callbacks=cb,\n",
    "              initial_epoch=0,                    # start training epoch - useful if continuing training\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')\n",
    "        \n",
    "    model.save_weights('../all_faces_bucket/trained_models/weights/{}/lastepoch.hdf5'.format(config_file)) \n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50, frozen_base=True):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    \n",
    "    # Create video frame generator\n",
    "    \n",
    "    train_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_train,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=batch,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_train)\n",
    "    \n",
    "    val_data = VideoFrameGenerator.VideoFrameGenerator(\n",
    "        classes=classes, \n",
    "        glob_pattern=glob_pattern_val,\n",
    "        nb_frames=NBFRAME,\n",
    "        shuffle=True,\n",
    "        batch_size=batch,\n",
    "        target_shape=SIZE,\n",
    "        nb_channel=CHANNELS,\n",
    "        transformation=data_aug_val)\n",
    "    # class_weights are manually inserted as they are precisely as in the original ff++ dataset\n",
    "    class_weights = {0: 4, 1: 1}\n",
    "    \n",
    "    if params['class_weights']=='True':\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate)\n",
    "    else:\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, None, architecture, lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(dropout = params['dropout'], lr_rate = params['learning_rate'], \n",
    "             architecture = params['architecture'], batch = params['batch_size'],\n",
    "             epochs = params['epochs'], frozen_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 20, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 1280)          4049564   \n",
      "_________________________________________________________________\n",
      "transformer_block_6 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_7 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_8 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_9 (Transfo (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_10 (Transf (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "transformer_block_11 (Transf (None, 20, 1280)          7876352   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 512)               655872    \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 52,029,470\n",
      "Trainable params: 51,987,454\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Save model to bucket from highest val acc weights\n",
    "save_model_from_best_weights(dropout = 0.3, lr_rate = 0.00004, architecture = 'transformer', frozen_base=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
