{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udRhv-d-2i8l"
   },
   "source": [
    "# Train preliminary models - Xception, ResNet, EfficentNet etc.\n",
    "       -- built for FF+ dataset with file structure as required by Keras' flow_from_directory method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "t7E1CjC9fqhq",
    "outputId": "abdef6eb-823c-4fb1-ad28-7521345d04fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 12 15:29:12 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P0    29W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# See available GPU RAM \n",
    "!nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !watch -n0.1 nvidia-smi # better way to see GPU utilisation\n",
    "# !htop # cpu threads and if they're all working\n",
    "# !pip3 install --no-cache-dir -I tensorflow==2.2 #Â use if no gpu is attached so code will run \n",
    "# !sudo kill -9 PID # clear GPU memory where 9 is PID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Llx-HRnYiWQU",
    "outputId": "6e6a3556-fbb1-4972-b046-8586183f768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "3DRA3QPDgLLR",
    "outputId": "ed171b89-378d-469d-a254-c291be71af4d"
   },
   "outputs": [],
   "source": [
    "# Required for EfficientNet\n",
    "# !pip install git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtO5vELz8i3-"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ7mWThq32JA"
   },
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture):\n",
    "    '''Builds a specified network with the selected dropout after the last dense layer.\n",
    "\n",
    "    Architectures that can be selected are:\n",
    "    vgg, xception, resnet50, mobilenet, efficientnet, densenet\n",
    "    \n",
    "    Optimiser is Adam, with a provided learning rate (lr_rate) and fixed\n",
    "    decay 1e-6, loss is traditionally categorical_crossentropy.'''\n",
    "\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "    if architecture=='xception':\n",
    "        from tensorflow.keras.applications.xception import Xception\n",
    "        conv_base = Xception(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture=='vgg':\n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='resnet50':\n",
    "        from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "        conv_base = ResNet50(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='mobilenet':\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture== 'efficientnet':\n",
    "        # EfficientNetB7 has the highest top-1 accuracy on imagenet\n",
    "        # among EfficientNextB{0:7}\n",
    "        from efficientnet.tfkeras import EfficientNetB0\n",
    "        conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "        \n",
    "    elif architecture== 'densenet':\n",
    "        from tensorflow.keras.applications.densenet import DenseNet201\n",
    "        conv_base = DenseNet201(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture not in ['vgg', 'xception', 'resnet50',\n",
    "                              'mobilenet', 'efficientnet', 'densenet']:\n",
    "        return \"An unknown network is specified\"\n",
    "    \n",
    "\n",
    "    outputconv_base = conv_base.output\n",
    "    t_flat = Flatten()(outputconv_base)\n",
    "    t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
    "    t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "    t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
    "    t_do = Dropout(dropout)(t_dense3)\n",
    "    predictions = Dense(2, activation= 'softmax')(t_do)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=predictions, name = 'model')\n",
    "\n",
    "#     conv_base.trainable = False # freeze the convolutional base\n",
    "    \n",
    "    # # Code below trains all layers without using any pretrained weights\n",
    "    #for layer in conv_base.layers:\n",
    "    #  layer.trainable = True\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate= lr_rate, decay=1e-6)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_rate)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen_train = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "#             width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "#             height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            brightness_range=[0.6, 1.4],\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "    \n",
    "    datagen_test = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen_train.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    val_data = datagen_test.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary train time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights\n",
    "\n",
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed. '''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/mobilenet_new\"\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # if there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # pick out accuracies out of file names\n",
    "            acc = [el[len(el)-10 : len(el)-5] for el in all_weights]\n",
    "            # get index of the first maximum accuracy\n",
    "            optimal_index = acc.index(max(acc))\n",
    "            # get the name of the file with optimal weights, load corresponding weights\n",
    "            optimal_weights = all_weights[optimal_index]\n",
    "            print(\"Loading\", path_to_weights + '/' + optimal_weights)\n",
    "            model.load_weights(path_to_weights + '/' + optimal_weights)\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture):\n",
    "    '''Takes the weights with the highest val accuracy and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/mobilenet_new_model_fine_tuned.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/mobilenet_new'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/mobilenet_new_fine_tuned')\n",
    "\n",
    "    # Save weights - below saves every epoch where there is improvement\n",
    "    # filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/mobilenet_new_fine_tuned/highest_val_acc.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/mobilenet_new_fine_tuned.csv',\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    # Set learning rate config \n",
    "    sample_count = 60000 # number of training samples\n",
    "    epochs = 50 # total epochs - affects total steps (and hence speed of decay)\n",
    "    warmup_epoch = 3 # number of warmup epochs\n",
    "    batch_size = train_data.batch_size\n",
    "    learning_rate_base = 0.00001\n",
    "    total_steps = int(epochs * sample_count / batch_size)\n",
    "    warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "    \n",
    "    warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warmup_steps,\n",
    "                                        hold_base_rate_steps=2,\n",
    "                                        verbose=0)\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses, checkpoint, csv_logger, es, warm_up_lr],\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')\n",
    "        \n",
    "    model.save_weights('../all_faces_bucket/trained_models/weights/mobilenet_new_fine_tunedlastepoch.hdf5') \n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/mobilenet_new_model_fine_tunedlastepoch.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIUZirJoxsdx"
   },
   "source": [
    "## Unifying Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYFsNbZMqYTv"
   },
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture)\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in8HHH594qtA"
   },
   "source": [
    "## Train Various Model Architectures\n",
    "Note: Make sure CPUs have enough memory for each batch eg. 1 core with 3.75GB RAM cant take batches larger than 32.  \n",
    "8 CPUs with 30GB RAM typically works well for batches of 256.\n",
    "\n",
    "In this model: 4 cores/8GB and T4 used - took approx 8 hours to train top dense layers and fine tune. \n",
    "\n",
    "Also note that while multiprocessing speeds up training, it interacts badly with Tensorflow and leads to deadlocks. To be on the safe side, set use_multiprocessing to False when training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz0ilB_F8F4g"
   },
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "colab_type": "code",
    "id": "_GZwsiNC7rkK",
    "outputId": "7b0348f4-e292-4cb7-af8f-49af0eda2638"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_accuracy improved from 0.98037 to 0.98047, saving model to ../all_faces_bucket/trained_models/weights/mobilenet_new_fine_tuned/highest_val_acc.hdf5\n",
      "1780/1780 [==============================] - 802s 451ms/step - accuracy: 0.9969 - loss: 0.0094 - val_accuracy: 0.9805 - val_loss: 0.1166\n",
      "Epoch 16/50\n",
      "\n",
      "Batch 26701: setting learning rate to 4.340433126285293e-06.\n",
      "   1/1780 [..............................] - ETA: 0s - accuracy: 1.0000 - loss: 0.0120\n",
      "Batch 26702: setting learning rate to 4.340079738450552e-06.\n",
      "   2/1780 [..............................] - ETA: 6:38 - accuracy: 1.0000 - loss: 0.0178\n",
      "Batch 26703: setting learning rate to 4.339726353970732e-06.\n",
      "   3/1780 [..............................] - ETA: 8:35 - accuracy: 1.0000 - loss: 0.0119\n",
      "Batch 26704: setting learning rate to 4.3393729728476345e-06.\n",
      "   4/1780 [..............................] - ETA: 9:38 - accuracy: 1.0000 - loss: 0.0090\n",
      "Batch 26705: setting learning rate to 4.339019595083051e-06.\n",
      "   5/1780 [..............................] - ETA: 10:36 - accuracy: 1.0000 - loss: 0.0075\n",
      "Batch 26706: setting learning rate to 4.338666220678779e-06.\n",
      "   6/1780 [..............................] - ETA: 10:53 - accuracy: 0.9974 - loss: 0.0104\n",
      "Batch 26707: setting learning rate to 4.338312849636616e-06.\n",
      "   7/1780 [..............................] - ETA: 11:17 - accuracy: 0.9978 - loss: 0.0104\n",
      "Batch 26708: setting learning rate to 4.337959481958361e-06.\n",
      "   8/1780 [..............................] - ETA: 11:31 - accuracy: 0.9980 - loss: 0.0147\n",
      "Batch 26709: setting learning rate to 4.337606117645803e-06.\n",
      "   9/1780 [..............................] - ETA: 11:42 - accuracy: 0.9983 - loss: 0.0133\n",
      "Batch 26710: setting learning rate to 4.337252756700746e-06.\n",
      "  10/1780 [..............................] - ETA: 11:47 - accuracy: 0.9984 - loss: 0.0119\n",
      "Batch 26711: setting learning rate to 4.336899399124982e-06.\n",
      "  11/1780 [..............................] - ETA: 11:49 - accuracy: 0.9986 - loss: 0.0160\n",
      "Batch 26712: setting learning rate to 4.33654604492031e-06.\n",
      "  12/1780 [..............................] - ETA: 11:52 - accuracy: 0.9987 - loss: 0.0150\n",
      "Batch 26713: setting learning rate to 4.336192694088526e-06.\n",
      "  13/1780 [..............................] - ETA: 11:55 - accuracy: 0.9988 - loss: 0.0153\n",
      "Batch 26714: setting learning rate to 4.335839346631424e-06.\n",
      "  14/1780 [..............................] - ETA: 11:59 - accuracy: 0.9978 - loss: 0.0151\n",
      "Batch 26715: setting learning rate to 4.335486002550804e-06.\n",
      "  15/1780 [..............................] - ETA: 12:02 - accuracy: 0.9979 - loss: 0.0141\n",
      "Batch 26716: setting learning rate to 4.335132661848458e-06.\n",
      "  16/1780 [..............................] - ETA: 12:04 - accuracy: 0.9980 - loss: 0.0133\n",
      "Batch 26717: setting learning rate to 4.334779324526186e-06.\n",
      "  17/1780 [..............................] - ETA: 12:03 - accuracy: 0.9982 - loss: 0.0127\n",
      "Batch 26718: setting learning rate to 4.3344259905857826e-06.\n",
      "  18/1780 [..............................] - ETA: 12:04 - accuracy: 0.9983 - loss: 0.0120\n",
      "Batch 26719: setting learning rate to 4.3340726600290465e-06.\n",
      "  19/1780 [..............................] - ETA: 12:09 - accuracy: 0.9984 - loss: 0.0114\n",
      "Batch 26720: setting learning rate to 4.33371933285777e-06.\n",
      "  20/1780 [..............................] - ETA: 12:10 - accuracy: 0.9977 - loss: 0.0115\n",
      "Batch 26721: setting learning rate to 4.333366009073751e-06.\n",
      "  21/1780 [..............................] - ETA: 12:11 - accuracy: 0.9978 - loss: 0.0110\n",
      "Batch 26722: setting learning rate to 4.333012688678788e-06.\n",
      "  22/1780 [..............................] - ETA: 12:12 - accuracy: 0.9979 - loss: 0.0106\n",
      "Batch 26723: setting learning rate to 4.332659371674672e-06.\n",
      "  23/1780 [..............................] - ETA: 12:13 - accuracy: 0.9980 - loss: 0.0101\n",
      "Batch 26724: setting learning rate to 4.332306058063206e-06.\n",
      "  24/1780 [..............................] - ETA: 12:15 - accuracy: 0.9980 - loss: 0.0097\n",
      "Batch 26725: setting learning rate to 4.331952747846182e-06.\n",
      "  25/1780 [..............................] - ETA: 12:15 - accuracy: 0.9981 - loss: 0.0093\n",
      "Batch 26726: setting learning rate to 4.331599441025395e-06.\n",
      "  26/1780 [..............................] - ETA: 12:16 - accuracy: 0.9982 - loss: 0.0091\n",
      "Batch 26727: setting learning rate to 4.331246137602645e-06.\n",
      "  27/1780 [..............................] - ETA: 12:15 - accuracy: 0.9983 - loss: 0.0088\n",
      "Batch 26728: setting learning rate to 4.330892837579727e-06.\n",
      "  28/1780 [..............................] - ETA: 12:17 - accuracy: 0.9983 - loss: 0.0085\n",
      "Batch 26729: setting learning rate to 4.330539540958433e-06.\n",
      "  29/1780 [..............................] - ETA: 12:17 - accuracy: 0.9984 - loss: 0.0089\n",
      "Batch 26730: setting learning rate to 4.3301862477405645e-06.\n",
      "  30/1780 [..............................] - ETA: 12:16 - accuracy: 0.9984 - loss: 0.0086\n",
      "Batch 26731: setting learning rate to 4.329832957927917e-06.\n",
      "  31/1780 [..............................] - ETA: 12:16 - accuracy: 0.9985 - loss: 0.0087\n",
      "Batch 26732: setting learning rate to 4.329479671522281e-06.\n",
      "  32/1780 [..............................] - ETA: 12:14 - accuracy: 0.9985 - loss: 0.0104\n",
      "Batch 26733: setting learning rate to 4.32912638852546e-06.\n",
      "  33/1780 [..............................] - ETA: 12:16 - accuracy: 0.9986 - loss: 0.0101\n",
      "Batch 26734: setting learning rate to 4.328773108939243e-06.\n",
      "  34/1780 [..............................] - ETA: 12:15 - accuracy: 0.9986 - loss: 0.0098\n",
      "Batch 26735: setting learning rate to 4.328419832765433e-06.\n",
      "  35/1780 [..............................] - ETA: 12:16 - accuracy: 0.9987 - loss: 0.0095\n",
      "Batch 26736: setting learning rate to 4.3280665600058224e-06.\n",
      "  36/1780 [..............................] - ETA: 12:16 - accuracy: 0.9987 - loss: 0.0094\n",
      "Batch 26737: setting learning rate to 4.327713290662205e-06.\n",
      "  37/1780 [..............................] - ETA: 12:16 - accuracy: 0.9987 - loss: 0.0091\n",
      "Batch 26738: setting learning rate to 4.327360024736381e-06.\n",
      "  38/1780 [..............................] - ETA: 12:17 - accuracy: 0.9988 - loss: 0.0089\n",
      "Batch 26739: setting learning rate to 4.327006762230144e-06.\n",
      "  39/1780 [..............................] - ETA: 12:16 - accuracy: 0.9980 - loss: 0.0102\n",
      "Batch 26740: setting learning rate to 4.326653503145291e-06.\n",
      "  40/1780 [..............................] - ETA: 12:16 - accuracy: 0.9980 - loss: 0.0099\n",
      "Batch 26741: setting learning rate to 4.326300247483617e-06.\n",
      "  41/1780 [..............................] - ETA: 12:16 - accuracy: 0.9981 - loss: 0.0097\n",
      "Batch 26742: setting learning rate to 4.325946995246918e-06.\n",
      "  42/1780 [..............................] - ETA: 12:17 - accuracy: 0.9981 - loss: 0.0095\n",
      "Batch 26743: setting learning rate to 4.32559374643699e-06.\n",
      "  43/1780 [..............................] - ETA: 12:16 - accuracy: 0.9978 - loss: 0.0095\n",
      "Batch 26744: setting learning rate to 4.325240501055629e-06.\n",
      "  44/1780 [..............................] - ETA: 12:16 - accuracy: 0.9979 - loss: 0.0093\n",
      "Batch 26745: setting learning rate to 4.324887259104632e-06.\n",
      "  45/1780 [..............................] - ETA: 12:16 - accuracy: 0.9972 - loss: 0.0108\n",
      "Batch 26746: setting learning rate to 4.324534020585792e-06.\n",
      "  46/1780 [..............................] - ETA: 12:15 - accuracy: 0.9969 - loss: 0.0109\n",
      "Batch 26747: setting learning rate to 4.324180785500907e-06.\n",
      "  47/1780 [..............................] - ETA: 12:16 - accuracy: 0.9967 - loss: 0.0109\n",
      "Batch 26748: setting learning rate to 4.323827553851772e-06.\n",
      "  48/1780 [..............................] - ETA: 12:16 - accuracy: 0.9967 - loss: 0.0107\n",
      "Batch 26749: setting learning rate to 4.323474325640182e-06.\n",
      "  49/1780 [..............................] - ETA: 12:16 - accuracy: 0.9968 - loss: 0.0105\n",
      "Batch 26750: setting learning rate to 4.323121100867936e-06.\n",
      "  50/1780 [..............................] - ETA: 12:16 - accuracy: 0.9969 - loss: 0.0107\n",
      "Batch 26751: setting learning rate to 4.3227678795368245e-06.\n",
      "  51/1780 [..............................] - ETA: 12:16 - accuracy: 0.9969 - loss: 0.0105\n",
      "Batch 26752: setting learning rate to 4.322414661648649e-06.\n",
      "  52/1780 [..............................] - ETA: 12:16 - accuracy: 0.9970 - loss: 0.0103\n",
      "Batch 26753: setting learning rate to 4.322061447205202e-06.\n",
      "  53/1780 [..............................] - ETA: 12:17 - accuracy: 0.9971 - loss: 0.0101\n",
      "Batch 26754: setting learning rate to 4.321708236208277e-06.\n",
      "  54/1780 [..............................] - ETA: 12:17 - accuracy: 0.9971 - loss: 0.0099\n",
      "Batch 26755: setting learning rate to 4.321355028659676e-06.\n",
      "  55/1780 [..............................] - ETA: 12:17 - accuracy: 0.9972 - loss: 0.0097\n",
      "Batch 26756: setting learning rate to 4.3210018245611895e-06.\n",
      "  56/1780 [..............................] - ETA: 12:17 - accuracy: 0.9969 - loss: 0.0105\n",
      "Batch 26757: setting learning rate to 4.320648623914613e-06.\n",
      "  57/1780 [..............................] - ETA: 12:16 - accuracy: 0.9970 - loss: 0.0103\n",
      "Batch 26758: setting learning rate to 4.320295426721745e-06.\n",
      "  58/1780 [..............................] - ETA: 12:16 - accuracy: 0.9970 - loss: 0.0103\n",
      "Batch 26759: setting learning rate to 4.319942232984381e-06.\n",
      "  59/1780 [..............................] - ETA: 12:16 - accuracy: 0.9971 - loss: 0.0101\n",
      "Batch 26760: setting learning rate to 4.319589042704312e-06.\n",
      "  60/1780 [>.............................] - ETA: 12:16 - accuracy: 0.9971 - loss: 0.0101\n",
      "Batch 26761: setting learning rate to 4.3192358558833396e-06.\n",
      "  61/1780 [>.............................] - ETA: 12:16 - accuracy: 0.9969 - loss: 0.0112\n",
      "Batch 26762: setting learning rate to 4.3188826725232555e-06.\n",
      "  62/1780 [>.............................] - ETA: 12:16 - accuracy: 0.9970 - loss: 0.0111\n",
      "Batch 26763: setting learning rate to 4.318529492625855e-06.\n",
      "  63/1780 [>.............................] - ETA: 12:15 - accuracy: 0.9970 - loss: 0.0109\n",
      "Batch 26764: setting learning rate to 4.318176316192937e-06.\n",
      "  64/1780 [>.............................] - ETA: 12:15 - accuracy: 0.9971 - loss: 0.0107\n",
      "Batch 26765: setting learning rate to 4.317823143226294e-06.\n",
      "  65/1780 [>.............................] - ETA: 12:15 - accuracy: 0.9971 - loss: 0.0106\n",
      "Batch 26766: setting learning rate to 4.317469973727722e-06.\n",
      "  66/1780 [>.............................] - ETA: 12:14 - accuracy: 0.9972 - loss: 0.0104\n",
      "Batch 26767: setting learning rate to 4.317116807699018e-06.\n",
      "  67/1780 [>.............................] - ETA: 12:14 - accuracy: 0.9972 - loss: 0.0103\n",
      "Batch 26768: setting learning rate to 4.316763645141975e-06.\n",
      "  68/1780 [>.............................] - ETA: 12:14 - accuracy: 0.9970 - loss: 0.0105\n",
      "Batch 26769: setting learning rate to 4.316410486058391e-06.\n",
      "  69/1780 [>.............................] - ETA: 12:14 - accuracy: 0.9971 - loss: 0.0104\n",
      "Batch 26770: setting learning rate to 4.316057330450059e-06.\n",
      "  70/1780 [>.............................] - ETA: 12:13 - accuracy: 0.9971 - loss: 0.0102\n",
      "Batch 26771: setting learning rate to 4.3157041783187755e-06.\n",
      "  71/1780 [>.............................] - ETA: 12:13 - accuracy: 0.9969 - loss: 0.0105\n",
      "Batch 26772: setting learning rate to 4.315351029666336e-06.\n",
      "  72/1780 [>.............................] - ETA: 12:12 - accuracy: 0.9970 - loss: 0.0105\n",
      "Batch 26773: setting learning rate to 4.3149978844945365e-06.\n",
      "  73/1780 [>.............................] - ETA: 12:12 - accuracy: 0.9970 - loss: 0.0108\n",
      "Batch 26774: setting learning rate to 4.314644742805171e-06.\n",
      "  74/1780 [>.............................] - ETA: 12:11 - accuracy: 0.9970 - loss: 0.0106\n",
      "Batch 26775: setting learning rate to 4.314291604600035e-06.\n",
      "  75/1780 [>.............................] - ETA: 12:11 - accuracy: 0.9969 - loss: 0.0107\n",
      "Batch 26776: setting learning rate to 4.313938469880925e-06.\n",
      "  76/1780 [>.............................] - ETA: 12:11 - accuracy: 0.9967 - loss: 0.0107\n",
      "Batch 26777: setting learning rate to 4.313585338649633e-06.\n",
      "  77/1780 [>.............................] - ETA: 12:10 - accuracy: 0.9968 - loss: 0.0106\n",
      "Batch 26778: setting learning rate to 4.313232210907959e-06.\n",
      "  78/1780 [>.............................] - ETA: 12:11 - accuracy: 0.9968 - loss: 0.0104\n",
      "Batch 26779: setting learning rate to 4.312879086657697e-06.\n",
      "  79/1780 [>.............................] - ETA: 12:10 - accuracy: 0.9968 - loss: 0.0103\n",
      "Batch 26780: setting learning rate to 4.312525965900638e-06.\n",
      "  80/1780 [>.............................] - ETA: 12:10 - accuracy: 0.9969 - loss: 0.0102\n",
      "Batch 26781: setting learning rate to 4.312172848638583e-06.\n",
      "  81/1780 [>.............................] - ETA: 12:09 - accuracy: 0.9969 - loss: 0.0101\n",
      "Batch 26782: setting learning rate to 4.311819734873324e-06.\n",
      "  82/1780 [>.............................] - ETA: 12:09 - accuracy: 0.9970 - loss: 0.0100\n",
      "Batch 26783: setting learning rate to 4.3114666246066545e-06.\n",
      "  83/1780 [>.............................] - ETA: 12:08 - accuracy: 0.9970 - loss: 0.0099\n",
      "Batch 26784: setting learning rate to 4.311113517840374e-06.\n",
      "  84/1780 [>.............................] - ETA: 12:09 - accuracy: 0.9970 - loss: 0.0099\n",
      "Batch 26785: setting learning rate to 4.310760414576273e-06.\n",
      "  85/1780 [>.............................] - ETA: 12:09 - accuracy: 0.9971 - loss: 0.0098\n",
      "Batch 26786: setting learning rate to 4.310407314816152e-06.\n",
      "  86/1780 [>.............................] - ETA: 12:10 - accuracy: 0.9971 - loss: 0.0097\n",
      "Batch 26787: setting learning rate to 4.310054218561803e-06.\n",
      "  87/1780 [>.............................] - ETA: 12:09 - accuracy: 0.9969 - loss: 0.0103\n",
      "Batch 26788: setting learning rate to 4.309701125815019e-06.\n",
      "  88/1780 [>.............................] - ETA: 12:09 - accuracy: 0.9970 - loss: 0.0101\n",
      "Batch 26789: setting learning rate to 4.3093480365776e-06.\n",
      "  89/1780 [>.............................] - ETA: 12:08 - accuracy: 0.9970 - loss: 0.0101\n",
      "Batch 26790: setting learning rate to 4.308994950851337e-06.\n",
      "  90/1780 [>.............................] - ETA: 12:08 - accuracy: 0.9970 - loss: 0.0099\n",
      "Batch 26791: setting learning rate to 4.308641868638026e-06.\n",
      "  91/1780 [>.............................] - ETA: 12:07 - accuracy: 0.9969 - loss: 0.0102\n",
      "Batch 26792: setting learning rate to 4.3082887899394635e-06.\n",
      "  92/1780 [>.............................] - ETA: 12:07 - accuracy: 0.9968 - loss: 0.0102\n",
      "Batch 26793: setting learning rate to 4.307935714757443e-06.\n",
      "  93/1780 [>.............................] - ETA: 12:07 - accuracy: 0.9968 - loss: 0.0102\n",
      "Batch 26794: setting learning rate to 4.307582643093759e-06.\n",
      "  94/1780 [>.............................] - ETA: 12:07 - accuracy: 0.9967 - loss: 0.0105\n",
      "Batch 26795: setting learning rate to 4.307229574950209e-06.\n",
      "  95/1780 [>.............................] - ETA: 12:06 - accuracy: 0.9967 - loss: 0.0104\n",
      "Batch 26796: setting learning rate to 4.306876510328585e-06.\n",
      "  96/1780 [>.............................] - ETA: 12:06 - accuracy: 0.9967 - loss: 0.0103\n",
      "Batch 26797: setting learning rate to 4.3065234492306835e-06.\n",
      "  97/1780 [>.............................] - ETA: 12:05 - accuracy: 0.9968 - loss: 0.0105\n",
      "Batch 26798: setting learning rate to 4.306170391658299e-06.\n",
      "  98/1780 [>.............................] - ETA: 12:05 - accuracy: 0.9968 - loss: 0.0104\n",
      "Batch 26799: setting learning rate to 4.305817337613227e-06.\n",
      "  99/1780 [>.............................] - ETA: 12:04 - accuracy: 0.9968 - loss: 0.0103\n",
      "Batch 26800: setting learning rate to 4.305464287097261e-06.\n",
      " 100/1780 [>.............................] - ETA: 12:04 - accuracy: 0.9969 - loss: 0.0102\n",
      "Batch 26801: setting learning rate to 4.305111240112198e-06.\n",
      " 101/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9969 - loss: 0.0104\n",
      "Batch 26802: setting learning rate to 4.304758196659831e-06.\n",
      " 102/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9968 - loss: 0.0105\n",
      "Batch 26803: setting learning rate to 4.304405156741955e-06.\n",
      " 103/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9968 - loss: 0.0104\n",
      "Batch 26804: setting learning rate to 4.304052120360365e-06.\n",
      " 104/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9968 - loss: 0.0103\n",
      "Batch 26805: setting learning rate to 4.303699087516855e-06.\n",
      " 105/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9969 - loss: 0.0102\n",
      "Batch 26806: setting learning rate to 4.303346058213223e-06.\n",
      " 106/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9969 - loss: 0.0101\n",
      "Batch 26807: setting learning rate to 4.3029930324512605e-06.\n",
      " 107/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9968 - loss: 0.0103\n",
      "Batch 26808: setting learning rate to 4.302640010232762e-06.\n",
      " 108/1780 [>.............................] - ETA: 12:03 - accuracy: 0.9968 - loss: 0.0102\n",
      "Batch 26809: setting learning rate to 4.302286991559525e-06.\n",
      " 109/1780 [>.............................] - ETA: 12:02 - accuracy: 0.9968 - loss: 0.0101\n",
      "Batch 26810: setting learning rate to 4.301933976433343e-06.\n",
      " 110/1780 [>.............................] - ETA: 12:02 - accuracy: 0.9969 - loss: 0.0100\n",
      "Batch 26811: setting learning rate to 4.301580964856008e-06.\n",
      " 111/1780 [>.............................] - ETA: 12:02 - accuracy: 0.9969 - loss: 0.0099\n",
      "Batch 26812: setting learning rate to 4.301227956829318e-06.\n",
      " 112/1780 [>.............................] - ETA: 12:01 - accuracy: 0.9968 - loss: 0.0103\n",
      "Batch 26813: setting learning rate to 4.300874952355067e-06.\n",
      " 113/1780 [>.............................] - ETA: 12:01 - accuracy: 0.9968 - loss: 0.0103\n",
      "Batch 26814: setting learning rate to 4.300521951435047e-06.\n",
      " 114/1780 [>.............................] - ETA: 12:01 - accuracy: 0.9968 - loss: 0.0102\n",
      "Batch 26815: setting learning rate to 4.300168954071058e-06.\n",
      " 115/1780 [>.............................] - ETA: 12:01 - accuracy: 0.9969 - loss: 0.0101\n",
      "Batch 26816: setting learning rate to 4.29981596026489e-06.\n",
      " 116/1780 [>.............................] - ETA: 12:00 - accuracy: 0.9969 - loss: 0.0100\n",
      "Batch 26817: setting learning rate to 4.299462970018337e-06.\n",
      " 117/1780 [>.............................] - ETA: 12:00 - accuracy: 0.9968 - loss: 0.0101\n",
      "Batch 26818: setting learning rate to 4.299109983333197e-06.\n",
      " 118/1780 [>.............................] - ETA: 11:59 - accuracy: 0.9968 - loss: 0.0100\n",
      "Batch 26819: setting learning rate to 4.298757000211264e-06.\n",
      " 119/1780 [=>............................] - ETA: 11:59 - accuracy: 0.9968 - loss: 0.0100\n",
      "Batch 26820: setting learning rate to 4.298404020654329e-06.\n",
      " 120/1780 [=>............................] - ETA: 11:58 - accuracy: 0.9969 - loss: 0.0099\n",
      "Batch 26821: setting learning rate to 4.298051044664189e-06.\n",
      " 121/1780 [=>............................] - ETA: 11:57 - accuracy: 0.9969 - loss: 0.0098\n",
      "Batch 26822: setting learning rate to 4.2976980722426395e-06.\n",
      " 122/1780 [=>............................] - ETA: 11:57 - accuracy: 0.9969 - loss: 0.0097\n",
      "Batch 26823: setting learning rate to 4.297345103391475e-06.\n",
      " 123/1780 [=>............................] - ETA: 11:57 - accuracy: 0.9970 - loss: 0.0097\n",
      "Batch 26824: setting learning rate to 4.2969921381124866e-06.\n",
      " 124/1780 [=>............................] - ETA: 11:57 - accuracy: 0.9970 - loss: 0.0096\n",
      "Batch 26825: setting learning rate to 4.2966391764074714e-06.\n",
      " 125/1780 [=>............................] - ETA: 11:57 - accuracy: 0.9970 - loss: 0.0095\n",
      "Batch 26826: setting learning rate to 4.296286218278224e-06.\n",
      " 126/1780 [=>............................] - ETA: 11:57 - accuracy: 0.9970 - loss: 0.0094\n",
      "Batch 26827: setting learning rate to 4.295933263726538e-06.\n",
      " 127/1780 [=>............................] - ETA: 11:56 - accuracy: 0.9970 - loss: 0.0094\n",
      "Batch 26828: setting learning rate to 4.295580312754206e-06.\n",
      " 128/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9971 - loss: 0.0093\n",
      "Batch 26829: setting learning rate to 4.2952273653630265e-06.\n",
      " 129/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9971 - loss: 0.0093\n",
      "Batch 26830: setting learning rate to 4.294874421554791e-06.\n",
      " 130/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9971 - loss: 0.0093\n",
      "Batch 26831: setting learning rate to 4.294521481331293e-06.\n",
      " 131/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9971 - loss: 0.0092\n",
      "Batch 26832: setting learning rate to 4.2941685446943296e-06.\n",
      " 132/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9972 - loss: 0.0092\n",
      "Batch 26833: setting learning rate to 4.2938156116456935e-06.\n",
      " 133/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9972 - loss: 0.0093\n",
      "Batch 26834: setting learning rate to 4.293462682187177e-06.\n",
      " 134/1780 [=>............................] - ETA: 11:55 - accuracy: 0.9972 - loss: 0.0092\n",
      "Batch 26835: setting learning rate to 4.293109756320579e-06.\n",
      " 135/1780 [=>............................] - ETA: 11:54 - accuracy: 0.9972 - loss: 0.0092\n",
      "Batch 26836: setting learning rate to 4.29275683404769e-06.\n",
      " 136/1780 [=>............................] - ETA: 11:53 - accuracy: 0.9971 - loss: 0.0093\n",
      "Batch 26837: setting learning rate to 4.292403915370304e-06.\n",
      " 137/1780 [=>............................] - ETA: 11:53 - accuracy: 0.9971 - loss: 0.0093\n",
      "Batch 26838: setting learning rate to 4.292051000290218e-06.\n",
      " 138/1780 [=>............................] - ETA: 11:52 - accuracy: 0.9972 - loss: 0.0093\n",
      "Batch 26839: setting learning rate to 4.291698088809223e-06.\n",
      " 139/1780 [=>............................] - ETA: 11:52 - accuracy: 0.9972 - loss: 0.0093\n",
      "Batch 26840: setting learning rate to 4.291345180929117e-06.\n",
      " 140/1780 [=>............................] - ETA: 11:51 - accuracy: 0.9972 - loss: 0.0093\n",
      "Batch 26841: setting learning rate to 4.290992276651691e-06.\n",
      " 141/1780 [=>............................] - ETA: 11:50 - accuracy: 0.9971 - loss: 0.0094\n",
      "Batch 26842: setting learning rate to 4.2906393759787375e-06.\n",
      " 142/1780 [=>............................] - ETA: 11:51 - accuracy: 0.9971 - loss: 0.0095\n",
      "Batch 26843: setting learning rate to 4.290286478912057e-06.\n",
      " 143/1780 [=>............................] - ETA: 11:50 - accuracy: 0.9972 - loss: 0.0095\n",
      "Batch 26844: setting learning rate to 4.289933585453438e-06.\n",
      " 144/1780 [=>............................] - ETA: 11:50 - accuracy: 0.9971 - loss: 0.0095\n",
      "Batch 26845: setting learning rate to 4.289580695604675e-06.\n",
      " 145/1780 [=>............................] - ETA: 11:49 - accuracy: 0.9971 - loss: 0.0094\n",
      "Batch 26846: setting learning rate to 4.289227809367564e-06.\n",
      " 146/1780 [=>............................] - ETA: 11:48 - accuracy: 0.9971 - loss: 0.0094\n",
      "Batch 26847: setting learning rate to 4.288874926743899e-06.\n",
      " 147/1780 [=>............................] - ETA: 11:48 - accuracy: 0.9971 - loss: 0.0093\n",
      "Batch 26848: setting learning rate to 4.288522047735472e-06.\n",
      " 148/1780 [=>............................] - ETA: 11:47 - accuracy: 0.9971 - loss: 0.0092\n",
      "Batch 26849: setting learning rate to 4.288169172344078e-06.\n",
      " 149/1780 [=>............................] - ETA: 11:46 - accuracy: 0.9972 - loss: 0.0092\n",
      "Batch 26850: setting learning rate to 4.287816300571513e-06.\n",
      " 150/1780 [=>............................] - ETA: 11:46 - accuracy: 0.9972 - loss: 0.0091\n",
      "Batch 26851: setting learning rate to 4.287463432419567e-06.\n",
      " 151/1780 [=>............................] - ETA: 11:45 - accuracy: 0.9972 - loss: 0.0091\n",
      "Batch 26852: setting learning rate to 4.287110567890037e-06.\n",
      " 152/1780 [=>............................] - ETA: 11:44 - accuracy: 0.9972 - loss: 0.0091\n",
      "Batch 26853: setting learning rate to 4.286757706984716e-06.\n",
      " 153/1780 [=>............................] - ETA: 11:44 - accuracy: 0.9972 - loss: 0.0090\n",
      "Batch 26854: setting learning rate to 4.286404849705397e-06.\n",
      " 154/1780 [=>............................] - ETA: 11:43 - accuracy: 0.9973 - loss: 0.0090\n",
      "Batch 26855: setting learning rate to 4.286051996053876e-06.\n",
      " 155/1780 [=>............................] - ETA: 11:42 - accuracy: 0.9973 - loss: 0.0089\n",
      "Batch 26856: setting learning rate to 4.285699146031943e-06.\n",
      " 156/1780 [=>............................] - ETA: 11:42 - accuracy: 0.9973 - loss: 0.0089\n",
      "Batch 26857: setting learning rate to 4.285346299641396e-06.\n",
      " 157/1780 [=>............................] - ETA: 11:41 - accuracy: 0.9973 - loss: 0.0089\n",
      "Batch 26858: setting learning rate to 4.284993456884029e-06.\n",
      " 158/1780 [=>............................] - ETA: 11:40 - accuracy: 0.9973 - loss: 0.0089\n",
      "Batch 26859: setting learning rate to 4.284640617761631e-06.\n",
      " 159/1780 [=>............................] - ETA: 11:39 - accuracy: 0.9972 - loss: 0.0091\n",
      "Batch 26860: setting learning rate to 4.2842877822759995e-06.\n",
      " 160/1780 [=>............................] - ETA: 11:39 - accuracy: 0.9973 - loss: 0.0090\n",
      "Batch 26861: setting learning rate to 4.283934950428929e-06.\n",
      " 161/1780 [=>............................] - ETA: 11:38 - accuracy: 0.9973 - loss: 0.0091\n",
      "Batch 26862: setting learning rate to 4.28358212222221e-06.\n",
      " 162/1780 [=>............................] - ETA: 11:38 - accuracy: 0.9973 - loss: 0.0090\n",
      "Batch 26863: setting learning rate to 4.283229297657639e-06.\n",
      " 163/1780 [=>............................] - ETA: 11:37 - accuracy: 0.9973 - loss: 0.0090\n",
      "Batch 26864: setting learning rate to 4.282876476737008e-06.\n",
      " 164/1780 [=>............................] - ETA: 11:36 - accuracy: 0.9973 - loss: 0.0089\n",
      "Batch 26865: setting learning rate to 4.28252365946211e-06.\n",
      " 165/1780 [=>............................] - ETA: 11:36 - accuracy: 0.9973 - loss: 0.0089\n",
      "Batch 26866: setting learning rate to 4.282170845834742e-06.\n",
      " 166/1780 [=>............................] - ETA: 11:35 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 26867: setting learning rate to 4.2818180358566964e-06.\n",
      " 167/1780 [=>............................] - ETA: 11:34 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 26868: setting learning rate to 4.281465229529763e-06.\n",
      " 168/1780 [=>............................] - ETA: 11:34 - accuracy: 0.9973 - loss: 0.0097\n",
      "Batch 26869: setting learning rate to 4.281112426855741e-06.\n",
      " 169/1780 [=>............................] - ETA: 11:33 - accuracy: 0.9973 - loss: 0.0097\n",
      "Batch 26870: setting learning rate to 4.2807596278364196e-06.\n",
      " 170/1780 [=>............................] - ETA: 11:32 - accuracy: 0.9973 - loss: 0.0096\n",
      "Batch 26871: setting learning rate to 4.280406832473595e-06.\n",
      " 171/1780 [=>............................] - ETA: 11:31 - accuracy: 0.9974 - loss: 0.0096\n",
      "Batch 26872: setting learning rate to 4.28005404076906e-06.\n",
      " 172/1780 [=>............................] - ETA: 11:31 - accuracy: 0.9973 - loss: 0.0096\n",
      "Batch 26873: setting learning rate to 4.279701252724608e-06.\n",
      " 173/1780 [=>............................] - ETA: 11:30 - accuracy: 0.9973 - loss: 0.0096\n",
      "Batch 26874: setting learning rate to 4.279348468342033e-06.\n",
      " 174/1780 [=>............................] - ETA: 11:29 - accuracy: 0.9973 - loss: 0.0096\n",
      "Batch 26875: setting learning rate to 4.2789956876231276e-06.\n",
      " 175/1780 [=>............................] - ETA: 11:29 - accuracy: 0.9973 - loss: 0.0095\n",
      "Batch 26876: setting learning rate to 4.2786429105696856e-06.\n",
      " 176/1780 [=>............................] - ETA: 11:28 - accuracy: 0.9973 - loss: 0.0095\n",
      "Batch 26877: setting learning rate to 4.278290137183501e-06.\n",
      " 177/1780 [=>............................] - ETA: 11:28 - accuracy: 0.9974 - loss: 0.0096\n",
      "Batch 26878: setting learning rate to 4.277937367466367e-06.\n",
      " 178/1780 [==>...........................] - ETA: 11:27 - accuracy: 0.9974 - loss: 0.0096\n",
      "Batch 26879: setting learning rate to 4.2775846014200765e-06.\n",
      " 179/1780 [==>...........................] - ETA: 11:26 - accuracy: 0.9973 - loss: 0.0096\n",
      "Batch 26880: setting learning rate to 4.277231839046424e-06.\n",
      " 180/1780 [==>...........................] - ETA: 11:26 - accuracy: 0.9973 - loss: 0.0096\n",
      "Batch 26881: setting learning rate to 4.276879080347202e-06.\n",
      " 181/1780 [==>...........................] - ETA: 11:25 - accuracy: 0.9973 - loss: 0.0095\n",
      "Batch 26882: setting learning rate to 4.276526325324203e-06.\n",
      " 182/1780 [==>...........................] - ETA: 11:24 - accuracy: 0.9973 - loss: 0.0095\n",
      "Batch 26883: setting learning rate to 4.276173573979223e-06.\n",
      " 183/1780 [==>...........................] - ETA: 11:24 - accuracy: 0.9974 - loss: 0.0095\n",
      "Batch 26884: setting learning rate to 4.275820826314053e-06.\n",
      " 184/1780 [==>...........................] - ETA: 11:23 - accuracy: 0.9974 - loss: 0.0094\n",
      "Batch 26885: setting learning rate to 4.275468082330486e-06.\n",
      " 185/1780 [==>...........................] - ETA: 11:23 - accuracy: 0.9974 - loss: 0.0094\n",
      "Batch 26886: setting learning rate to 4.275115342030319e-06.\n",
      " 186/1780 [==>...........................] - ETA: 11:22 - accuracy: 0.9974 - loss: 0.0093\n",
      "Batch 26887: setting learning rate to 4.274762605415341e-06.\n",
      " 187/1780 [==>...........................] - ETA: 11:21 - accuracy: 0.9974 - loss: 0.0093\n",
      "Batch 26888: setting learning rate to 4.274409872487345e-06.\n",
      " 188/1780 [==>...........................] - ETA: 11:21 - accuracy: 0.9974 - loss: 0.0092\n",
      "Batch 26889: setting learning rate to 4.2740571432481285e-06.\n",
      " 189/1780 [==>...........................] - ETA: 11:20 - accuracy: 0.9974 - loss: 0.0092\n",
      "Batch 26890: setting learning rate to 4.27370441769948e-06.\n",
      " 190/1780 [==>...........................] - ETA: 11:20 - accuracy: 0.9975 - loss: 0.0091\n",
      "Batch 26891: setting learning rate to 4.273351695843198e-06.\n",
      " 191/1780 [==>...........................] - ETA: 11:19 - accuracy: 0.9975 - loss: 0.0091\n",
      "Batch 26892: setting learning rate to 4.272998977681072e-06.\n",
      " 192/1780 [==>...........................] - ETA: 11:18 - accuracy: 0.9975 - loss: 0.0090\n",
      "Batch 26893: setting learning rate to 4.272646263214893e-06.\n",
      " 193/1780 [==>...........................] - ETA: 11:18 - accuracy: 0.9975 - loss: 0.0090\n",
      "Batch 26894: setting learning rate to 4.27229355244646e-06.\n",
      " 194/1780 [==>...........................] - ETA: 11:17 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26895: setting learning rate to 4.271940845377564e-06.\n",
      " 195/1780 [==>...........................] - ETA: 11:16 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26896: setting learning rate to 4.271588142009993e-06.\n",
      " 196/1780 [==>...........................] - ETA: 11:16 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26897: setting learning rate to 4.271235442345547e-06.\n",
      " 197/1780 [==>...........................] - ETA: 11:15 - accuracy: 0.9975 - loss: 0.0088\n",
      "Batch 26898: setting learning rate to 4.2708827463860166e-06.\n",
      " 198/1780 [==>...........................] - ETA: 11:15 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26899: setting learning rate to 4.270530054133193e-06.\n",
      " 199/1780 [==>...........................] - ETA: 11:14 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26900: setting learning rate to 4.270177365588873e-06.\n",
      " 200/1780 [==>...........................] - ETA: 11:14 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26901: setting learning rate to 4.269824680754846e-06.\n",
      " 201/1780 [==>...........................] - ETA: 11:13 - accuracy: 0.9975 - loss: 0.0088\n",
      "Batch 26902: setting learning rate to 4.2694719996329065e-06.\n",
      " 202/1780 [==>...........................] - ETA: 11:13 - accuracy: 0.9975 - loss: 0.0088\n",
      "Batch 26903: setting learning rate to 4.269119322224847e-06.\n",
      " 203/1780 [==>...........................] - ETA: 11:12 - accuracy: 0.9975 - loss: 0.0090\n",
      "Batch 26904: setting learning rate to 4.2687666485324625e-06.\n",
      " 204/1780 [==>...........................] - ETA: 11:12 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26905: setting learning rate to 4.268413978557542e-06.\n",
      " 205/1780 [==>...........................] - ETA: 11:11 - accuracy: 0.9976 - loss: 0.0089\n",
      "Batch 26906: setting learning rate to 4.268061312301882e-06.\n",
      " 206/1780 [==>...........................] - ETA: 11:10 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26907: setting learning rate to 4.267708649767275e-06.\n",
      " 207/1780 [==>...........................] - ETA: 11:10 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26908: setting learning rate to 4.2673559909555106e-06.\n",
      " 208/1780 [==>...........................] - ETA: 11:09 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26909: setting learning rate to 4.267003335868386e-06.\n",
      " 209/1780 [==>...........................] - ETA: 11:09 - accuracy: 0.9975 - loss: 0.0090\n",
      "Batch 26910: setting learning rate to 4.26665068450769e-06.\n",
      " 210/1780 [==>...........................] - ETA: 11:08 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26911: setting learning rate to 4.266298036875219e-06.\n",
      " 211/1780 [==>...........................] - ETA: 11:08 - accuracy: 0.9975 - loss: 0.0090\n",
      "Batch 26912: setting learning rate to 4.265945392972765e-06.\n",
      " 212/1780 [==>...........................] - ETA: 11:07 - accuracy: 0.9975 - loss: 0.0090\n",
      "Batch 26913: setting learning rate to 4.2655927528021184e-06.\n",
      " 213/1780 [==>...........................] - ETA: 11:07 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26914: setting learning rate to 4.265240116365075e-06.\n",
      " 214/1780 [==>...........................] - ETA: 11:06 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26915: setting learning rate to 4.264887483663427e-06.\n",
      " 215/1780 [==>...........................] - ETA: 11:05 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26916: setting learning rate to 4.264534854698963e-06.\n",
      " 216/1780 [==>...........................] - ETA: 11:05 - accuracy: 0.9975 - loss: 0.0089\n",
      "Batch 26917: setting learning rate to 4.264182229473482e-06.\n",
      " 217/1780 [==>...........................] - ETA: 11:05 - accuracy: 0.9976 - loss: 0.0089\n",
      "Batch 26918: setting learning rate to 4.263829607988773e-06.\n",
      " 218/1780 [==>...........................] - ETA: 11:04 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26919: setting learning rate to 4.263476990246628e-06.\n",
      " 219/1780 [==>...........................] - ETA: 11:03 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26920: setting learning rate to 4.263124376248842e-06.\n",
      " 220/1780 [==>...........................] - ETA: 11:03 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26921: setting learning rate to 4.262771765997208e-06.\n",
      " 221/1780 [==>...........................] - ETA: 11:02 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26922: setting learning rate to 4.262419159493514e-06.\n",
      " 222/1780 [==>...........................] - ETA: 11:02 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26923: setting learning rate to 4.2620665567395585e-06.\n",
      " 223/1780 [==>...........................] - ETA: 11:01 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26924: setting learning rate to 4.26171395773713e-06.\n",
      " 224/1780 [==>...........................] - ETA: 11:01 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26925: setting learning rate to 4.261361362488023e-06.\n",
      " 225/1780 [==>...........................] - ETA: 11:00 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26926: setting learning rate to 4.261008770994029e-06.\n",
      " 226/1780 [==>...........................] - ETA: 11:00 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26927: setting learning rate to 4.260656183256941e-06.\n",
      " 227/1780 [==>...........................] - ETA: 10:59 - accuracy: 0.9977 - loss: 0.0088\n",
      "Batch 26928: setting learning rate to 4.260303599278553e-06.\n",
      " 228/1780 [==>...........................] - ETA: 10:59 - accuracy: 0.9976 - loss: 0.0089\n",
      "Batch 26929: setting learning rate to 4.2599510190606545e-06.\n",
      " 229/1780 [==>...........................] - ETA: 10:58 - accuracy: 0.9976 - loss: 0.0089\n",
      "Batch 26930: setting learning rate to 4.25959844260504e-06.\n",
      " 230/1780 [==>...........................] - ETA: 10:57 - accuracy: 0.9976 - loss: 0.0089\n",
      "Batch 26931: setting learning rate to 4.2592458699135005e-06.\n",
      " 231/1780 [==>...........................] - ETA: 10:57 - accuracy: 0.9976 - loss: 0.0089\n",
      "Batch 26932: setting learning rate to 4.258893300987831e-06.\n",
      " 232/1780 [==>...........................] - ETA: 10:56 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26933: setting learning rate to 4.25854073582982e-06.\n",
      " 233/1780 [==>...........................] - ETA: 10:56 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26934: setting learning rate to 4.258188174441264e-06.\n",
      " 234/1780 [==>...........................] - ETA: 10:55 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26935: setting learning rate to 4.2578356168239536e-06.\n",
      " 235/1780 [==>...........................] - ETA: 10:55 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26936: setting learning rate to 4.257483062979678e-06.\n",
      " 236/1780 [==>...........................] - ETA: 10:54 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26937: setting learning rate to 4.257130512910236e-06.\n",
      " 237/1780 [==>...........................] - ETA: 10:54 - accuracy: 0.9976 - loss: 0.0088\n",
      "Batch 26938: setting learning rate to 4.256777966617416e-06.\n",
      " 238/1780 [===>..........................] - ETA: 10:53 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26939: setting learning rate to 4.256425424103009e-06.\n",
      " 239/1780 [===>..........................] - ETA: 10:52 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26940: setting learning rate to 4.2560728853688104e-06.\n",
      " 240/1780 [===>..........................] - ETA: 10:52 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26941: setting learning rate to 4.255720350416611e-06.\n",
      " 241/1780 [===>..........................] - ETA: 10:51 - accuracy: 0.9976 - loss: 0.0087\n",
      "Batch 26942: setting learning rate to 4.255367819248201e-06.\n",
      " 242/1780 [===>..........................] - ETA: 10:51 - accuracy: 0.9976 - loss: 0.0086\n",
      "Batch 26943: setting learning rate to 4.255015291865377e-06.\n",
      " 243/1780 [===>..........................] - ETA: 10:50 - accuracy: 0.9976 - loss: 0.0086\n",
      "Batch 26944: setting learning rate to 4.254662768269926e-06.\n",
      " 244/1780 [===>..........................] - ETA: 10:50 - accuracy: 0.9976 - loss: 0.0086\n",
      "Batch 26945: setting learning rate to 4.254310248463646e-06.\n",
      " 245/1780 [===>..........................] - ETA: 10:49 - accuracy: 0.9976 - loss: 0.0086\n",
      "Batch 26946: setting learning rate to 4.253957732448326e-06.\n",
      " 246/1780 [===>..........................] - ETA: 10:49 - accuracy: 0.9976 - loss: 0.0086\n",
      "Batch 26947: setting learning rate to 4.253605220225756e-06.\n",
      " 247/1780 [===>..........................] - ETA: 10:48 - accuracy: 0.9977 - loss: 0.0086\n",
      "Batch 26948: setting learning rate to 4.253252711797733e-06.\n",
      " 248/1780 [===>..........................] - ETA: 10:48 - accuracy: 0.9977 - loss: 0.0085\n",
      "Batch 26949: setting learning rate to 4.2529002071660455e-06.\n",
      " 249/1780 [===>..........................] - ETA: 10:48 - accuracy: 0.9977 - loss: 0.0085\n",
      "Batch 26950: setting learning rate to 4.2525477063324846e-06.\n",
      " 250/1780 [===>..........................] - ETA: 10:47 - accuracy: 0.9977 - loss: 0.0085\n",
      "Batch 26951: setting learning rate to 4.252195209298847e-06.\n",
      " 251/1780 [===>..........................] - ETA: 10:46 - accuracy: 0.9977 - loss: 0.0084\n",
      "Batch 26952: setting learning rate to 4.251842716066922e-06.\n",
      " 252/1780 [===>..........................] - ETA: 10:46 - accuracy: 0.9977 - loss: 0.0084\n",
      "Batch 26953: setting learning rate to 4.2514902266385e-06.\n",
      " 253/1780 [===>..........................] - ETA: 10:45 - accuracy: 0.9977 - loss: 0.0084\n",
      "Batch 26954: setting learning rate to 4.251137741015376e-06.\n",
      " 254/1780 [===>..........................] - ETA: 10:45 - accuracy: 0.9977 - loss: 0.0084\n",
      "Batch 26955: setting learning rate to 4.25078525919934e-06.\n",
      " 255/1780 [===>..........................] - ETA: 10:44 - accuracy: 0.9977 - loss: 0.0084\n",
      "Batch 26956: setting learning rate to 4.250432781192184e-06.\n",
      " 256/1780 [===>..........................] - ETA: 10:44 - accuracy: 0.9977 - loss: 0.0084\n",
      "Batch 26957: setting learning rate to 4.250080306995701e-06.\n",
      " 257/1780 [===>..........................] - ETA: 10:44 - accuracy: 0.9978 - loss: 0.0083\n",
      "Batch 26958: setting learning rate to 4.249727836611684e-06.\n",
      " 258/1780 [===>..........................] - ETA: 10:43 - accuracy: 0.9978 - loss: 0.0083\n",
      "Batch 26959: setting learning rate to 4.24937537004192e-06.\n",
      " 259/1780 [===>..........................] - ETA: 10:42 - accuracy: 0.9978 - loss: 0.0083\n",
      "Batch 26960: setting learning rate to 4.249022907288208e-06.\n",
      " 260/1780 [===>..........................] - ETA: 10:42 - accuracy: 0.9978 - loss: 0.0082\n",
      "Batch 26961: setting learning rate to 4.2486704483523335e-06.\n",
      " 261/1780 [===>..........................] - ETA: 10:41 - accuracy: 0.9978 - loss: 0.0083\n",
      "Batch 26962: setting learning rate to 4.248317993236093e-06.\n",
      " 262/1780 [===>..........................] - ETA: 10:41 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26963: setting learning rate to 4.247965541941276e-06.\n",
      " 263/1780 [===>..........................] - ETA: 10:41 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26964: setting learning rate to 4.247613094469671e-06.\n",
      " 264/1780 [===>..........................] - ETA: 10:40 - accuracy: 0.9978 - loss: 0.0083\n",
      "Batch 26965: setting learning rate to 4.247260650823077e-06.\n",
      " 265/1780 [===>..........................] - ETA: 10:40 - accuracy: 0.9978 - loss: 0.0082\n",
      "Batch 26966: setting learning rate to 4.2469082110032824e-06.\n",
      " 266/1780 [===>..........................] - ETA: 10:39 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26967: setting learning rate to 4.2465557750120765e-06.\n",
      " 267/1780 [===>..........................] - ETA: 10:39 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26968: setting learning rate to 4.246203342851255e-06.\n",
      " 268/1780 [===>..........................] - ETA: 10:38 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26969: setting learning rate to 4.245850914522607e-06.\n",
      " 269/1780 [===>..........................] - ETA: 10:38 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26970: setting learning rate to 4.245498490027924e-06.\n",
      " 270/1780 [===>..........................] - ETA: 10:37 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26971: setting learning rate to 4.245146069369e-06.\n",
      " 271/1780 [===>..........................] - ETA: 10:36 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26972: setting learning rate to 4.244793652547626e-06.\n",
      " 272/1780 [===>..........................] - ETA: 10:36 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26973: setting learning rate to 4.244441239565591e-06.\n",
      " 273/1780 [===>..........................] - ETA: 10:36 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26974: setting learning rate to 4.24408883042469e-06.\n",
      " 274/1780 [===>..........................] - ETA: 10:35 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26975: setting learning rate to 4.243736425126712e-06.\n",
      " 275/1780 [===>..........................] - ETA: 10:34 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26976: setting learning rate to 4.24338402367345e-06.\n",
      " 276/1780 [===>..........................] - ETA: 10:34 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26977: setting learning rate to 4.243031626066696e-06.\n",
      " 277/1780 [===>..........................] - ETA: 10:34 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26978: setting learning rate to 4.242679232308239e-06.\n",
      " 278/1780 [===>..........................] - ETA: 10:33 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26979: setting learning rate to 4.242326842399874e-06.\n",
      " 279/1780 [===>..........................] - ETA: 10:33 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26980: setting learning rate to 4.2419744563433896e-06.\n",
      " 280/1780 [===>..........................] - ETA: 10:32 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26981: setting learning rate to 4.241622074140579e-06.\n",
      " 281/1780 [===>..........................] - ETA: 10:32 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26982: setting learning rate to 4.241269695793232e-06.\n",
      " 282/1780 [===>..........................] - ETA: 10:31 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26983: setting learning rate to 4.240917321303144e-06.\n",
      " 283/1780 [===>..........................] - ETA: 10:31 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26984: setting learning rate to 4.240564950672101e-06.\n",
      " 284/1780 [===>..........................] - ETA: 10:30 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26985: setting learning rate to 4.240212583901897e-06.\n",
      " 285/1780 [===>..........................] - ETA: 10:30 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26986: setting learning rate to 4.239860220994325e-06.\n",
      " 286/1780 [===>..........................] - ETA: 10:29 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26987: setting learning rate to 4.239507861951172e-06.\n",
      " 287/1780 [===>..........................] - ETA: 10:29 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26988: setting learning rate to 4.239155506774235e-06.\n",
      " 288/1780 [===>..........................] - ETA: 10:28 - accuracy: 0.9977 - loss: 0.0082\n",
      "Batch 26989: setting learning rate to 4.238803155465301e-06.\n",
      " 289/1780 [===>..........................] - ETA: 10:28 - accuracy: 0.9976 - loss: 0.0084\n",
      "Batch 26990: setting learning rate to 4.238450808026161e-06.\n",
      " 290/1780 [===>..........................] - ETA: 10:27 - accuracy: 0.9976 - loss: 0.0084\n",
      "Batch 26991: setting learning rate to 4.23809846445861e-06.\n",
      " 291/1780 [===>..........................] - ETA: 10:26 - accuracy: 0.9976 - loss: 0.0084\n",
      "Batch 26992: setting learning rate to 4.237746124764438e-06.\n",
      " 292/1780 [===>..........................] - ETA: 10:26 - accuracy: 0.9976 - loss: 0.0083\n",
      "Batch 26993: setting learning rate to 4.237393788945432e-06.\n",
      " 293/1780 [===>..........................] - ETA: 10:25 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26994: setting learning rate to 4.237041457003389e-06.\n",
      " 294/1780 [===>..........................] - ETA: 10:25 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26995: setting learning rate to 4.236689128940098e-06.\n",
      " 295/1780 [===>..........................] - ETA: 10:24 - accuracy: 0.9977 - loss: 0.0083\n",
      "Batch 26996: setting learning rate to 4.236336804757348e-06.\n",
      " 296/1780 [===>..........................] - ETA: 10:24 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 26997: setting learning rate to 4.235984484456934e-06.\n",
      " 297/1780 [====>.........................] - ETA: 10:23 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 26998: setting learning rate to 4.2356321680406436e-06.\n",
      " 298/1780 [====>.........................] - ETA: 10:23 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 26999: setting learning rate to 4.235279855510271e-06.\n",
      " 299/1780 [====>.........................] - ETA: 10:22 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27000: setting learning rate to 4.234927546867605e-06.\n",
      " 300/1780 [====>.........................] - ETA: 10:22 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27001: setting learning rate to 4.234575242114437e-06.\n",
      " 301/1780 [====>.........................] - ETA: 10:21 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27002: setting learning rate to 4.23422294125256e-06.\n",
      " 302/1780 [====>.........................] - ETA: 10:21 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27003: setting learning rate to 4.233870644283762e-06.\n",
      " 303/1780 [====>.........................] - ETA: 10:21 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27004: setting learning rate to 4.233518351209837e-06.\n",
      " 304/1780 [====>.........................] - ETA: 10:20 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27005: setting learning rate to 4.233166062032575e-06.\n",
      " 305/1780 [====>.........................] - ETA: 10:20 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27006: setting learning rate to 4.232813776753766e-06.\n",
      " 306/1780 [====>.........................] - ETA: 10:19 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27007: setting learning rate to 4.232461495375201e-06.\n",
      " 307/1780 [====>.........................] - ETA: 10:19 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27008: setting learning rate to 4.232109217898671e-06.\n",
      " 308/1780 [====>.........................] - ETA: 10:18 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27009: setting learning rate to 4.231756944325969e-06.\n",
      " 309/1780 [====>.........................] - ETA: 10:18 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27010: setting learning rate to 4.231404674658883e-06.\n",
      " 310/1780 [====>.........................] - ETA: 10:17 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27011: setting learning rate to 4.231052408899206e-06.\n",
      " 311/1780 [====>.........................] - ETA: 10:17 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27012: setting learning rate to 4.230700147048729e-06.\n",
      " 312/1780 [====>.........................] - ETA: 10:16 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27013: setting learning rate to 4.23034788910924e-06.\n",
      " 313/1780 [====>.........................] - ETA: 10:16 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27014: setting learning rate to 4.229995635082533e-06.\n",
      " 314/1780 [====>.........................] - ETA: 10:15 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27015: setting learning rate to 4.229643384970396e-06.\n",
      " 315/1780 [====>.........................] - ETA: 10:15 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27016: setting learning rate to 4.229291138774623e-06.\n",
      " 316/1780 [====>.........................] - ETA: 10:14 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27017: setting learning rate to 4.228938896497003e-06.\n",
      " 317/1780 [====>.........................] - ETA: 10:14 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27018: setting learning rate to 4.228586658139325e-06.\n",
      " 318/1780 [====>.........................] - ETA: 10:14 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27019: setting learning rate to 4.228234423703384e-06.\n",
      " 319/1780 [====>.........................] - ETA: 10:13 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27020: setting learning rate to 4.227882193190968e-06.\n",
      " 320/1780 [====>.........................] - ETA: 10:13 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27021: setting learning rate to 4.227529966603865e-06.\n",
      " 321/1780 [====>.........................] - ETA: 10:12 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27022: setting learning rate to 4.227177743943872e-06.\n",
      " 322/1780 [====>.........................] - ETA: 10:12 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27023: setting learning rate to 4.226825525212776e-06.\n",
      " 323/1780 [====>.........................] - ETA: 10:11 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27024: setting learning rate to 4.2264733104123645e-06.\n",
      " 324/1780 [====>.........................] - ETA: 10:11 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27025: setting learning rate to 4.226121099544434e-06.\n",
      " 325/1780 [====>.........................] - ETA: 10:10 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27026: setting learning rate to 4.225768892610773e-06.\n",
      " 326/1780 [====>.........................] - ETA: 10:10 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27027: setting learning rate to 4.225416689613169e-06.\n",
      " 327/1780 [====>.........................] - ETA: 10:09 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27028: setting learning rate to 4.225064490553418e-06.\n",
      " 328/1780 [====>.........................] - ETA: 10:09 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27029: setting learning rate to 4.224712295433306e-06.\n",
      " 329/1780 [====>.........................] - ETA: 10:09 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27030: setting learning rate to 4.224360104254625e-06.\n",
      " 330/1780 [====>.........................] - ETA: 10:08 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27031: setting learning rate to 4.224007917019165e-06.\n",
      " 331/1780 [====>.........................] - ETA: 10:08 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27032: setting learning rate to 4.223655733728719e-06.\n",
      " 332/1780 [====>.........................] - ETA: 10:07 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27033: setting learning rate to 4.223303554385075e-06.\n",
      " 333/1780 [====>.........................] - ETA: 10:07 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27034: setting learning rate to 4.222951378990024e-06.\n",
      " 334/1780 [====>.........................] - ETA: 10:06 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27035: setting learning rate to 4.222599207545355e-06.\n",
      " 335/1780 [====>.........................] - ETA: 10:06 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27036: setting learning rate to 4.22224704005286e-06.\n",
      " 336/1780 [====>.........................] - ETA: 10:05 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27037: setting learning rate to 4.221894876514331e-06.\n",
      " 337/1780 [====>.........................] - ETA: 10:05 - accuracy: 0.9974 - loss: 0.0090\n",
      "Batch 27038: setting learning rate to 4.221542716931554e-06.\n",
      " 338/1780 [====>.........................] - ETA: 10:04 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27039: setting learning rate to 4.221190561306323e-06.\n",
      " 339/1780 [====>.........................] - ETA: 10:04 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27040: setting learning rate to 4.220838409640428e-06.\n",
      " 340/1780 [====>.........................] - ETA: 10:03 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27041: setting learning rate to 4.220486261935656e-06.\n",
      " 341/1780 [====>.........................] - ETA: 10:03 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27042: setting learning rate to 4.220134118193802e-06.\n",
      " 342/1780 [====>.........................] - ETA: 10:02 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27043: setting learning rate to 4.219781978416652e-06.\n",
      " 343/1780 [====>.........................] - ETA: 10:02 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27044: setting learning rate to 4.219429842605997e-06.\n",
      " 344/1780 [====>.........................] - ETA: 10:01 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27045: setting learning rate to 4.219077710763631e-06.\n",
      " 345/1780 [====>.........................] - ETA: 10:01 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27046: setting learning rate to 4.218725582891342e-06.\n",
      " 346/1780 [====>.........................] - ETA: 10:01 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27047: setting learning rate to 4.218373458990916e-06.\n",
      " 347/1780 [====>.........................] - ETA: 10:00 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27048: setting learning rate to 4.218021339064149e-06.\n",
      " 348/1780 [====>.........................] - ETA: 10:00 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27049: setting learning rate to 4.217669223112827e-06.\n",
      " 349/1780 [====>.........................] - ETA: 9:59 - accuracy: 0.9974 - loss: 0.0088 \n",
      "Batch 27050: setting learning rate to 4.217317111138744e-06.\n",
      " 350/1780 [====>.........................] - ETA: 9:59 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27051: setting learning rate to 4.216965003143688e-06.\n",
      " 351/1780 [====>.........................] - ETA: 9:58 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27052: setting learning rate to 4.216612899129447e-06.\n",
      " 352/1780 [====>.........................] - ETA: 9:58 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27053: setting learning rate to 4.216260799097815e-06.\n",
      " 353/1780 [====>.........................] - ETA: 9:58 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27054: setting learning rate to 4.21590870305058e-06.\n",
      " 354/1780 [====>.........................] - ETA: 9:57 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27055: setting learning rate to 4.215556610989531e-06.\n",
      " 355/1780 [====>.........................] - ETA: 9:57 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27056: setting learning rate to 4.21520452291646e-06.\n",
      " 356/1780 [=====>........................] - ETA: 9:56 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27057: setting learning rate to 4.214852438833155e-06.\n",
      " 357/1780 [=====>........................] - ETA: 9:56 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27058: setting learning rate to 4.214500358741408e-06.\n",
      " 358/1780 [=====>........................] - ETA: 9:55 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27059: setting learning rate to 4.214148282643007e-06.\n",
      " 359/1780 [=====>........................] - ETA: 9:55 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27060: setting learning rate to 4.213796210539744e-06.\n",
      " 360/1780 [=====>........................] - ETA: 9:54 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27061: setting learning rate to 4.213444142433407e-06.\n",
      " 361/1780 [=====>........................] - ETA: 9:54 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27062: setting learning rate to 4.213092078325786e-06.\n",
      " 362/1780 [=====>........................] - ETA: 9:53 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27063: setting learning rate to 4.212740018218672e-06.\n",
      " 363/1780 [=====>........................] - ETA: 9:53 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27064: setting learning rate to 4.212387962113853e-06.\n",
      " 364/1780 [=====>........................] - ETA: 9:52 - accuracy: 0.9975 - loss: 0.0087\n",
      "Batch 27065: setting learning rate to 4.212035910013121e-06.\n",
      " 365/1780 [=====>........................] - ETA: 9:52 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27066: setting learning rate to 4.211683861918265e-06.\n",
      " 366/1780 [=====>........................] - ETA: 9:51 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27067: setting learning rate to 4.2113318178310735e-06.\n",
      " 367/1780 [=====>........................] - ETA: 9:51 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27068: setting learning rate to 4.210979777753339e-06.\n",
      " 368/1780 [=====>........................] - ETA: 9:50 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27069: setting learning rate to 4.210627741686847e-06.\n",
      " 369/1780 [=====>........................] - ETA: 9:50 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27070: setting learning rate to 4.210275709633391e-06.\n",
      " 370/1780 [=====>........................] - ETA: 9:49 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27071: setting learning rate to 4.20992368159476e-06.\n",
      " 371/1780 [=====>........................] - ETA: 9:49 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27072: setting learning rate to 4.209571657572741e-06.\n",
      " 372/1780 [=====>........................] - ETA: 9:48 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27073: setting learning rate to 4.209219637569128e-06.\n",
      " 373/1780 [=====>........................] - ETA: 9:48 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27074: setting learning rate to 4.208867621585707e-06.\n",
      " 374/1780 [=====>........................] - ETA: 9:47 - accuracy: 0.9975 - loss: 0.0086\n",
      "Batch 27075: setting learning rate to 4.208515609624267e-06.\n",
      " 375/1780 [=====>........................] - ETA: 9:47 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27076: setting learning rate to 4.208163601686602e-06.\n",
      " 376/1780 [=====>........................] - ETA: 9:46 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27077: setting learning rate to 4.207811597774498e-06.\n",
      " 377/1780 [=====>........................] - ETA: 9:46 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27078: setting learning rate to 4.207459597889744e-06.\n",
      " 378/1780 [=====>........................] - ETA: 9:45 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27079: setting learning rate to 4.207107602034133e-06.\n",
      " 379/1780 [=====>........................] - ETA: 9:45 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27080: setting learning rate to 4.2067556102094516e-06.\n",
      " 380/1780 [=====>........................] - ETA: 9:45 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27081: setting learning rate to 4.206403622417489e-06.\n",
      " 381/1780 [=====>........................] - ETA: 9:44 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27082: setting learning rate to 4.206051638660038e-06.\n",
      " 382/1780 [=====>........................] - ETA: 9:44 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27083: setting learning rate to 4.205699658938884e-06.\n",
      " 383/1780 [=====>........................] - ETA: 9:43 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27084: setting learning rate to 4.205347683255818e-06.\n",
      " 384/1780 [=====>........................] - ETA: 9:43 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27085: setting learning rate to 4.204995711612631e-06.\n",
      " 385/1780 [=====>........................] - ETA: 9:42 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27086: setting learning rate to 4.204643744011109e-06.\n",
      " 386/1780 [=====>........................] - ETA: 9:42 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27087: setting learning rate to 4.204291780453045e-06.\n",
      " 387/1780 [=====>........................] - ETA: 9:41 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27088: setting learning rate to 4.203939820940226e-06.\n",
      " 388/1780 [=====>........................] - ETA: 9:41 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27089: setting learning rate to 4.203587865474441e-06.\n",
      " 389/1780 [=====>........................] - ETA: 9:40 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27090: setting learning rate to 4.203235914057481e-06.\n",
      " 390/1780 [=====>........................] - ETA: 9:40 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27091: setting learning rate to 4.2028839666911345e-06.\n",
      " 391/1780 [=====>........................] - ETA: 9:39 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27092: setting learning rate to 4.2025320233771895e-06.\n",
      " 392/1780 [=====>........................] - ETA: 9:39 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27093: setting learning rate to 4.202180084117437e-06.\n",
      " 393/1780 [=====>........................] - ETA: 9:39 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27094: setting learning rate to 4.201828148913667e-06.\n",
      " 394/1780 [=====>........................] - ETA: 9:38 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27095: setting learning rate to 4.201476217767665e-06.\n",
      " 395/1780 [=====>........................] - ETA: 9:38 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27096: setting learning rate to 4.201124290681225e-06.\n",
      " 396/1780 [=====>........................] - ETA: 9:37 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27097: setting learning rate to 4.200772367656132e-06.\n",
      " 397/1780 [=====>........................] - ETA: 9:37 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27098: setting learning rate to 4.200420448694176e-06.\n",
      " 398/1780 [=====>........................] - ETA: 9:36 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27099: setting learning rate to 4.200068533797148e-06.\n",
      " 399/1780 [=====>........................] - ETA: 9:36 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27100: setting learning rate to 4.199716622966836e-06.\n",
      " 400/1780 [=====>........................] - ETA: 9:35 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27101: setting learning rate to 4.199364716205026e-06.\n",
      " 401/1780 [=====>........................] - ETA: 9:35 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27102: setting learning rate to 4.1990128135135135e-06.\n",
      " 402/1780 [=====>........................] - ETA: 9:34 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27103: setting learning rate to 4.198660914894081e-06.\n",
      " 403/1780 [=====>........................] - ETA: 9:34 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27104: setting learning rate to 4.198309020348523e-06.\n",
      " 404/1780 [=====>........................] - ETA: 9:33 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27105: setting learning rate to 4.197957129878626e-06.\n",
      " 405/1780 [=====>........................] - ETA: 9:33 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27106: setting learning rate to 4.197605243486176e-06.\n",
      " 406/1780 [=====>........................] - ETA: 9:33 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27107: setting learning rate to 4.197253361172967e-06.\n",
      " 407/1780 [=====>........................] - ETA: 9:32 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27108: setting learning rate to 4.1969014829407864e-06.\n",
      " 408/1780 [=====>........................] - ETA: 9:32 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27109: setting learning rate to 4.19654960879142e-06.\n",
      " 409/1780 [=====>........................] - ETA: 9:31 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27110: setting learning rate to 4.196197738726661e-06.\n",
      " 410/1780 [=====>........................] - ETA: 9:31 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27111: setting learning rate to 4.195845872748294e-06.\n",
      " 411/1780 [=====>........................] - ETA: 9:31 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27112: setting learning rate to 4.195494010858112e-06.\n",
      " 412/1780 [=====>........................] - ETA: 9:30 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27113: setting learning rate to 4.195142153057902e-06.\n",
      " 413/1780 [=====>........................] - ETA: 9:30 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27114: setting learning rate to 4.1947902993494525e-06.\n",
      " 414/1780 [=====>........................] - ETA: 9:29 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27115: setting learning rate to 4.194438449734552e-06.\n",
      " 415/1780 [=====>........................] - ETA: 9:29 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27116: setting learning rate to 4.19408660421499e-06.\n",
      " 416/1780 [======>.......................] - ETA: 9:28 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27117: setting learning rate to 4.193734762792556e-06.\n",
      " 417/1780 [======>.......................] - ETA: 9:28 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27118: setting learning rate to 4.193382925469035e-06.\n",
      " 418/1780 [======>.......................] - ETA: 9:27 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27119: setting learning rate to 4.193031092246221e-06.\n",
      " 419/1780 [======>.......................] - ETA: 9:27 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27120: setting learning rate to 4.1926792631258974e-06.\n",
      " 420/1780 [======>.......................] - ETA: 9:27 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27121: setting learning rate to 4.192327438109858e-06.\n",
      " 421/1780 [======>.......................] - ETA: 9:26 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27122: setting learning rate to 4.1919756171998886e-06.\n",
      " 422/1780 [======>.......................] - ETA: 9:26 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27123: setting learning rate to 4.191623800397776e-06.\n",
      " 423/1780 [======>.......................] - ETA: 9:25 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27124: setting learning rate to 4.191271987705312e-06.\n",
      " 424/1780 [======>.......................] - ETA: 9:25 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27125: setting learning rate to 4.190920179124285e-06.\n",
      " 425/1780 [======>.......................] - ETA: 9:24 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27126: setting learning rate to 4.190568374656481e-06.\n",
      " 426/1780 [======>.......................] - ETA: 9:24 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27127: setting learning rate to 4.1902165743036904e-06.\n",
      " 427/1780 [======>.......................] - ETA: 9:23 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27128: setting learning rate to 4.189864778067703e-06.\n",
      " 428/1780 [======>.......................] - ETA: 9:23 - accuracy: 0.9974 - loss: 0.0082\n",
      "Batch 27129: setting learning rate to 4.189512985950303e-06.\n",
      " 429/1780 [======>.......................] - ETA: 9:23 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27130: setting learning rate to 4.189161197953283e-06.\n",
      " 430/1780 [======>.......................] - ETA: 9:22 - accuracy: 0.9975 - loss: 0.0082\n",
      "Batch 27131: setting learning rate to 4.18880941407843e-06.\n",
      " 431/1780 [======>.......................] - ETA: 9:22 - accuracy: 0.9975 - loss: 0.0081\n",
      "Batch 27132: setting learning rate to 4.188457634327531e-06.\n",
      " 432/1780 [======>.......................] - ETA: 9:21 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27133: setting learning rate to 4.188105858702376e-06.\n",
      " 433/1780 [======>.......................] - ETA: 9:21 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27134: setting learning rate to 4.187754087204754e-06.\n",
      " 434/1780 [======>.......................] - ETA: 9:21 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27135: setting learning rate to 4.187402319836451e-06.\n",
      " 435/1780 [======>.......................] - ETA: 9:20 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27136: setting learning rate to 4.187050556599259e-06.\n",
      " 436/1780 [======>.......................] - ETA: 9:20 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27137: setting learning rate to 4.186698797494961e-06.\n",
      " 437/1780 [======>.......................] - ETA: 9:19 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27138: setting learning rate to 4.18634704252535e-06.\n",
      " 438/1780 [======>.......................] - ETA: 9:19 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27139: setting learning rate to 4.185995291692213e-06.\n",
      " 439/1780 [======>.......................] - ETA: 9:18 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27140: setting learning rate to 4.185643544997337e-06.\n",
      " 440/1780 [======>.......................] - ETA: 9:18 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27141: setting learning rate to 4.185291802442511e-06.\n",
      " 441/1780 [======>.......................] - ETA: 9:17 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27142: setting learning rate to 4.184940064029524e-06.\n",
      " 442/1780 [======>.......................] - ETA: 9:17 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27143: setting learning rate to 4.184588329760163e-06.\n",
      " 443/1780 [======>.......................] - ETA: 9:17 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27144: setting learning rate to 4.184236599636216e-06.\n",
      " 444/1780 [======>.......................] - ETA: 9:16 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27145: setting learning rate to 4.183884873659473e-06.\n",
      " 445/1780 [======>.......................] - ETA: 9:16 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27146: setting learning rate to 4.183533151831718e-06.\n",
      " 446/1780 [======>.......................] - ETA: 9:15 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27147: setting learning rate to 4.1831814341547455e-06.\n",
      " 447/1780 [======>.......................] - ETA: 9:15 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27148: setting learning rate to 4.182829720630339e-06.\n",
      " 448/1780 [======>.......................] - ETA: 9:14 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27149: setting learning rate to 4.182478011260286e-06.\n",
      " 449/1780 [======>.......................] - ETA: 9:14 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27150: setting learning rate to 4.182126306046377e-06.\n",
      " 450/1780 [======>.......................] - ETA: 9:14 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27151: setting learning rate to 4.181774604990401e-06.\n",
      " 451/1780 [======>.......................] - ETA: 9:13 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27152: setting learning rate to 4.18142290809414e-06.\n",
      " 452/1780 [======>.......................] - ETA: 9:13 - accuracy: 0.9974 - loss: 0.0083\n",
      "Batch 27153: setting learning rate to 4.181071215359389e-06.\n",
      " 453/1780 [======>.......................] - ETA: 9:12 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27154: setting learning rate to 4.180719526787931e-06.\n",
      " 454/1780 [======>.......................] - ETA: 9:12 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27155: setting learning rate to 4.180367842381558e-06.\n",
      " 455/1780 [======>.......................] - ETA: 9:12 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27156: setting learning rate to 4.180016162142056e-06.\n",
      " 456/1780 [======>.......................] - ETA: 9:11 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27157: setting learning rate to 4.179664486071209e-06.\n",
      " 457/1780 [======>.......................] - ETA: 9:11 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27158: setting learning rate to 4.179312814170812e-06.\n",
      " 458/1780 [======>.......................] - ETA: 9:10 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27159: setting learning rate to 4.17896114644265e-06.\n",
      " 459/1780 [======>.......................] - ETA: 9:10 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27160: setting learning rate to 4.1786094828885075e-06.\n",
      " 460/1780 [======>.......................] - ETA: 9:10 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27161: setting learning rate to 4.178257823510177e-06.\n",
      " 461/1780 [======>.......................] - ETA: 9:09 - accuracy: 0.9974 - loss: 0.0089\n",
      "Batch 27162: setting learning rate to 4.177906168309442e-06.\n",
      " 462/1780 [======>.......................] - ETA: 9:09 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27163: setting learning rate to 4.177554517288094e-06.\n",
      " 463/1780 [======>.......................] - ETA: 9:08 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27164: setting learning rate to 4.17720287044792e-06.\n",
      " 464/1780 [======>.......................] - ETA: 9:08 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27165: setting learning rate to 4.176851227790706e-06.\n",
      " 465/1780 [======>.......................] - ETA: 9:07 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27166: setting learning rate to 4.176499589318241e-06.\n",
      " 466/1780 [======>.......................] - ETA: 9:07 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27167: setting learning rate to 4.176147955032312e-06.\n",
      " 467/1780 [======>.......................] - ETA: 9:06 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27168: setting learning rate to 4.175796324934708e-06.\n",
      " 468/1780 [======>.......................] - ETA: 9:06 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27169: setting learning rate to 4.175444699027214e-06.\n",
      " 469/1780 [======>.......................] - ETA: 9:05 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27170: setting learning rate to 4.17509307731162e-06.\n",
      " 470/1780 [======>.......................] - ETA: 9:05 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27171: setting learning rate to 4.174741459789715e-06.\n",
      " 471/1780 [======>.......................] - ETA: 9:05 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27172: setting learning rate to 4.1743898464632805e-06.\n",
      " 472/1780 [======>.......................] - ETA: 9:04 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27173: setting learning rate to 4.17403823733411e-06.\n",
      " 473/1780 [======>.......................] - ETA: 9:04 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27174: setting learning rate to 4.173686632403988e-06.\n",
      " 474/1780 [======>.......................] - ETA: 9:03 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27175: setting learning rate to 4.173335031674704e-06.\n",
      " 475/1780 [=======>......................] - ETA: 9:03 - accuracy: 0.9974 - loss: 0.0088\n",
      "Batch 27176: setting learning rate to 4.172983435148045e-06.\n",
      " 476/1780 [=======>......................] - ETA: 9:02 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27177: setting learning rate to 4.172631842825796e-06.\n",
      " 477/1780 [=======>......................] - ETA: 9:02 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27178: setting learning rate to 4.172280254709748e-06.\n",
      " 478/1780 [=======>......................] - ETA: 9:01 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27179: setting learning rate to 4.171928670801687e-06.\n",
      " 479/1780 [=======>......................] - ETA: 9:01 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27180: setting learning rate to 4.171577091103397e-06.\n",
      " 480/1780 [=======>......................] - ETA: 9:01 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27181: setting learning rate to 4.171225515616672e-06.\n",
      " 481/1780 [=======>......................] - ETA: 9:00 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27182: setting learning rate to 4.170873944343295e-06.\n",
      " 482/1780 [=======>......................] - ETA: 9:00 - accuracy: 0.9974 - loss: 0.0087\n",
      "Batch 27183: setting learning rate to 4.170522377285052e-06.\n",
      " 483/1780 [=======>......................] - ETA: 8:59 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27184: setting learning rate to 4.170170814443736e-06.\n",
      " 484/1780 [=======>......................] - ETA: 8:59 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27185: setting learning rate to 4.169819255821129e-06.\n",
      " 485/1780 [=======>......................] - ETA: 8:58 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27186: setting learning rate to 4.169467701419019e-06.\n",
      " 486/1780 [=======>......................] - ETA: 8:58 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27187: setting learning rate to 4.169116151239196e-06.\n",
      " 487/1780 [=======>......................] - ETA: 8:57 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27188: setting learning rate to 4.168764605283444e-06.\n",
      " 488/1780 [=======>......................] - ETA: 8:57 - accuracy: 0.9974 - loss: 0.0086\n",
      "Batch 27189: setting learning rate to 4.1684130635535515e-06.\n",
      " 489/1780 [=======>......................] - ETA: 8:56 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27190: setting learning rate to 4.168061526051307e-06.\n",
      " 490/1780 [=======>......................] - ETA: 8:56 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27191: setting learning rate to 4.167709992778496e-06.\n",
      " 491/1780 [=======>......................] - ETA: 8:56 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27192: setting learning rate to 4.167358463736907e-06.\n",
      " 492/1780 [=======>......................] - ETA: 8:55 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27193: setting learning rate to 4.167006938928325e-06.\n",
      " 493/1780 [=======>......................] - ETA: 8:55 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27194: setting learning rate to 4.166655418354539e-06.\n",
      " 494/1780 [=======>......................] - ETA: 8:54 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27195: setting learning rate to 4.166303902017336e-06.\n",
      " 495/1780 [=======>......................] - ETA: 8:54 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27196: setting learning rate to 4.165952389918503e-06.\n",
      " 496/1780 [=======>......................] - ETA: 8:53 - accuracy: 0.9974 - loss: 0.0084\n",
      "Batch 27197: setting learning rate to 4.165600882059823e-06.\n",
      " 497/1780 [=======>......................] - ETA: 8:53 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27198: setting learning rate to 4.16524937844309e-06.\n",
      " 498/1780 [=======>......................] - ETA: 8:52 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27199: setting learning rate to 4.164897879070088e-06.\n",
      " 499/1780 [=======>......................] - ETA: 8:52 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27200: setting learning rate to 4.1645463839426e-06.\n",
      " 500/1780 [=======>......................] - ETA: 8:51 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27201: setting learning rate to 4.16419489306242e-06.\n",
      " 501/1780 [=======>......................] - ETA: 8:51 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27202: setting learning rate to 4.16384340643133e-06.\n",
      " 502/1780 [=======>......................] - ETA: 8:51 - accuracy: 0.9974 - loss: 0.0085\n",
      "Batch 27203: setting learning rate to 4.163491924051117e-06.\n",
      " 503/1780 [=======>......................] - ETA: 8:50 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27204: setting learning rate to 4.163140445923571e-06.\n",
      " 504/1780 [=======>......................] - ETA: 8:50 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27205: setting learning rate to 4.162788972050477e-06.\n",
      " 505/1780 [=======>......................] - ETA: 8:49 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27206: setting learning rate to 4.162437502433619e-06.\n",
      " 506/1780 [=======>......................] - ETA: 8:49 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27207: setting learning rate to 4.162086037074789e-06.\n",
      " 507/1780 [=======>......................] - ETA: 8:48 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27208: setting learning rate to 4.16173457597577e-06.\n",
      " 508/1780 [=======>......................] - ETA: 8:48 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27209: setting learning rate to 4.161383119138352e-06.\n",
      " 509/1780 [=======>......................] - ETA: 8:47 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27210: setting learning rate to 4.1610316665643195e-06.\n",
      " 510/1780 [=======>......................] - ETA: 8:47 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27211: setting learning rate to 4.160680218255458e-06.\n",
      " 511/1780 [=======>......................] - ETA: 8:47 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27212: setting learning rate to 4.1603287742135575e-06.\n",
      " 512/1780 [=======>......................] - ETA: 8:46 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27213: setting learning rate to 4.159977334440404e-06.\n",
      " 513/1780 [=======>......................] - ETA: 8:46 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27214: setting learning rate to 4.15962589893778e-06.\n",
      " 514/1780 [=======>......................] - ETA: 8:45 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27215: setting learning rate to 4.159274467707478e-06.\n",
      " 515/1780 [=======>......................] - ETA: 8:45 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27216: setting learning rate to 4.158923040751281e-06.\n",
      " 516/1780 [=======>......................] - ETA: 8:44 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27217: setting learning rate to 4.158571618070976e-06.\n",
      " 517/1780 [=======>......................] - ETA: 8:44 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27218: setting learning rate to 4.15822019966835e-06.\n",
      " 518/1780 [=======>......................] - ETA: 8:43 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27219: setting learning rate to 4.157868785545191e-06.\n",
      " 519/1780 [=======>......................] - ETA: 8:43 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27220: setting learning rate to 4.1575173757032834e-06.\n",
      " 520/1780 [=======>......................] - ETA: 8:43 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27221: setting learning rate to 4.157165970144415e-06.\n",
      " 521/1780 [=======>......................] - ETA: 8:42 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27222: setting learning rate to 4.156814568870371e-06.\n",
      " 522/1780 [=======>......................] - ETA: 8:42 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27223: setting learning rate to 4.156463171882938e-06.\n",
      " 523/1780 [=======>......................] - ETA: 8:41 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27224: setting learning rate to 4.156111779183905e-06.\n",
      " 524/1780 [=======>......................] - ETA: 8:41 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27225: setting learning rate to 4.155760390775054e-06.\n",
      " 525/1780 [=======>......................] - ETA: 8:40 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27226: setting learning rate to 4.155409006658177e-06.\n",
      " 526/1780 [=======>......................] - ETA: 8:40 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27227: setting learning rate to 4.155057626835056e-06.\n",
      " 527/1780 [=======>......................] - ETA: 8:40 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27228: setting learning rate to 4.154706251307477e-06.\n",
      " 528/1780 [=======>......................] - ETA: 8:39 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27229: setting learning rate to 4.15435488007723e-06.\n",
      " 529/1780 [=======>......................] - ETA: 8:39 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27230: setting learning rate to 4.1540035131460995e-06.\n",
      " 530/1780 [=======>......................] - ETA: 8:38 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27231: setting learning rate to 4.15365215051587e-06.\n",
      " 531/1780 [=======>......................] - ETA: 8:38 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27232: setting learning rate to 4.153300792188331e-06.\n",
      " 532/1780 [=======>......................] - ETA: 8:37 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27233: setting learning rate to 4.152949438165267e-06.\n",
      " 533/1780 [=======>......................] - ETA: 8:37 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27234: setting learning rate to 4.152598088448462e-06.\n",
      " 534/1780 [========>.....................] - ETA: 8:36 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27235: setting learning rate to 4.152246743039708e-06.\n",
      " 535/1780 [========>.....................] - ETA: 8:36 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27236: setting learning rate to 4.151895401940786e-06.\n",
      " 536/1780 [========>.....................] - ETA: 8:36 - accuracy: 0.9976 - loss: 0.0084\n",
      "Batch 27237: setting learning rate to 4.151544065153482e-06.\n",
      " 537/1780 [========>.....................] - ETA: 8:35 - accuracy: 0.9976 - loss: 0.0084\n",
      "Batch 27238: setting learning rate to 4.151192732679586e-06.\n",
      " 538/1780 [========>.....................] - ETA: 8:35 - accuracy: 0.9976 - loss: 0.0084\n",
      "Batch 27239: setting learning rate to 4.150841404520882e-06.\n",
      " 539/1780 [========>.....................] - ETA: 8:34 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27240: setting learning rate to 4.150490080679156e-06.\n",
      " 540/1780 [========>.....................] - ETA: 8:34 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27241: setting learning rate to 4.150138761156195e-06.\n",
      " 541/1780 [========>.....................] - ETA: 8:33 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27242: setting learning rate to 4.1497874459537825e-06.\n",
      " 542/1780 [========>.....................] - ETA: 8:33 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27243: setting learning rate to 4.149436135073708e-06.\n",
      " 543/1780 [========>.....................] - ETA: 8:33 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27244: setting learning rate to 4.149084828517755e-06.\n",
      " 544/1780 [========>.....................] - ETA: 8:32 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27245: setting learning rate to 4.1487335262877105e-06.\n",
      " 545/1780 [========>.....................] - ETA: 8:32 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27246: setting learning rate to 4.1483822283853605e-06.\n",
      " 546/1780 [========>.....................] - ETA: 8:31 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27247: setting learning rate to 4.148030934812491e-06.\n",
      " 547/1780 [========>.....................] - ETA: 8:31 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27248: setting learning rate to 4.1476796455708865e-06.\n",
      " 548/1780 [========>.....................] - ETA: 8:31 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27249: setting learning rate to 4.1473283606623345e-06.\n",
      " 549/1780 [========>.....................] - ETA: 8:30 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27250: setting learning rate to 4.146977080088622e-06.\n",
      " 550/1780 [========>.....................] - ETA: 8:30 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27251: setting learning rate to 4.1466258038515315e-06.\n",
      " 551/1780 [========>.....................] - ETA: 8:29 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27252: setting learning rate to 4.1462745319528516e-06.\n",
      " 552/1780 [========>.....................] - ETA: 8:29 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27253: setting learning rate to 4.145923264394368e-06.\n",
      " 553/1780 [========>.....................] - ETA: 8:28 - accuracy: 0.9975 - loss: 0.0085\n",
      "Batch 27254: setting learning rate to 4.145572001177862e-06.\n",
      " 554/1780 [========>.....................] - ETA: 8:28 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27255: setting learning rate to 4.1452207423051265e-06.\n",
      " 555/1780 [========>.....................] - ETA: 8:28 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27256: setting learning rate to 4.144869487777943e-06.\n",
      " 556/1780 [========>.....................] - ETA: 8:27 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27257: setting learning rate to 4.144518237598097e-06.\n",
      " 557/1780 [========>.....................] - ETA: 8:27 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27258: setting learning rate to 4.144166991767376e-06.\n",
      " 558/1780 [========>.....................] - ETA: 8:26 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27259: setting learning rate to 4.143815750287564e-06.\n",
      " 559/1780 [========>.....................] - ETA: 8:26 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27260: setting learning rate to 4.143464513160446e-06.\n",
      " 560/1780 [========>.....................] - ETA: 8:25 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27261: setting learning rate to 4.1431132803878124e-06.\n",
      " 561/1780 [========>.....................] - ETA: 8:25 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27262: setting learning rate to 4.142762051971443e-06.\n",
      " 562/1780 [========>.....................] - ETA: 8:25 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27263: setting learning rate to 4.142410827913127e-06.\n",
      " 563/1780 [========>.....................] - ETA: 8:24 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27264: setting learning rate to 4.142059608214649e-06.\n",
      " 564/1780 [========>.....................] - ETA: 8:24 - accuracy: 0.9975 - loss: 0.0084\n",
      "Batch 27265: setting learning rate to 4.1417083928777926e-06.\n",
      " 565/1780 [========>.....................] - ETA: 8:23 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27266: setting learning rate to 4.141357181904348e-06.\n",
      " 566/1780 [========>.....................] - ETA: 8:23 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27267: setting learning rate to 4.141005975296095e-06.\n",
      " 567/1780 [========>.....................] - ETA: 8:22 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27268: setting learning rate to 4.140654773054823e-06.\n",
      " 568/1780 [========>.....................] - ETA: 8:22 - accuracy: 0.9976 - loss: 0.0083\n",
      "Batch 27269: setting learning rate to 4.140303575182317e-06.\n",
      " 569/1780 [========>.....................] - ETA: 8:22 - accuracy: 0.9976 - loss: 0.0083\n",
      "Batch 27270: setting learning rate to 4.139952381680361e-06.\n",
      " 570/1780 [========>.....................] - ETA: 8:21 - accuracy: 0.9976 - loss: 0.0083\n",
      "Batch 27271: setting learning rate to 4.139601192550741e-06.\n",
      " 571/1780 [========>.....................] - ETA: 8:21 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27272: setting learning rate to 4.139250007795243e-06.\n",
      " 572/1780 [========>.....................] - ETA: 8:20 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27273: setting learning rate to 4.138898827415652e-06.\n",
      " 573/1780 [========>.....................] - ETA: 8:20 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27274: setting learning rate to 4.1385476514137515e-06.\n",
      " 574/1780 [========>.....................] - ETA: 8:19 - accuracy: 0.9976 - loss: 0.0083\n",
      "Batch 27275: setting learning rate to 4.138196479791331e-06.\n",
      " 575/1780 [========>.....................] - ETA: 8:19 - accuracy: 0.9976 - loss: 0.0083\n",
      "Batch 27276: setting learning rate to 4.137845312550173e-06.\n",
      " 576/1780 [========>.....................] - ETA: 8:18 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27277: setting learning rate to 4.137494149692061e-06.\n",
      " 577/1780 [========>.....................] - ETA: 8:18 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27278: setting learning rate to 4.137142991218785e-06.\n",
      " 578/1780 [========>.....................] - ETA: 8:18 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27279: setting learning rate to 4.136791837132125e-06.\n",
      " 579/1780 [========>.....................] - ETA: 8:17 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27280: setting learning rate to 4.136440687433871e-06.\n",
      " 580/1780 [========>.....................] - ETA: 8:17 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27281: setting learning rate to 4.136089542125806e-06.\n",
      " 581/1780 [========>.....................] - ETA: 8:16 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27282: setting learning rate to 4.1357384012097126e-06.\n",
      " 582/1780 [========>.....................] - ETA: 8:16 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27283: setting learning rate to 4.1353872646873814e-06.\n",
      " 583/1780 [========>.....................] - ETA: 8:15 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27284: setting learning rate to 4.135036132560595e-06.\n",
      " 584/1780 [========>.....................] - ETA: 8:15 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27285: setting learning rate to 4.134685004831135e-06.\n",
      " 585/1780 [========>.....................] - ETA: 8:15 - accuracy: 0.9975 - loss: 0.0083\n",
      "Batch 27286: setting learning rate to 4.134333881500792e-06.\n",
      " 586/1780 [========>.....................] - ETA: 8:14 - accuracy: 0.9975 - loss: 0.0083"
     ]
    }
   ],
   "source": [
    "run_training(dropout = 0.5, lr_rate = 0.00001, architecture = 'mobilenet', batch = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../all_faces_bucket/trained_models/weights/mobilenet_new/highest_val_acc.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "564DXmWLDMxa"
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "1qVdSNFxjg5-",
    "outputId": "c571704d-0a0e-4a65-a7f8-15fcb6b82127"
   },
   "outputs": [],
   "source": [
    "# run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-6Yocnrq2u4F"
   ],
   "name": "25_May.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
