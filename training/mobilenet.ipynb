{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udRhv-d-2i8l"
   },
   "source": [
    "# Train preliminary models - Xception, ResNet, EfficentNet etc.\n",
    "       -- built for FF+ dataset with file structure as required by Keras' flow_from_directory method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "t7E1CjC9fqhq",
    "outputId": "abdef6eb-823c-4fb1-ad28-7521345d04fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 12 11:34:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P0    30W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# See available GPU RAM \n",
    "!nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !watch -n0.1 nvidia-smi # better way to see GPU utilisation\n",
    "# !htop # cpu threads and if they're all working\n",
    "# !pip3 install --no-cache-dir -I tensorflow==2.2 #Â use if no gpu is attached so code will run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Llx-HRnYiWQU",
    "outputId": "6e6a3556-fbb1-4972-b046-8586183f768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.1-dlenv_tfe\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "3DRA3QPDgLLR",
    "outputId": "ed171b89-378d-469d-a254-c291be71af4d"
   },
   "outputs": [],
   "source": [
    "# Required for EfficientNet\n",
    "# !pip install git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtO5vELz8i3-"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ7mWThq32JA"
   },
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture):\n",
    "    '''Builds a specified network with the selected dropout after the last dense layer.\n",
    "\n",
    "    Architectures that can be selected are:\n",
    "    vgg, xception, resnet50, mobilenet, efficientnet, densenet\n",
    "    \n",
    "    Optimiser is Adam, with a provided learning rate (lr_rate) and fixed\n",
    "    decay 1e-6, loss is traditionally categorical_crossentropy.'''\n",
    "\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "    if architecture=='xception':\n",
    "        from tensorflow.keras.applications.xception import Xception\n",
    "        conv_base = Xception(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture=='vgg':\n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='resnet50':\n",
    "        from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "        conv_base = ResNet50(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='mobilenet':\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture== 'efficientnet':\n",
    "        # EfficientNetB7 has the highest top-1 accuracy on imagenet\n",
    "        # among EfficientNextB{0:7}\n",
    "        from efficientnet.tfkeras import EfficientNetB0\n",
    "        conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "        \n",
    "    elif architecture== 'densenet':\n",
    "        from tensorflow.keras.applications.densenet import DenseNet201\n",
    "        conv_base = DenseNet201(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture not in ['vgg', 'xception', 'resnet50',\n",
    "                              'mobilenet', 'efficientnet', 'densenet']:\n",
    "        return \"An unknown network is specified\"\n",
    "    \n",
    "\n",
    "    outputconv_base = conv_base.output\n",
    "    t_flat = Flatten()(outputconv_base)\n",
    "    t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
    "    t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "    t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
    "    t_do = Dropout(dropout)(t_dense3)\n",
    "    predictions = Dense(2, activation= 'softmax')(t_do)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=predictions, name = 'model')\n",
    "\n",
    "    conv_base.trainable = False # freeze the convolutional base\n",
    "    \n",
    "    # # Code below trains all layers without using any pretrained weights\n",
    "    #for layer in conv_base.layers:\n",
    "    #  layer.trainable = True\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate= lr_rate, decay=1e-6)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_rate)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen_train = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "#             width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "#             height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            brightness_range=[0.6, 1.4],\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "    \n",
    "    datagen_test = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen_train.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    val_data = datagen_test.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary train time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights\n",
    "\n",
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed. '''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/mobilenet_new\"\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # if there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # pick out accuracies out of file names\n",
    "            acc = [el[len(el)-10 : len(el)-5] for el in all_weights]\n",
    "            # get index of the first maximum accuracy\n",
    "            optimal_index = acc.index(max(acc))\n",
    "            # get the name of the file with optimal weights, load corresponding weights\n",
    "            optimal_weights = all_weights[optimal_index]\n",
    "            print(\"Loading\", path_to_weights + '/' + optimal_weights)\n",
    "            model.load_weights(path_to_weights + '/' + optimal_weights)\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture):\n",
    "    '''Takes the latest saved weights and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/mobilenet_new_model.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/mobilenet_new'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/mobilenet_new')\n",
    "\n",
    "    # Save weights - below saves every epoch where there is improvement\n",
    "    # filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/mobilenet_new/highest_val_acc.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/mobilenet_new.csv',\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    # Set learning rate config \n",
    "    sample_count = 60000 # number of training samples\n",
    "    epochs = 50 # total epochs - affects total steps (and hence speed of decay)\n",
    "    warmup_epoch = 3 # number of warmup epochs\n",
    "    batch_size = train_data.batch_size\n",
    "    learning_rate_base = 0.0002\n",
    "    total_steps = int(epochs * sample_count / batch_size)\n",
    "    warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "    \n",
    "    warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warmup_steps,\n",
    "                                        hold_base_rate_steps=2,\n",
    "                                        verbose=0)\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses, checkpoint, csv_logger, es, warm_up_lr],\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIUZirJoxsdx"
   },
   "source": [
    "## Unifying Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYFsNbZMqYTv"
   },
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture)\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in8HHH594qtA"
   },
   "source": [
    "## Train Various Model Architectures\n",
    "Note: Make sure CPUs have enough memory for each batch eg. 1 core with 3.75GB RAM cant take batches larger than 32.  \n",
    "8 CPUs with 30GB RAM typically works well for batches of 256.\n",
    "\n",
    "In this model: 2 cores/8GB and T4 used. \n",
    "\n",
    "Also note that while multiprocessing speeds up training, it interacts badly with Tensorflow and leads to deadlocks. To be on the safe side, set use_multiprocessing to False when training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz0ilB_F8F4g"
   },
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "colab_type": "code",
    "id": "_GZwsiNC7rkK",
    "outputId": "7b0348f4-e292-4cb7-af8f-49af0eda2638"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD5CAYAAADm8QjUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8deHLISsJJCEJUBYAgFCFgyLqMgiiCharVSsGyhy3W17a61ee/X+Wm+9ble9euVSitaH1uWitIqoFa8WbV1YZBEJS1kkBCGsIWzZPr8/ziSZDJNkMplkJsnn+Xjkkcw533PmM5mTvOd7lu8RVcUYY4zxR6dgF2CMMabtshAxxhjjNwsRY4wxfrMQMcYY4zcLEWOMMX6zEDHGGOO38GAX0NK6d++u6enpwS7DtFOrV68+oKrJACIyDXgaCAMWquoj7m1FJBFYBAwETgE3quo3bvPDgFXAHlW9pKHnte3atDT3bbsh7T5E0tPTWbVqVbDLMO2UiOxyfQ8DngOmAIXAShF5W1W/dWt+P7BWVS8XkUxX+8lu8+8GNgHxjT2vbdempVVv242x3VnGBMZoYJuqblfVMuA14DKPNsOAjwBUtQBIF5FUABFJAy4GFrZeycY0n4WIMYHRG9jt9rjQNc3dOuAKABEZDfQD0lzzngJ+AVS1bJnGBJaFiDGBIV6meY4p9AiQKCJrgTuBr4EKEbkE2K+qqxt8ApF5IrJKRFYVFxcHpGhjmqvdHxMxppUUAn3cHqcBRe4NVLUEmAMgIgLscH3NAi4VkelAFBAvIi+r6rUeyy8AFgDk5+fboHcmJFhPxJjAWAlkiEh/EYnECYa33RuISFfXPIC5wApVLVHV+1Q1TVXTXcv9n2eAGBOqrCdiTACoaoWI3AF8gHOK7yJV3Sgit7jmzweGAi+JSCXwLXBT0Ao2JkAsRIwJEFVdBizzmDbf7efPgYxG1vEJ8EkLlGdMi7AQMcadKpw8DEd2wZHv4PAu6JIII68LdmXGNFtZRRU7Dx5ny75jbNlXysUjejKkR1yz1mkhYjoWVTh1xAmI6pCo/rk6OMpK6y7Tf7yFiGlTPMNi237n+84Dx6mocs7JEIF+SdEWIsac4dTRekLCFRSnS+q2j4yDxH6QmO4ERtd+0LVv7VeXrkF5GcY0xj0stu4rZWsDYZGRGsfUYakMTo0jIzWWgcmxREWENbsGCxHT9pw+5iUk3L6fOlq3fUSMExJd+0K/s72ERKLzl2ZMiAqFsKiPhYgJPWXH6/YeDu+s+/jkobrtw7vUhkSf0R4h0Q+ikywkTJtQHRZb95U6gbHfCY0dXsJiUEptWAxKiWVQSsuGRX0sREzrKzsBR3fX9hw8dzmdOFC3fXhUbSj0HlkbDtVhEdPdQsK0KeWVVew8cJwtrrDYtt/57hkWfZOiyUiJY8qwVDJSY8lIiQtaWNTHQsQEXvkpV0js8n5c4vj+uu3DImtDomf2mSERm2IhYdqk9hQW9bEQMU1XcRqOFtYfEqXf123fKQK69nECYcg0V0ik1wZHbCp0ssETTNvlHhbVu6AaCosLhqUy2BUWA5Nj6RIZ+mFRHwsRc6bKcrfdTV7Ocjq2lzpjC3YKh4Q0JxAyLqjbi+jaF+J6QKe2+0diTLXqsNjq6lFUH+TeceA45ZWeYRHbrsKiPhYiHVFlBZQUNhASRaBuI5JLJ4hPcw5eD5xY96B1174Q1xPCbFMy7UdTw2Ly0PYfFvWxv/z2qKoSSvbUHxIle0Ara9tLJ4jr5YRE//PODIn4XhAWEbzXY0wLKa+sYtfB2mMWW/eXsnXfmWHRJzGawalOWGSkxDI4teOFRX0sRNqiqko49r3btREeZzmV7IGqCrcFxOkt1Fwn4RkSvSE8st6nM6atcw+LrftK2bL/WINhMSnT6VlYWDTOQiQUVVVB6b66F9C59yaOFkJVed1lYnu4XSfRt25QJKRBeOfgvBZjWpGFReuzEAkGVSjd30BI7IbKsrrLxKTUXicx/AduIZHuhEREVFBeijHBUB0WzllQTlhs21fK9gOlZ4RFRkptWFSfOmthETgWIi1BFY4fcAXEzjOPSxzdDRWn6i4T3b32Oomhl9S9ViIhDSKjg/JSjAkmJyxOsNU1kGD16bP1hcXEzJTaA9wpMURH2r+4lma/YX+owolDrh7ELu8HsCtO1l2mS5ITDClDXddK9HMLij4QGROc12JMCPAlLAD6JHVhcEocEzNTag9wW1gElf3mvam5p8R39YdE+fG6y0R1dUKhewZkTPE4LtEXOjdvuGVj2oOKyip2usLC/fTZ+sJiQmYyg1PiLCxCWMd9R04eqXtmk2dIlB2r275zvNNrSBrgca2E6ysqITivw5gQVB0W1fexqB7yY3vxccoqa69BsrBo+zrmO3VoBzyTW3daZGztLqb0c2tHhXUfLtwYU0dTwiIjJY7zhzhhkZHqjDprYdH2tal3UERigP8GyoBPVPUVv1aUkAZTf1P3NFi7p4Qx9aqorGLXIfdjFs5FeZ5hkZbYhcGpTlhkpMQx2MKi3Qv6Oysii4BLgP2qmuU2fRrwNBAGLFTVR4ArgMWq+o6IvA74FyJhETDuzmbXbkx74x4WznUWFhamYaHwjr8IPAu8VD1BRMKA54ApQCGwUkTeBtKADa5mlRhj/NLksBicTEaqExYDk2OJ6RwK/zpMKAj6lqCqK0Qk3WPyaGCbqm4HEJHXgMtwAiUNWAvY2OEmpNTTe3afnwgsAgYCp4AbVfUbEemD8yGqB1AFLFDVpwNRU21YOCHRUFhkpMRaWJgmC9UtpDew2+1xITAGeAZ4VkQuBt6pb2ERmQfMA+jbt28LlmmMo77es6p+69bsfmCtql4uIpmu9pOBCuCfVXWNiMQBq0XkQ49lfbK+8Ah/3Vxcc/rs9gPHKauoPywyXLdVtbAw/grVLcfbEW5V1ePAnMYWVtUFwAKA/Px8baS5MYFQX+/ZPQiGAb8FUNUCEUkXkVRV3QvsdU0/JiKbcD5INTlEPtt2gCc+3ELvrl0YnOqExSDXRXkWFqYlhOoWVQj0cXucBhQFqRZjfFFf79ndOpyTQz4TkdFAP5xte191A9eu3TzgS3+KuHZsP244O93CwrSaUD2usBLIEJH+IhIJzALeDnJNxjTEa+/Z4/EjQKKIrAXuBL7G2ZXlrEAkFngT+ImqlpzxBCLzRGSViKwqLi72WkR8VIQFiGlVQQ8REXkV+BwYIiKFInKTqlYAdwAfAJuAN1R1YzDrNKYRjfaeVbVEVeeoai5wPZAM7AAQkQicAHlFVd/y9gSqukBV81U1Pzk5uSVegzFNFvSPLKp6dT3TlwHLWrkcY/xV03sG9uD0nn/s3kBEugInVLUMmAusUNUSERHg98AmVX2yles2plmCHiLGtAeqWiEi1b3nMGCRqm4UkVtc8+cDQ4GXRKQS56D5Ta7FzwGuAza4dnUB3O/6IGVMSLMQMSZAvPWeXeFR/fPnQIaX5T7D+zEVY0Je0I+JGGOMabssRIwxxvjNQsQYY4zfLESMMcb4zULEGGOM39ptiIjIDBFZcPTo0WCXYowx7Va7DRFVfUdV5yUk2L3PjTGmpbTbEDHGGNPyLESMMcb4zULEGGOM3yxEjDHG+M1CxBhjjN8sRIwxxvjNQsQYY4zfLESMMcb4zULEGGOM3yxEjDHG+M1CxBhjjN8sRIwxxvjNQsQYY4zf2m2I2FDwxhjT8tptiNhQ8Ka1icg0EdksIttE5Jde5ieKyBIRWS8iX4lIlq/LGhOq2m2IGNOaRCQMeA64CBgGXC0iwzya3Q+sVdVs4Hrg6SYsa0xIshAxJjBGA9tUdbuqlgGvAZd5tBkGfASgqgVAuoik+risMSHJQsSYwOgN7HZ7XOia5m4dcAWAiIwG+gFpPi5rTEiyEDEmMMTLNPV4/AiQKCJrgTuBr4EKH5dFROaJyCoRWVVcXNzceo0JiPBgF2BMO1EI9HF7nAYUuTdQ1RJgDoCICLDD9RXd2LKu5RcACwDy8/PPCBljgsF6IsYExkogQ0T6i0gkMAt4272BiHR1zQOYC6xwBUujyxoTqqwnYkwAqGqFiNwBfACEAYtUdaOI3OKaPx8YCrwkIpXAt8BNDS0bjNdhTFNZiBgTIKq6DFjmMW2+28+fAxm+LmtMW2C7s4wxxvjNQsQYY4zfLESMMcb4zULEGGOM3yxEjDHG+M1CxBhjjN/abYjY/USMMabltdsQsfuJGGNMy2u3IWKMMablWYgYY4zxm4WIMcYYv1mIGGOM8ZuFiDHGGL9ZiBhjjPGbhYgxxhi/WYgYY4zxm4WIMcYYv1mIGGOM8ZuFiDHGGL9ZiBhjjPGbhYgxxhi/tckQEZEBIvJ7EVkc7FqMMaYjC/elkYh0BRYCWYACN6rq5019MhFZBFwC7FfVLI9504CngTBgoao+Ut96VHU7cJOFSPOVl5dTWFjIqVOngl1KSIuKiiItLY2IiIh62zS2DYtIAvAy0Bfnb+9xVX3BNe+nwFycv68NwBxVtTfFhDyfQgTnD+N9Vb1SRCKBaPeZIpICnFTVY27TBqnqNo/1vAg8C7zksXwY8BwwBSgEVorI2zh/jL/1WMeNqrrfx7pNIwoLC4mLiyM9PR0RCXY5IUlVOXjwIIWFhfTv399rm/q2YVX91q3Z7cC3qjpDRJKBzSLyCpAM3AUMU9WTIvIGMAvn7yWg7EOD8eTLB6SGNBoiIhIPjAdmA6hqGVDm0ex84FYRma6qp0TkZuByYLp7I1VdISLpXp5mNLDN1cNARF4DLlPV3+L0XEwLOXXqlAVII0SEbt26UVxc3FAzr9sw4B4iCsSJ88uOBQ4BFa554UAXESnH+ZBWFNhX4bAPDcadLx+QGuPLMZEBQDHwgoh8LSILRSTGo5D/Bd4HXhORa4AbgR81oY7ewG63x4WuaV6JSDcRmQ/kich9TXge44X9M2mcD78jX7bhZ4GhOAGxAbhbVatUdQ/wOPAdsBc4qqp/CUTdnk6dOkW3bt3sPTdA7Qek5vRMfQmRcGAk8Lyq5gHHgV96NlLVR4FTwPPApapa2oQ6vG3RWl9jVT2oqreo6kBXb+XMFdo91tuM2NjYYJcQCL5swxcCa4FeQC7wrIjEi0giTq+lv2tejIhce8YTiMwTkVUisqqRXlHDhVqAGDfN3R58CZFCoFBVv3Q9XowTKp6FnIdz4H0J8GAT6ygE+rg9TqOZ3Xm7x7ppZb5sw3OAt9SxDdgBZAIXADtUtVhVy4G3gHGeT6CqC1Q1X1Xzk5OTW+RFGNNUjYaIqn4P7BaRIa5Jk6m7nxcRyQN+h/Npag6QJCK/aUIdK4EMEenvOnA/C3i7CcubdkBVueeee8jKymLEiBG8/vrrAOzdu5fx48eTm5tLVlYWn376KZWVlcyePbum7X/+538GuXqftuHvcP5+EJFUYAiw3TV9rIhEu46XTAY2tVrl7VRFRUXjjUyz+XqdyJ3AKyKyHqcb/u8e86OBmar6D1WtAm4AdnmuREReBT4HhohIoYjcBKCqFcAdwAc4fzxvqOpGf16Qabveeust1q5dy7p161i+fDn33HMPe/fu5Y9//CMXXnhhzbzc3FzWrl3Lnj17+Oabb9iwYQNz5swJau31bcMicouI3OJq9mtgnIhsAD4C7lXVA65e/mJgDc6xkk7AglZ/Ea3oBz/4AWeddRbDhw9nwQLnpb7//vuMHDmSnJwcJk+eDEBpaSlz5sxhxIgRZGdn8+abbwJ1d4EuXryY2bNnAzB79mx+9rOfMXHiRO69916++uorxo0bR15eHuPGjWPz5s0AVFZW8vOf/7xmvf/1X//FRx99xOWXX16z3g8//JArrriiNX4dbZpPp/iq6logv4H5f/N4XI7TM/Fsd3UD61gGLPOlHtMy/u2djXxbVBLQdQ7rFc+DM4b71Pazzz7j6quvJiwsjNTUVM4//3xWrlzJqFGjuPHGGykvL+cHP/gBubm5DBgwgO3bt3PnnXdy8cUXM3Xq1IDW7Q9v27Cqznf7uQjwWqiqPkjTdwM3SzDf70WLFpGUlMTJkycZNWoUl112GTfffDMrVqygf//+HDp0CIBf//rXJCQksGHDBgAOHz7c6Lq3bNnC8uXLCQsLo6SkhBUrVhAeHs7y5cu5//77efPNN1mwYAE7duzg66+/Jjw8nEOHDpGYmMjtt99OcXExycnJvPDCC0H/cNIWtMkr1k37pOr9XIrx48ezYsUKevfuzXXXXcdLL71EYmIi69atY8KECTz33HPMnTu3las1zfHMM8+Qk5PD2LFj2b17NwsWLGD8+PE1p5kmJSUBsHz5cm6//faa5RITExtd98yZMwkLCwPg6NGjzJw5k6ysLH7605+ycePGmvXecssthIeH1zyfiHDdddfx8ssvc+TIET7//HMuuuiigL7u9sjXiw1NB+Brj6GljB8/nv/5n//hhhtu4NChQ6xYsYLHHnuMXbt20bt3b26++WaOHz/OmjVrmD59OpGRkfzwhz9k4MCBNbszjO+C9X5/8sknLF++nM8//5zo6GgmTJhATk5Oza4md6rq9ewh92mep6fGxNRegfCrX/2KiRMnsmTJEnbu3MmECRMaXO+cOXOYMWMGUVFRzJw5syZkTP2sJ2JCxuWXX052djY5OTlMmjSJRx99lB49evDJJ5+Qm5tLXl4eb775JnfffTd79uxhwoQJ5ObmMnv2bH77W69nepsQdPToURITE4mOjqagoIAvvviC06dP89e//pUdO3YA1OzOmjp1Ks8++2zNstW7s1JTU9m0aRNVVVUsWbKkwefq3du5XOfFF1+smT516lTmz59fc/C9+vl69epFr169+M1vfmMfTHxkMWuCrrTUuaRIRHjsscd47LHH6sy/4YYbuOGGG85Ybs2aNa1SnwmsadOmMX/+fLKzsxkyZAhjx44lOTmZBQsWcMUVV1BVVUVKSgoffvghDzzwALfffjtZWVmEhYXx4IMPcsUVV/DII49wySWX0KdPH7Kysmq2IU+/+MUvuOGGG3jyySeZNGlSzfS5c+eyZcsWsrOziYiI4Oabb+aOO+4A4JprrqG4uJhhw4a1yu+jrZP69kO3F/n5+bpq1apglxGyNm3axNChQ4NdRpvg7XclIqtVtd6TTlqKv9u1vd+Nu+OOO8jLy+Omm24KdimtpjnbtvVEjDHG5ayzziImJoYnnngi2KW0GRYixhjjsnr16mCX0ObYgXVjjDF+sxAxxhjjNwsRY4wxfrMQMcYY4zcLEdOmNHTvkZ07d5KVldWK1ZjWUP2eFxUVceWVV3ptM2HCBBo75fmpp57ixIkTNY+nT5/OkSNHAldoB2UhYoxpE3r16sXixYv9Xt4zRJYtW0bXrl0DUVqrUFWqqqqCXcYZLERMUN17773893//d83jhx56iH/7t39j8uTJjBw5khEjRvDnP/+5yes9depUzRDieXl5fPzxxwBs3LiR0aNHk5ubS3Z2Nlu3buX48eNcfPHF5OTkkJWVVXMfExN43t7vJ554gtLS0kbfc/ee5smTJ5k1axbZ2dlcddVVnDx5sqbdrbfeSn5+PsOHD+fBB52BkZ955hmKioqYOHEiEydOBCA9PZ0DBw4A8OSTT5KVlUVWVhZPPfVUzfMNHTqUm2++meHDhzN16tQ6z1PtnXfeYcyYMeTl5XHBBRewb98+oP5h7L0Nef/QQw/x+OOP16wzKyuLnTt31tRw2223MXLkSHbv3u319QGsXLmScePGkZOTw+jRozl27BjnnXcea9eurWlzzjnnsH79ep/fL1/YdSKm1nu/hO83BHadPUbARY/UO3vWrFn85Cc/4bbbbgPgjTfe4P333+enP/0p8fHxHDhwgLFjx3LppZc26Taezz33HAAbNmygoKCAqVOnsmXLFubPn8/dd9/NNddcQ1lZGZWVlSxbtoxevXrx7rvvAs54Sx1CCL3fUVFRLFmyxOf3/Pnnnyc6Opr169ezfv16Ro6svdnqww8/TFJSEpWVlUyePJn169dz11138eSTT/Lxxx/TvXv3OutavXo1L7zwAl9++SWqypgxYzj//PNJTExk69atvPrqq/zud7/jRz/6EW+++SbXXlv3zsXnnnsuX3zxBSLCwoULefTRR3niiSe8DmNfXFzsdcj7hmzevJkXXnihJny9vb7MzEyuuuoqXn/9dUaNGkVJSQldunRh7ty5vPjiizz11FNs2bKF06dPk52d3ehzNoX1RExQ5eXlsX//foqKili3bh2JiYn07NmT+++/n+zsbC644AL27NlT8+nOV5999hnXXXcdAJmZmfTr148tW7Zw9tln8+///u/8x3/8B7t27aJLly6MGDGC5cuXc++99/Lpp59it1RuOd7e7759+6KqTXrPV6xYUfPPPDs7u84/xjfeeIORI0eSl5fHxo0b+fbbb+tbDeBsK5dffjkxMTHExsZyxRVX8OmnnwLQv39/cnNzAedq9p07d56xfGFhIRdeeCEjRozgscceqzPcvOcw9l988YXXIe8b0q9fP8aOHdvg69u8eTM9e/Zk1KhRAMTHxxMeHs7MmTNZunQp5eXlLFq0qEUGlbSeiKnVwCfIlnTllVeyePFivv/+e2bNmsUrr7xCcXExq1evJiIigvT09DOG+25MfWPC/fjHP2bMmDG8++67XHjhhSxcuJBJkyaxevVqli1bxn333cfUqVP513/910C8tNAWIu834Nd77q2XsmPHDh5//HFWrlxJYmIis2fPbnQ9DY0f2Llz55qfw8LCvO7OuvPOO/nZz37GpZdeyieffMJDDz1Us17PGusbgj48PLzO8Q73mt2Htq/v9dW33ujoaKZMmcKf//xn3njjjUZPPvBHu+2JiMgMEVnQYXZNtGGzZs3itddeY/HixVx55ZUcPXqUlJQUIiIi+Pjjj9m164w7LTdq/PjxvPLKK4Bzp7vvvvuOIUOGsH37dgYMGMBdd93FpZdeyvr16ykqKiI6Opprr72Wn//85zY6cAvzfL+BJr/n7u/vN998U7Ofv6SkhJiYGBISEti3bx/vvfdezTJxcXEcO3bM67r+9Kc/ceLECY4fP86SJUs477zzfH497sPN/+EPf6iZ7m0Y+7PPPtvrkPfp6ek1292aNWtq5nuq7/VlZmZSVFTEypUrATh27FjNMPdz587lrrvuYtSoUT71fJqq3YaIqr6jqvNs10ToGz58OMeOHaN379707NmTa665hlWrVpGfn88rr7xCZmZmk9d52223UVlZyYgRI7jqqqt48cUX6dy5M6+//jpZWVnk5uZSUFDA9ddfz4YNG2oOtj/88MM88MADLfAqTTXP9xto8nt+6623UlpaSnZ2No8++iijR48GICcnh7y8PIYPH86NN97IOeecU7PMvHnzuOiii2oOrFcbOXIks2fPZvTo0YwZM4a5c+eSl5fn8+t56KGHmDlzJuedd16d4y0PPPAAhw8fJisri5ycHD7++OM6Q97n5ORw1VVXAfDDH/6QQ4cOkZuby/PPP8/gwYO9Pld9ry8yMpLXX3+dO++8k5ycHKZMmVLTmznrrLOIj49vsVv92lDwHZwNDe47GwretEVFRUVMmDCBgoICOnXy3m9ozrbdbnsixhjT0b300kuMGTOGhx9+uN4AaS47sG7anA0bNtSceVWtc+fOfPnll0GqyCEi04CngTBgoao+4jE/AXgZ6Ivzt/e4qr7gmtcVWAhkAQrcqKqft2L5ph26/vrruf7661v0OSxETJszYsSIOhdQhQIRCQOeA6YAhcBKEXlbVd3PL70d+FZVZ4hIMrBZRF5R1TKc8HlfVa8UkUggurVfgzH+sN1ZpsFTHI3Dh9/RaGCbqm53hcJrwGWeqwHixDkXMxY4BFSISDwwHvi967nKVLXFBnWy99u4a+72YCHSwUVFRXHw4EH7x9IAVeXgwYNERUU11Kw3sNvtcaFrmrtngaFAEbABuFtVq4ABQDHwgoh8LSILRSSGFmDvt3Hn47bdINud1cGlpaVRWFhIcXFxsEsJaVFRUaSlpTXUxNv4HJ7/qS8E1gKTgIHAhyLyKc7f4UjgTlX9UkSeBn4J/KrOE4jMA+YB9O3b15+XYe+3OYMP23aDLEQ6uIiIiJohGEyzFAJ93B6n4fQ43M0BHlGnG7BNRHYAmcB3QKGqVp8ZsBgnROpQ1QXAAnBO8fWnSHu/TaDZ7ixjAmMlkCEi/V0HxmcBb3u0+Q6YDCAiqcAQYLuqfg/sFpEhrnaTgYYHfDImRFhPxJgAUNUKEbkD+ADnFN9FqrpRRG5xzZ8P/Bp4UUQ24Oz+uldVD7hWcSfwiiuAtuP0WowJeRYixgSIqi4DlnlMm+/2cxEwtZ5l1wKtfuW7Mc1lu7OMMcb4zULEGGOM3yxEjDHG+M1CxBhjjN8sRIwxxvjNQsQYY4zfLESMMcb4zULEGGOM3yxEjDHG+M1CxBhjjN8sRIwxxvitTYaIiAwQkd+LyOJg12KMMR2ZzyEiImGuu64t9ffJRGSRiOwXkW+8zJsmIptFZJuInHEvBXeuW5De5G8dxhhjAqMpPZG7gU3eZohIiojEeUwb5KXpi8A0L8uHAc8BFwHDgKtFZJiIjBCRpR5fKU2o2RhjTAvyKUREJA24GFhYT5PzgT+LSJSr/c3AM56NVHUFcMjL8qOBba4eRhnwGnCZqm5Q1Us8vvb7UrMxxpiW52tP5CngF0CVt5mq+r/A+8BrInINcCPwoybU0RvY7fa40DXNKxHpJiLzgTwRua+eNjNEZMHRo0ebUIYxxpimaDREROQSYL+qrm6onao+CpwCngcuVdXSJtQh3lbZwHMdVNVbVHWgqv62njbvqOq8hISEJpRhjDGmKXzpiZwDXCoiO3F2M00SkZc9G4nIeUAWsAR4sIl1FAJ93B6nAUVNXIcxxphW1miIqOp9qpqmqunALOD/VPVa9zYikgf8DrgM597QSSLymybUsRLIEJH+rntMzwLebsLyxhhjgiBQ14lEAzNV9R+qWgXcAOzybCQirwKfA0NEpFBEbgJQ1QrgDuADnDPA3lDVjQGqzRhjTAsJb0pjVf0E+MTL9L95PC7H6Zl4tru6gXUvA5Y1pR4+BFsAABNRSURBVB5jjDHB1SavWDfGGBMaLESMCZDGRl0QkQQReUdE1onIRhGZ4zG/2aNCGNPaLESMCYD6Rl3waHY78K2q5gATgCdcJ5JUq3dUCGNClYWIMYHhddQFjzYKxImIALE4ozdUgE+jQhgTkixEjAkMX0ZdeBYYinMN1AbgbtfZjNDIqBDGhCoLEWMCw5dRFy4E1gK9gFzgWRGJ93VUCBGZJyKrRGRVcXFxQIo2prksRIwJDF9GXZgDvKWObcAOIBMfR4VQ1QWqmq+q+cnJyS3xGoxpMgsRYwLDl1EXvgMmA4hIKjAE2O7LqBDGhKomXWxojPFOVStEpHrUhTBgkapuFJFbXPPnA78GXhSRDTi7v+5V1QNBK9qYALAQMSZAvI264AqP6p+LgKmNrOMTvIwKYUyost1Zxhhj/GYhYowxHc3Jw7DudSjZ2+xV2e4sY4zpCEqKoOBd2PQO7PobVFXAxU/CqJuatVoLEWOMaa+KN0PBUti0FIrWONO6D4Zxd0LmDOiV1+ynsBAxxpj2oqrKCYtN7zi9joNbnem9z4LJD0LmJZA8OKBPaSFijDFtWUUZ7PzUCY3Ny+DYXugUDunnwph/giHTIcFzBJ7AsRAxxpi25nQpbFvu7Kra8hc4fRQiomHQBU5vY/BU6JLYKqVYiBhjTFtw/ABsfs8Jjn98DJWnoUsSDJ0BQy+BARMgokurl2UhYowxoerwTmc3VcG78N3noFWQ0Nc5oyrzYugzFsKC+2/cQsQYY0KFKuz7xnUq7lLYt8GZnjIcxt/jBEePbBBvg0YHh4WIMcYEU1Ul7P7SCY2CpXBkFyDQdyxM/Y0THEkDgl1lvSxEjDGmtZWfgh1/dU7F3fwenDgAYZHOcY3z/hmGXASxKcGu0icWIsYY0xpOHXXOpCpY6pxZVVYKneMhY6rT28iYAp3jgl1lk1mIGGNMSynZ61y7UbAUdnwKVeUQmwojZjpnVKWfB+Gdg11ls1iIGGNMIB3YBgWuK8YLVzrTkgbA2Fud03F750On9jP2rYWIMcY0hyoUfe30NgreheICZ3rPXJj0gGuokcyQOqMqkNpkiIjIAOBfgARVvTLY9RhjOpjKcmck3OprOEr2gIRB+jmQf6NzjCMhLdhVtopGQ0REooAVQGdX+8Wq+qA/TyYii4BLgP2qmuUxbxrwNM6tRReq6iP1rUdVtwM3ichif+owxpgmKzsO//g/51TcLe/DqSMQ3gUGTYZJv4LBF0J0UrCrbHW+9EROA5NUtVREIoDPROQ9Vf2iuoGIpAAnVfWY27RBqrrNY10vAs8CL7lPFJEw4DlgClAIrBSRt3EC5bce67hRVff79OqMMaY5ThxyDTXyrhMgFSedMamGTHd6GwMnQWR0sKsMqkZDRFUVKHU9jHB9qUez84FbRWS6qp4SkZuBy4HpHutaISLpXp5mNLDN1cNARF4DLlPV3+L0XIwxpnUc2e3aTbUUdv0dtBLi02Dk9c4ZVX3HBX2okVDi02/C1VNYDQwCnlPVL93nq+r/ikh/4DUR+V/gRpxeha96A7vdHhcCYxqopxvwMJAnIve5wsazzQxgxqBBg85Y/kRZBZc/93eG9Igjs2ccQ3vEk9kzjh7xUUg7PfhljKmHKuzf5AqOd2DvOmd68lA496dOcPTMbbcHxpvLpxBR1UogV0S6AktEJEtVv/Fo86irB/E8MFBVS72tqx7e3h3P3o77cx0Ebmmk5neAd/Lz82/2nFdysoI+SV1Yveswb68rqpneNTqCzB5xZPaIZ2hP5/vg1Di6RIb5/kqMMaGvqso5/bb6VNxD253paaNhyv9zzqjqNjC4NbYRTeqTqeoREfkEmAbUCREROQ/IApYADwJ3NGHVhUAft8dpQFE9bZutR0IUC28YBcDRk+Vs/v4YBd+XsGmv8/2NVbs5UVYJQCeB9O4xTm+lRxyZPZ3vaYldrNdi6mjs5BARSQBeBvri/O09rqoviEgfnOOEPYAqYIGqPt2qxXcEFadhxwrXqbjL4Ph+6BQBA853bhc7ZDrE9Qh2lW2OL2dnJQPlrgDpAlwA/IdHmzzgd8DFwA7gZRH5jao+4GMdK4EM1y6xPcAs4Me+vwz/JXSJYHT/JEb3rz2roqpK2X34BJv21gbLN0VHeXfD3po2cZ3DyXT1Vqq/D+kRR2xn21faEdV3coiqfuvW7HbgW1Wd4fq72iwirwAVwD+r6hoRiQNWi8iHHssaf5wqgW0fOr2NLX+BsmMQGesMMZJ5ifM9KiHYVbZpvvzH6wn8wfVH0gl4Q1WXerSJBmaq6j8AROQGYLbnikTkVWAC0F1ECoEHVfX3qlohIncAH+B8ilukqhv9fE3N1qmT0K9bDP26xTAtq2fN9NLTFTW9lgJXuPzp6z0c+6Kipk2/btFn7BLrmxRNp07Wa2nnvJ4cArgHgQJx4nRhY4FDQIWq7gX2AqjqMRHZhHOc0ELEH6X7naFGNi11BjmsLIPo7pB1OWTOgP7jISIq2FW2G76cnbUeyGukzd88Hpfj9Ew8213dwDqWAcsaqyeYYjuHc1a/RM7qV3vbSVWl8PBJCr4/RsHeEgq+P8am70v4y7f7UNdRnejIMOcgvluwDOkRR0KXiCC9EtMCfDk55FngbZxdtXHAVapa5d7AdfZiHvAlxneHtruGUn/XGVYdhcR0GD3P6XH0GQ2d7NhmS7B9L80kIvRJiqZPUjRThqXWTD9ZVsmWfXWPtSzbsJdXv/qupk3vrl1qQqV6l1j/7jGEWa+lLfLl5JALgbXAJGAg8KGIfKqqJQAiEgu8CfykelqdJxCZB8wD6Nu3bwBLb4NUnbOoqk/F3e/qtPXIhgn3OWdUpQyzM6pagYVIC+kSGUZOn67k9OlaM01V+b7kFAV7nd5K9S6xjzcXU1nl/L/pHN7J1WupDZehPeJJjIkM1ksxvvHl5JA5wCOua6+2icgOIBP4ynUh75vAK6r6lrcnUNUFwAKA/Pz8es9ebLcqK5xbxFYPNXL0O5BOznUb0x5xDown9gt2lR2OhUgrEhF6JnShZ0IXJmbW3nDmdEUl2/aXOj0W1y6xjzbt541VhTVtesRH1fRWqnsvA5JjiAhrP6OBtnG+nBzyHTAZ+FREUoEhwHbXMZLfA5tU9clWrDn0lZ+Ef3zs9DY2vwcnD0F4lHOl+IR7YfBFENMt2FV2aBYiIaBzeBjDeyUwvFfds0SKj51m096SmgP5m74/xt+2bae80vkQGhnWiUEpsXUumMzsEU9yXNu+P0FbVN/JISJyi2v+fODXwIsisgFn99e9qnpARM4FrgM2iMha1yrvdx0n7HhOHoYtH7hu3vQRlJ9wzqAaPM011Mhk6Bwb7CqNi4VICEuO60xyXDLjByfXTCuvrOIfxaV1don9bdsB3lqzp6ZN99jOrt5K7S6xQSmxdA63A4stydvJIa7wqP65CJjqZbnP8H5MpeM4usd1RtU7sPMzZ6iRuJ6Q+2PnwHj6uRBmJ6KEIguRNiYirJMTDD3i+QG9a6YfOl5Gwd4SNrmdJfaHz3dRVuGc/BPeSRiYHFvn2pahPeJJje9sF02a4Cje7IRGwbtQtMaZ1n0wnHO3Exy98trVzZvaKwuRdiIpJpJxg7ozblD3mmkVlVXsPHi85uywgr3HWLXzMH9eW3u8NzE6ok6oZPaMY3BqHFER1msxAVZV5YRFdXAc3OpM750Pkx903bxpcHBrNE1mIdKOhYd1YlBKHINS4piR06tm+tET5U6ouA338tpXuzlZXjvUS//uMWT2jGeo2y6x3l1tqBfTRBVlsPNTJzQ2L4Nje6FTuHNv8TH/5BzjiO/V+HpMyLIQ6YASoiMYM6AbYwbUntVSVaXsOnSizi6xDYVHeXe921AvUeF1DuBn9oxjSGocMTbUi3F3uhS2LXcOjG/5C5w+ChHRMOgC5x7jGVOce3KYdsH++g3gDPXSv3sM/bvHcNEIz6FeSursEntrzR5KT+8CnGu5+iVF17lgcmjPOPok2lAvHcrxA05Po+Bd55TcytMQ3Q2GzXB2Uw2YABFdgl2laQEWIqZBzlAvSZzVr3aAyuqhXjbtrd0lVrD3GB98+33NUC8x1UO9VO8S6+kM9RIfZWfYtBuHdzqhsWkp7P4CtAoS+sKom1xDjYyxmzd1APYOmyZzH+pl6vDaobNPlFWwZV9p7Rhie0tYuq6IP35ZO0BlWmKXOhdMZvaMI72bDfXSJqjCvm9qg2PfBmd6ahaMv8cJjh4jbKiRDsZCxARMdGQ4uX26kutlqJfaYfWd4y0fb95fM9RLVEQnhqTGnbFLrGu0DfUSdFWVzoCGm5Y6xziO7AIE+o6FqQ9D5nRIGhDsKk0QWYiYFuU+1MukzNoBKk+VVw/1UrtL7MNN+3h9Ve1AuD0TourcCGxoz3gGdI8h3IZ6aVnlp2D7J7VDjZw4AGGRMGAinPfPzhhVscmNrsZ0DBYiJiiiIsLI6p1AVu/aoV5UleLS03XGENu0t4TPth2oM9RLRmrsGbvEusfaUC/NcvIIbP3QuV3s1uVQfhw6x0PGVGdE3EEXQOe4YFdpQpCFiAkZIkJKXBQpcVGc7zbUS1mFa6gXtzHEPt1azJtrageorB7qZair15LZI55BKbFEhluvpV4le2Gza0TcHZ9CVTnEpkL2j5zgSB8P4bZL0TTMQsSEvMjwTgztGc/QnvF1bo92sPR0TW+lepfYi3/fWWeol0EpsWfsEkuJ68BDvRzY5vQ2Ni2FPaucaUkD4ezbnAPjvfNtqBHTJBYips3qFtuZcwZ15hyPoV52HDheZwyxr3Yc4k9uQ70kxUSecb+WjNTY9jnUi6oz1Ej1GVUHNjvTe+XBpAec28UmD7EzqozfLERMuxIe1omM1DgyUuO41G2olyMnyjxuYXyMP361i1PlTq+lk8CA5Nia3kp176VXQlTb67VUlsOuvzmhsXkZlOwBCYP0c2DUXOeMqoS0YFdp2gkLEdMhdI2OZOyAbox1G+qlskr57tAJZ3eYa7iXdYVHWOo21Et8VDhTh/fg8Zk5wSi76VYuhI9+DaeOQHgXGDQZJv0KBl8I0UmNL29ME1mImA4rzG2ol+luQ70cO1XOZldvpWBvSdu6yVd8b+cU3MyLnbv/RUYHuyLTzlmIGOMhLiqC/PQk8tPb4Cf3IRc5X8a0EjsNwxhjjN8sRIwxxvjNQsQYY4zfLESMMcb4zULEGGOM3yxEjDHG+M1CxBhjjN8sRIwxxvhNtPqm2O2UiBQDu+qZ3R040Irl1CdU6oDQqSVU6oCGa+mnqq1+h6Y2sl1D6NQSKnVA26nFp2273YdIQ0RklarmWx21QqWWUKkDQqsWX4RSvaFSS6jUAe2vFtudZYwxxm8WIsYYY/zW0UNkQbALcAmVOiB0agmVOiC0avFFKNUbKrWESh3Qzmrp0MdEjDHGNE9H74kYY4xphnYZIiIyTUQ2i8g2Efmll/kiIs+45q8XkZG+LtsCtVzjqmG9iPxdRHLc5u0UkQ0islZEVrVwHRNE5KjrudaKyL/6umwL1HKPWx3fiEiliCS55gXyd7JIRPaLyDf1zG+17aQJNYfEth0q27WPtbTKth0q27Vrfa23batqu/oCwoB/AAOASGAdMMyjzXTgPUCAscCXvi7bArWMAxJdP19UXYvr8U6geyv9TiYAS/1ZNtC1eLSfAfxfoH8nrnWNB0YC39Qzv1W2k7a2bYfKdh1K23YobdetvW23x57IaGCbqm5X1TLgNeAyjzaXAS+p4wugq4j09HHZgNaiqn9X1cOuh18Aac14Pr/raKFlA7G+q4FXm/F89VLVFcChBpq01nbiq1DZtkNlu/aplhZatrnrarHtGlp3226PIdIb2O32uNA1zZc2viwb6Frc3YTz6aCaAn8RkdUiMq8V6jhbRNaJyHsiMryJywa6FkQkGpgGvOk2OVC/E1+01nbS3Hp8aRPImkNlu25KLS29bbel7RoCuJ20x3usi5dpnqeg1dfGl2UDXYvTUGQizh/buW6Tz1HVIhFJAT4UkQLXJ4yWqGMNzjAHpSIyHfgTkOHjsoGupdoM4G+q6v6JKlC/E1+01nbiq1DZtkNlu/a1ltbYttvSdg0B3E7aY0+kEOjj9jgNKPKxjS/LBroWRCQbWAhcpqoHq6erapHr+35gCU5Xs0XqUNUSVS11/bwMiBCR7r6+hkDW4mYWHl3+AP5OfNFa20lz6/GlTSBrDpXt2qdaWmnbbkvbNQRyOwnUgZxQ+cLpXW0H+lN7YGi4R5uLqXtQ6Stfl22BWvoC24BxHtNjgDi3n/8OTGvBOnpQe93QaOA71++n1X8nrnYJOPt0Y1rid+K2znTqP/jYKttJW9u2Q2W7DqVtO9S269bctlt0ow/WF86ZB1twzjL4F9e0W4BbXD8L8Jxr/gYgv6FlW7iWhcBhYK3ra5Vr+gDXG7gO2NjcWnyo4w7X86zDORA6rqFlW7IW1+PZwGseywX6d/IqsBcox/kEdlOwtpO2tm2HynYdStt2qGzXrb1t2xXrxhhj/NYej4kYY4xpJRYixhhj/GYhYowxxm8WIsYYY/xmIWKMMcZvFiLGGGP8ZiFijDHGbxYixhhj/Pb/AUXq04s9JqfLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_accuracy improved from 0.80671 to 0.83518, saving model to ../all_faces_bucket/trained_models/weights/mobilenet_new/highest_val_acc.hdf5\n",
      "1780/1780 [==============================] - 660s 371ms/step - loss: 0.2441 - accuracy: 0.9360 - val_loss: 1.1866 - val_accuracy: 0.8352\n",
      "Epoch 3/50\n",
      "\n",
      "Batch 03561: setting learning rate to 0.0001998585717037019.\n",
      "   1/1780 [..............................] - ETA: 2:02:19 - loss: 0.1670 - accuracy: 0.9219\n",
      "Batch 03562: setting learning rate to 0.00019985819237493653.\n",
      "   2/1780 [..............................] - ETA: 1:08:40 - loss: 0.2187 - accuracy: 0.9141\n",
      "Batch 03563: setting learning rate to 0.00019985781253850906.\n",
      "   3/1780 [..............................] - ETA: 47:06 - loss: 0.3522 - accuracy: 0.9167  \n",
      "Batch 03564: setting learning rate to 0.0001998574321944214.\n",
      "   4/1780 [..............................] - ETA: 36:10 - loss: 0.3278 - accuracy: 0.9219\n",
      "Batch 03565: setting learning rate to 0.0001998570513426755.\n",
      "   5/1780 [..............................] - ETA: 29:33 - loss: 0.3365 - accuracy: 0.9156\n",
      "Batch 03566: setting learning rate to 0.00019985666998327333.\n",
      "   6/1780 [..............................] - ETA: 25:07 - loss: 0.3098 - accuracy: 0.9167\n",
      "Batch 03567: setting learning rate to 0.00019985628811621678.\n",
      "   7/1780 [..............................] - ETA: 21:57 - loss: 0.2937 - accuracy: 0.9174\n",
      "Batch 03568: setting learning rate to 0.00019985590574150779.\n",
      "   8/1780 [..............................] - ETA: 19:35 - loss: 0.2791 - accuracy: 0.9199\n",
      "Batch 03569: setting learning rate to 0.00019985552285914831.\n",
      "   9/1780 [..............................] - ETA: 17:44 - loss: 0.2780 - accuracy: 0.9253\n",
      "Batch 03570: setting learning rate to 0.00019985513946914031.\n",
      "  10/1780 [..............................] - ETA: 16:15 - loss: 0.2725 - accuracy: 0.9297\n",
      "Batch 03571: setting learning rate to 0.00019985475557148576.\n",
      "  11/1780 [..............................] - ETA: 15:02 - loss: 0.2562 - accuracy: 0.9361\n",
      "Batch 03572: setting learning rate to 0.00019985437116618653.\n",
      "  12/1780 [..............................] - ETA: 14:02 - loss: 0.2424 - accuracy: 0.9401\n",
      "Batch 03573: setting learning rate to 0.00019985398625324465.\n",
      "  13/1780 [..............................] - ETA: 13:11 - loss: 0.2356 - accuracy: 0.9435\n",
      "Batch 03574: setting learning rate to 0.00019985360083266205.\n",
      "  14/1780 [..............................] - ETA: 12:27 - loss: 0.2357 - accuracy: 0.9453\n",
      "Batch 03575: setting learning rate to 0.00019985321490444068.\n",
      "  15/1780 [..............................] - ETA: 11:50 - loss: 0.2275 - accuracy: 0.9479\n",
      "Batch 03576: setting learning rate to 0.00019985282846858248.\n",
      "  16/1780 [..............................] - ETA: 11:15 - loss: 0.2317 - accuracy: 0.9473\n",
      "Batch 03577: setting learning rate to 0.00019985244152508948.\n",
      "  17/1780 [..............................] - ETA: 14:24 - loss: 0.2307 - accuracy: 0.9476\n",
      "Batch 03578: setting learning rate to 0.00019985205407396358.\n",
      "  18/1780 [..............................] - ETA: 13:48 - loss: 0.2249 - accuracy: 0.9479\n",
      "Batch 03579: setting learning rate to 0.00019985166611520682.\n",
      "  19/1780 [..............................] - ETA: 13:12 - loss: 0.2201 - accuracy: 0.9482\n",
      "Batch 03580: setting learning rate to 0.0001998512776488211.\n",
      "  20/1780 [..............................] - ETA: 13:01 - loss: 0.2248 - accuracy: 0.9453\n",
      "Batch 03581: setting learning rate to 0.00019985088867480844.\n",
      "  21/1780 [..............................] - ETA: 12:35 - loss: 0.2159 - accuracy: 0.9472\n",
      "Batch 03582: setting learning rate to 0.0001998504991931708.\n",
      "  22/1780 [..............................] - ETA: 12:09 - loss: 0.2247 - accuracy: 0.9446\n",
      "Batch 03583: setting learning rate to 0.0001998501092039102.\n",
      "  23/1780 [..............................] - ETA: 11:45 - loss: 0.2217 - accuracy: 0.9443\n",
      "Batch 03584: setting learning rate to 0.00019984971870702856.\n",
      "  24/1780 [..............................] - ETA: 11:22 - loss: 0.2176 - accuracy: 0.9440\n",
      "Batch 03585: setting learning rate to 0.0001998493277025279.\n",
      "  25/1780 [..............................] - ETA: 11:08 - loss: 0.2277 - accuracy: 0.9450\n",
      "Batch 03586: setting learning rate to 0.0001998489361904102.\n",
      "  26/1780 [..............................] - ETA: 10:49 - loss: 0.2277 - accuracy: 0.9447\n",
      "Batch 03587: setting learning rate to 0.00019984854417067743.\n",
      "  27/1780 [..............................] - ETA: 10:31 - loss: 0.2237 - accuracy: 0.9468\n",
      "Batch 03588: setting learning rate to 0.00019984815164333163.\n",
      "  28/1780 [..............................] - ETA: 10:16 - loss: 0.2221 - accuracy: 0.9475\n",
      "Batch 03589: setting learning rate to 0.00019984775860837478.\n",
      "  29/1780 [..............................] - ETA: 10:01 - loss: 0.2171 - accuracy: 0.9494\n",
      "Batch 03590: setting learning rate to 0.00019984736506580887.\n",
      "  30/1780 [..............................] - ETA: 9:46 - loss: 0.2138 - accuracy: 0.9500 \n",
      "Batch 03591: setting learning rate to 0.00019984697101563584.\n",
      "  31/1780 [..............................] - ETA: 9:33 - loss: 0.2099 - accuracy: 0.9496\n",
      "Batch 03592: setting learning rate to 0.0001998465764578578.\n",
      "  32/1780 [..............................] - ETA: 9:20 - loss: 0.2095 - accuracy: 0.9492\n",
      "Batch 03593: setting learning rate to 0.0001998461813924767.\n",
      "  33/1780 [..............................] - ETA: 11:05 - loss: 0.2097 - accuracy: 0.9498\n",
      "Batch 03594: setting learning rate to 0.00019984578581949455.\n",
      "  34/1780 [..............................] - ETA: 10:51 - loss: 0.2053 - accuracy: 0.9508\n",
      "Batch 03595: setting learning rate to 0.00019984538973891338.\n",
      "  35/1780 [..............................] - ETA: 11:12 - loss: 0.2024 - accuracy: 0.9513\n",
      "Batch 03596: setting learning rate to 0.00019984499315073523.\n",
      "  36/1780 [..............................] - ETA: 10:58 - loss: 0.2024 - accuracy: 0.9523\n",
      "Batch 03597: setting learning rate to 0.00019984459605496203.\n",
      "  37/1780 [..............................] - ETA: 10:52 - loss: 0.2058 - accuracy: 0.9519\n",
      "Batch 03598: setting learning rate to 0.00019984419845159583.\n",
      "  38/1780 [..............................] - ETA: 10:39 - loss: 0.2038 - accuracy: 0.9519\n",
      "Batch 03599: setting learning rate to 0.0001998438003406387.\n",
      "  39/1780 [..............................] - ETA: 10:27 - loss: 0.2015 - accuracy: 0.9519\n",
      "Batch 03600: setting learning rate to 0.0001998434017220926.\n",
      "  40/1780 [..............................] - ETA: 10:16 - loss: 0.2055 - accuracy: 0.9512\n",
      "Batch 03601: setting learning rate to 0.00019984300259595964.\n",
      "  41/1780 [..............................] - ETA: 10:05 - loss: 0.2033 - accuracy: 0.9508\n",
      "Batch 03602: setting learning rate to 0.00019984260296224176.\n",
      "  42/1780 [..............................] - ETA: 9:54 - loss: 0.1997 - accuracy: 0.9516 \n",
      "Batch 03603: setting learning rate to 0.00019984220282094102.\n",
      "  43/1780 [..............................] - ETA: 9:45 - loss: 0.1976 - accuracy: 0.9520\n",
      "Batch 03604: setting learning rate to 0.0001998418021720595.\n",
      "  44/1780 [..............................] - ETA: 9:35 - loss: 0.2015 - accuracy: 0.9506\n",
      "Batch 03605: setting learning rate to 0.00019984140101559917.\n",
      "  45/1780 [..............................] - ETA: 9:26 - loss: 0.1991 - accuracy: 0.9514\n",
      "Batch 03606: setting learning rate to 0.0001998409993515621.\n",
      "  46/1780 [..............................] - ETA: 9:18 - loss: 0.2033 - accuracy: 0.9514\n",
      "Batch 03607: setting learning rate to 0.00019984059717995036.\n",
      "  47/1780 [..............................] - ETA: 9:09 - loss: 0.2057 - accuracy: 0.9508\n",
      "Batch 03608: setting learning rate to 0.00019984019450076593.\n",
      "  48/1780 [..............................] - ETA: 9:01 - loss: 0.2033 - accuracy: 0.9518\n",
      "Batch 03609: setting learning rate to 0.00019983979131401092.\n",
      "  49/1780 [..............................] - ETA: 9:28 - loss: 0.2018 - accuracy: 0.9518\n",
      "Batch 03610: setting learning rate to 0.00019983938761968735.\n",
      "  50/1780 [..............................] - ETA: 9:49 - loss: 0.2023 - accuracy: 0.9516\n",
      "Batch 03611: setting learning rate to 0.00019983898341779726.\n",
      "  51/1780 [..............................] - ETA: 9:42 - loss: 0.2045 - accuracy: 0.9519\n",
      "Batch 03612: setting learning rate to 0.00019983857870834273.\n",
      "  52/1780 [..............................] - ETA: 9:34 - loss: 0.2099 - accuracy: 0.9519\n",
      "Batch 03613: setting learning rate to 0.0001998381734913258.\n",
      "  53/1780 [..............................] - ETA: 9:34 - loss: 0.2081 - accuracy: 0.9522\n",
      "Batch 03614: setting learning rate to 0.00019983776776674855.\n",
      "  54/1780 [..............................] - ETA: 9:26 - loss: 0.2067 - accuracy: 0.9523\n",
      "Batch 03615: setting learning rate to 0.000199837361534613.\n",
      "  55/1780 [..............................] - ETA: 9:23 - loss: 0.2078 - accuracy: 0.9511\n",
      "Batch 03616: setting learning rate to 0.00019983695479492127.\n",
      "  56/1780 [..............................] - ETA: 9:17 - loss: 0.2064 - accuracy: 0.9509\n",
      "Batch 03617: setting learning rate to 0.00019983654754767542.\n",
      "  57/1780 [..............................] - ETA: 9:10 - loss: 0.2065 - accuracy: 0.9507\n",
      "Batch 03618: setting learning rate to 0.00019983613979287747.\n",
      "  58/1780 [..............................] - ETA: 9:03 - loss: 0.2050 - accuracy: 0.9510\n",
      "Batch 03619: setting learning rate to 0.00019983573153052955.\n",
      "  59/1780 [..............................] - ETA: 8:56 - loss: 0.2045 - accuracy: 0.9507\n",
      "Batch 03620: setting learning rate to 0.0001998353227606337.\n",
      "  60/1780 [>.............................] - ETA: 8:55 - loss: 0.2063 - accuracy: 0.9500\n",
      "Batch 03621: setting learning rate to 0.000199834913483192.\n",
      "  61/1780 [>.............................] - ETA: 8:48 - loss: 0.2061 - accuracy: 0.9501\n",
      "Batch 03622: setting learning rate to 0.00019983450369820654.\n",
      "  62/1780 [>.............................] - ETA: 8:43 - loss: 0.2051 - accuracy: 0.9504\n",
      "Batch 03623: setting learning rate to 0.00019983409340567946.\n",
      "  63/1780 [>.............................] - ETA: 8:37 - loss: 0.2075 - accuracy: 0.9501\n",
      "Batch 03624: setting learning rate to 0.00019983368260561272.\n",
      "  64/1780 [>.............................] - ETA: 8:32 - loss: 0.2063 - accuracy: 0.9507\n",
      "Batch 03625: setting learning rate to 0.00019983327129800854.\n",
      "  65/1780 [>.............................] - ETA: 9:14 - loss: 0.2065 - accuracy: 0.9510\n",
      "Batch 03626: setting learning rate to 0.0001998328594828689.\n",
      "  66/1780 [>.............................] - ETA: 9:22 - loss: 0.2120 - accuracy: 0.9508\n",
      "Batch 03627: setting learning rate to 0.00019983244716019596.\n",
      "  67/1780 [>.............................] - ETA: 9:16 - loss: 0.2125 - accuracy: 0.9506\n",
      "Batch 03628: setting learning rate to 0.0001998320343299918.\n",
      "  68/1780 [>.............................] - ETA: 9:10 - loss: 0.2132 - accuracy: 0.9501\n",
      "Batch 03629: setting learning rate to 0.00019983162099225852.\n",
      "  69/1780 [>.............................] - ETA: 9:07 - loss: 0.2132 - accuracy: 0.9500\n",
      "Batch 03630: setting learning rate to 0.0001998312071469982.\n",
      "  70/1780 [>.............................] - ETA: 9:11 - loss: 0.2123 - accuracy: 0.9500\n",
      "Batch 03631: setting learning rate to 0.000199830792794213.\n",
      "  71/1780 [>.............................] - ETA: 9:05 - loss: 0.2111 - accuracy: 0.9500\n",
      "Batch 03632: setting learning rate to 0.00019983037793390495.\n",
      "  72/1780 [>.............................] - ETA: 9:00 - loss: 0.2110 - accuracy: 0.9497\n",
      "Batch 03633: setting learning rate to 0.00019982996256607622.\n",
      "  73/1780 [>.............................] - ETA: 8:54 - loss: 0.2117 - accuracy: 0.9486\n",
      "Batch 03634: setting learning rate to 0.0001998295466907289.\n",
      "  74/1780 [>.............................] - ETA: 8:49 - loss: 0.2181 - accuracy: 0.9464\n",
      "Batch 03635: setting learning rate to 0.0001998291303078651.\n",
      "  75/1780 [>.............................] - ETA: 8:44 - loss: 0.2171 - accuracy: 0.9467\n",
      "Batch 03636: setting learning rate to 0.00019982871341748694.\n",
      "  76/1780 [>.............................] - ETA: 8:39 - loss: 0.2175 - accuracy: 0.9465\n",
      "Batch 03637: setting learning rate to 0.00019982829601959657.\n",
      "  77/1780 [>.............................] - ETA: 8:43 - loss: 0.2208 - accuracy: 0.9462\n",
      "Batch 03638: setting learning rate to 0.00019982787811419604.\n",
      "  78/1780 [>.............................] - ETA: 8:39 - loss: 0.2231 - accuracy: 0.9461\n",
      "Batch 03639: setting learning rate to 0.00019982745970128756.\n",
      "  79/1780 [>.............................] - ETA: 8:34 - loss: 0.2221 - accuracy: 0.9462\n",
      "Batch 03640: setting learning rate to 0.00019982704078087317.\n",
      "  80/1780 [>.............................] - ETA: 8:29 - loss: 0.2229 - accuracy: 0.9459\n",
      "Batch 03641: setting learning rate to 0.00019982662135295507.\n",
      "  81/1780 [>.............................] - ETA: 8:53 - loss: 0.2223 - accuracy: 0.9464\n",
      "Batch 03642: setting learning rate to 0.00019982620141753538.\n",
      "  82/1780 [>.............................] - ETA: 8:51 - loss: 0.2205 - accuracy: 0.9466\n",
      "Batch 03643: setting learning rate to 0.00019982578097461619.\n",
      "  83/1780 [>.............................] - ETA: 8:58 - loss: 0.2204 - accuracy: 0.9465\n",
      "Batch 03644: setting learning rate to 0.00019982536002419968.\n",
      "  84/1780 [>.............................] - ETA: 8:55 - loss: 0.2201 - accuracy: 0.9466\n",
      "Batch 03645: setting learning rate to 0.00019982493856628798.\n",
      "  85/1780 [>.............................] - ETA: 8:51 - loss: 0.2203 - accuracy: 0.9465\n",
      "Batch 03646: setting learning rate to 0.00019982451660088325.\n",
      "  86/1780 [>.............................] - ETA: 8:47 - loss: 0.2235 - accuracy: 0.9455\n",
      "Batch 03647: setting learning rate to 0.00019982409412798758.\n",
      "  87/1780 [>.............................] - ETA: 8:43 - loss: 0.2251 - accuracy: 0.9452\n",
      "Batch 03648: setting learning rate to 0.0001998236711476032.\n",
      "  88/1780 [>.............................] - ETA: 8:39 - loss: 0.2237 - accuracy: 0.9455\n",
      "Batch 03649: setting learning rate to 0.0001998232476597322.\n",
      "  89/1780 [>.............................] - ETA: 8:34 - loss: 0.2239 - accuracy: 0.9450\n",
      "Batch 03650: setting learning rate to 0.0001998228236643767.\n",
      "  90/1780 [>.............................] - ETA: 8:32 - loss: 0.2234 - accuracy: 0.9453\n",
      "Batch 03651: setting learning rate to 0.00019982239916153897.\n",
      "  91/1780 [>.............................] - ETA: 8:28 - loss: 0.2230 - accuracy: 0.9452\n",
      "Batch 03652: setting learning rate to 0.00019982197415122107.\n",
      "  92/1780 [>.............................] - ETA: 8:25 - loss: 0.2228 - accuracy: 0.9453\n",
      "Batch 03653: setting learning rate to 0.00019982154863342519.\n",
      "  93/1780 [>.............................] - ETA: 8:21 - loss: 0.2222 - accuracy: 0.9454\n",
      "Batch 03654: setting learning rate to 0.00019982112260815348.\n",
      "  94/1780 [>.............................] - ETA: 8:17 - loss: 0.2210 - accuracy: 0.9456\n",
      "Batch 03655: setting learning rate to 0.00019982069607540816.\n",
      "  95/1780 [>.............................] - ETA: 8:17 - loss: 0.2199 - accuracy: 0.9462\n",
      "Batch 03656: setting learning rate to 0.00019982026903519136.\n",
      "  96/1780 [>.............................] - ETA: 8:18 - loss: 0.2190 - accuracy: 0.9463\n",
      "Batch 03657: setting learning rate to 0.0001998198414875052.\n",
      "  97/1780 [>.............................] - ETA: 8:34 - loss: 0.2183 - accuracy: 0.9467\n",
      "Batch 03658: setting learning rate to 0.00019981941343235194.\n",
      "  98/1780 [>.............................] - ETA: 8:42 - loss: 0.2186 - accuracy: 0.9467\n",
      "Batch 03659: setting learning rate to 0.00019981898486973374.\n",
      "  99/1780 [>.............................] - ETA: 8:38 - loss: 0.2188 - accuracy: 0.9470\n",
      "Batch 03660: setting learning rate to 0.0001998185557996527.\n",
      " 100/1780 [>.............................] - ETA: 8:45 - loss: 0.2184 - accuracy: 0.9469\n",
      "Batch 03661: setting learning rate to 0.00019981812622211112.\n",
      " 101/1780 [>.............................] - ETA: 8:42 - loss: 0.2178 - accuracy: 0.9469\n",
      "Batch 03662: setting learning rate to 0.00019981769613711112.\n",
      " 102/1780 [>.............................] - ETA: 8:38 - loss: 0.2175 - accuracy: 0.9468\n",
      "Batch 03663: setting learning rate to 0.00019981726554465484.\n",
      " 103/1780 [>.............................] - ETA: 8:35 - loss: 0.2162 - accuracy: 0.9472\n",
      "Batch 03664: setting learning rate to 0.0001998168344447446.\n",
      " 104/1780 [>.............................] - ETA: 8:31 - loss: 0.2157 - accuracy: 0.9473\n",
      "Batch 03665: setting learning rate to 0.00019981640283738244.\n",
      " 105/1780 [>.............................] - ETA: 8:28 - loss: 0.2172 - accuracy: 0.9472\n",
      "Batch 03666: setting learning rate to 0.00019981597072257065.\n",
      " 106/1780 [>.............................] - ETA: 8:24 - loss: 0.2159 - accuracy: 0.9475\n",
      "Batch 03667: setting learning rate to 0.0001998155381003114.\n",
      " 107/1780 [>.............................] - ETA: 8:21 - loss: 0.2146 - accuracy: 0.9479\n",
      "Batch 03668: setting learning rate to 0.0001998151049706069.\n",
      " 108/1780 [>.............................] - ETA: 8:18 - loss: 0.2146 - accuracy: 0.9476\n",
      "Batch 03669: setting learning rate to 0.00019981467133345934.\n",
      " 109/1780 [>.............................] - ETA: 8:14 - loss: 0.2146 - accuracy: 0.9477\n",
      "Batch 03670: setting learning rate to 0.00019981423718887093.\n",
      " 110/1780 [>.............................] - ETA: 8:13 - loss: 0.2148 - accuracy: 0.9480\n",
      "Batch 03671: setting learning rate to 0.00019981380253684388.\n",
      " 111/1780 [>.............................] - ETA: 8:10 - loss: 0.2140 - accuracy: 0.9483\n",
      "Batch 03672: setting learning rate to 0.00019981336737738038.\n",
      " 112/1780 [>.............................] - ETA: 8:13 - loss: 0.2131 - accuracy: 0.9484\n",
      "Batch 03673: setting learning rate to 0.00019981293171048267.\n",
      " 113/1780 [>.............................] - ETA: 8:18 - loss: 0.2120 - accuracy: 0.9486\n",
      "Batch 03674: setting learning rate to 0.00019981249553615297.\n",
      " 114/1780 [>.............................] - ETA: 8:26 - loss: 0.2109 - accuracy: 0.9490\n",
      "Batch 03675: setting learning rate to 0.00019981205885439345.\n",
      " 115/1780 [>.............................] - ETA: 8:28 - loss: 0.2116 - accuracy: 0.9490\n",
      "Batch 03676: setting learning rate to 0.00019981162166520636.\n",
      " 116/1780 [>.............................] - ETA: 8:25 - loss: 0.2103 - accuracy: 0.9494\n",
      "Batch 03677: setting learning rate to 0.00019981118396859393.\n",
      " 117/1780 [>.............................] - ETA: 8:23 - loss: 0.2135 - accuracy: 0.9495\n",
      "Batch 03678: setting learning rate to 0.00019981074576455837.\n",
      " 118/1780 [>.............................] - ETA: 8:20 - loss: 0.2122 - accuracy: 0.9498\n",
      "Batch 03679: setting learning rate to 0.00019981030705310194.\n",
      " 119/1780 [=>............................] - ETA: 8:17 - loss: 0.2117 - accuracy: 0.9498\n",
      "Batch 03680: setting learning rate to 0.00019980986783422683.\n",
      " 120/1780 [=>............................] - ETA: 8:14 - loss: 0.2124 - accuracy: 0.9497\n",
      "Batch 03681: setting learning rate to 0.0001998094281079353.\n",
      " 121/1780 [=>............................] - ETA: 8:12 - loss: 0.2120 - accuracy: 0.9499\n",
      "Batch 03682: setting learning rate to 0.00019980898787422957.\n",
      " 122/1780 [=>............................] - ETA: 8:09 - loss: 0.2109 - accuracy: 0.9499\n",
      "Batch 03683: setting learning rate to 0.00019980854713311187.\n",
      " 123/1780 [=>............................] - ETA: 8:06 - loss: 0.2098 - accuracy: 0.9502\n",
      "Batch 03684: setting learning rate to 0.00019980810588458446.\n",
      " 124/1780 [=>............................] - ETA: 8:06 - loss: 0.2095 - accuracy: 0.9502\n",
      "Batch 03685: setting learning rate to 0.00019980766412864957.\n",
      " 125/1780 [=>............................] - ETA: 8:03 - loss: 0.2093 - accuracy: 0.9504\n",
      "Batch 03686: setting learning rate to 0.00019980722186530944.\n",
      " 126/1780 [=>............................] - ETA: 8:11 - loss: 0.2087 - accuracy: 0.9504\n",
      "Batch 03687: setting learning rate to 0.00019980677909456636.\n",
      " 127/1780 [=>............................] - ETA: 8:08 - loss: 0.2090 - accuracy: 0.9503\n",
      "Batch 03688: setting learning rate to 0.00019980633581642254.\n",
      " 128/1780 [=>............................] - ETA: 8:10 - loss: 0.2089 - accuracy: 0.9502\n",
      "Batch 03689: setting learning rate to 0.00019980589203088023.\n",
      " 129/1780 [=>............................] - ETA: 8:10 - loss: 0.2081 - accuracy: 0.9506\n",
      "Batch 03690: setting learning rate to 0.00019980544773794172.\n",
      " 130/1780 [=>............................] - ETA: 8:15 - loss: 0.2082 - accuracy: 0.9507\n",
      "Batch 03691: setting learning rate to 0.00019980500293760925.\n",
      " 131/1780 [=>............................] - ETA: 8:15 - loss: 0.2070 - accuracy: 0.9510\n",
      "Batch 03692: setting learning rate to 0.00019980455762988507.\n",
      " 132/1780 [=>............................] - ETA: 8:12 - loss: 0.2087 - accuracy: 0.9511\n",
      "Batch 03693: setting learning rate to 0.00019980411181477149.\n",
      " 133/1780 [=>............................] - ETA: 8:11 - loss: 0.2076 - accuracy: 0.9512\n",
      "Batch 03694: setting learning rate to 0.0001998036654922707.\n",
      " 134/1780 [=>............................] - ETA: 8:11 - loss: 0.2066 - accuracy: 0.9513\n",
      "Batch 03695: setting learning rate to 0.00019980321866238502.\n",
      " 135/1780 [=>............................] - ETA: 8:09 - loss: 0.2065 - accuracy: 0.9512\n",
      "Batch 03696: setting learning rate to 0.00019980277132511672.\n",
      " 136/1780 [=>............................] - ETA: 8:06 - loss: 0.2068 - accuracy: 0.9507\n",
      "Batch 03697: setting learning rate to 0.00019980232348046806.\n",
      " 137/1780 [=>............................] - ETA: 8:04 - loss: 0.2074 - accuracy: 0.9504\n",
      "Batch 03698: setting learning rate to 0.0001998018751284413.\n",
      " 138/1780 [=>............................] - ETA: 8:01 - loss: 0.2064 - accuracy: 0.9504\n",
      "Batch 03699: setting learning rate to 0.00019980142626903878.\n",
      " 139/1780 [=>............................] - ETA: 7:59 - loss: 0.2064 - accuracy: 0.9502\n",
      "Batch 03700: setting learning rate to 0.00019980097690226273.\n",
      " 140/1780 [=>............................] - ETA: 7:57 - loss: 0.2069 - accuracy: 0.9501\n",
      "Batch 03701: setting learning rate to 0.00019980052702811544.\n",
      " 141/1780 [=>............................] - ETA: 7:55 - loss: 0.2091 - accuracy: 0.9500\n",
      "Batch 03702: setting learning rate to 0.00019980007664659923.\n",
      " 142/1780 [=>............................] - ETA: 8:00 - loss: 0.2113 - accuracy: 0.9496\n",
      "Batch 03703: setting learning rate to 0.00019979962575771633.\n",
      " 143/1780 [=>............................] - ETA: 8:02 - loss: 0.2121 - accuracy: 0.9496\n",
      "Batch 03704: setting learning rate to 0.0001997991743614691.\n",
      " 144/1780 [=>............................] - ETA: 8:00 - loss: 0.2122 - accuracy: 0.9493\n",
      "Batch 03705: setting learning rate to 0.0001997987224578598.\n",
      " 145/1780 [=>............................] - ETA: 8:05 - loss: 0.2117 - accuracy: 0.9494\n",
      "Batch 03706: setting learning rate to 0.00019979827004689075.\n",
      " 146/1780 [=>............................] - ETA: 8:04 - loss: 0.2140 - accuracy: 0.9483\n",
      "Batch 03707: setting learning rate to 0.00019979781712856415.\n",
      " 147/1780 [=>............................] - ETA: 8:03 - loss: 0.2157 - accuracy: 0.9474\n",
      "Batch 03708: setting learning rate to 0.00019979736370288244.\n",
      " 148/1780 [=>............................] - ETA: 8:05 - loss: 0.2170 - accuracy: 0.9469\n",
      "Batch 03709: setting learning rate to 0.00019979690976984786.\n",
      " 149/1780 [=>............................] - ETA: 8:03 - loss: 0.2168 - accuracy: 0.9468\n",
      "Batch 03710: setting learning rate to 0.00019979645532946274.\n",
      " 150/1780 [=>............................] - ETA: 8:00 - loss: 0.2180 - accuracy: 0.9465\n",
      "Batch 03711: setting learning rate to 0.00019979600038172937.\n",
      " 151/1780 [=>............................] - ETA: 8:01 - loss: 0.2177 - accuracy: 0.9466\n",
      "Batch 03712: setting learning rate to 0.00019979554492665006.\n",
      " 152/1780 [=>............................] - ETA: 8:02 - loss: 0.2179 - accuracy: 0.9465\n",
      "Batch 03713: setting learning rate to 0.00019979508896422713.\n",
      " 153/1780 [=>............................] - ETA: 7:59 - loss: 0.2172 - accuracy: 0.9468\n",
      "Batch 03714: setting learning rate to 0.0001997946324944629.\n",
      " 154/1780 [=>............................] - ETA: 7:57 - loss: 0.2167 - accuracy: 0.9470\n",
      "Batch 03715: setting learning rate to 0.0001997941755173597.\n",
      " 155/1780 [=>............................] - ETA: 7:55 - loss: 0.2162 - accuracy: 0.9473\n",
      "Batch 03716: setting learning rate to 0.00019979371803291985.\n",
      " 156/1780 [=>............................] - ETA: 7:56 - loss: 0.2171 - accuracy: 0.9473\n",
      "Batch 03717: setting learning rate to 0.00019979326004114566.\n",
      " 157/1780 [=>............................] - ETA: 7:54 - loss: 0.2170 - accuracy: 0.9474\n",
      "Batch 03718: setting learning rate to 0.0001997928015420395.\n",
      " 158/1780 [=>............................] - ETA: 7:51 - loss: 0.2162 - accuracy: 0.9476\n",
      "Batch 03719: setting learning rate to 0.00019979234253560363.\n",
      " 159/1780 [=>............................] - ETA: 7:55 - loss: 0.2164 - accuracy: 0.9476\n",
      "Batch 03720: setting learning rate to 0.00019979188302184043.\n",
      " 160/1780 [=>............................] - ETA: 7:52 - loss: 0.2167 - accuracy: 0.9476\n",
      "Batch 03721: setting learning rate to 0.00019979142300075224.\n",
      " 161/1780 [=>............................] - ETA: 8:00 - loss: 0.2164 - accuracy: 0.9476\n",
      "Batch 03722: setting learning rate to 0.0001997909624723414.\n",
      " 162/1780 [=>............................] - ETA: 7:57 - loss: 0.2162 - accuracy: 0.9476\n",
      "Batch 03723: setting learning rate to 0.00019979050143661023.\n",
      " 163/1780 [=>............................] - ETA: 7:55 - loss: 0.2166 - accuracy: 0.9477\n",
      "Batch 03724: setting learning rate to 0.0001997900398935611.\n",
      " 164/1780 [=>............................] - ETA: 7:57 - loss: 0.2168 - accuracy: 0.9474\n",
      "Batch 03725: setting learning rate to 0.00019978957784319633.\n",
      " 165/1780 [=>............................] - ETA: 8:00 - loss: 0.2165 - accuracy: 0.9473\n",
      "Batch 03726: setting learning rate to 0.00019978911528551824.\n",
      " 166/1780 [=>............................] - ETA: 7:58 - loss: 0.2167 - accuracy: 0.9473\n",
      "Batch 03727: setting learning rate to 0.00019978865222052927.\n",
      " 167/1780 [=>............................] - ETA: 7:56 - loss: 0.2163 - accuracy: 0.9475\n",
      "Batch 03728: setting learning rate to 0.0001997881886482317.\n",
      " 168/1780 [=>............................] - ETA: 7:54 - loss: 0.2161 - accuracy: 0.9476\n",
      "Batch 03729: setting learning rate to 0.00019978772456862793.\n",
      " 169/1780 [=>............................] - ETA: 7:52 - loss: 0.2158 - accuracy: 0.9477\n",
      "Batch 03730: setting learning rate to 0.00019978725998172033.\n",
      " 170/1780 [=>............................] - ETA: 7:50 - loss: 0.2157 - accuracy: 0.9475\n",
      "Batch 03731: setting learning rate to 0.00019978679488751117.\n",
      " 171/1780 [=>............................] - ETA: 7:48 - loss: 0.2164 - accuracy: 0.9473\n",
      "Batch 03732: setting learning rate to 0.0001997863292860029.\n",
      " 172/1780 [=>............................] - ETA: 7:46 - loss: 0.2158 - accuracy: 0.9473\n",
      "Batch 03733: setting learning rate to 0.00019978586317719786.\n",
      " 173/1780 [=>............................] - ETA: 7:46 - loss: 0.2157 - accuracy: 0.9474\n",
      "Batch 03734: setting learning rate to 0.00019978539656109848.\n",
      " 174/1780 [=>............................] - ETA: 7:44 - loss: 0.2152 - accuracy: 0.9476\n",
      "Batch 03735: setting learning rate to 0.00019978492943770702.\n",
      " 175/1780 [=>............................] - ETA: 7:47 - loss: 0.2151 - accuracy: 0.9477\n",
      "Batch 03736: setting learning rate to 0.0001997844618070259.\n",
      " 176/1780 [=>............................] - ETA: 7:47 - loss: 0.2150 - accuracy: 0.9475\n",
      "Batch 03737: setting learning rate to 0.00019978399366905753.\n",
      " 177/1780 [=>............................] - ETA: 7:47 - loss: 0.2147 - accuracy: 0.9476\n",
      "Batch 03738: setting learning rate to 0.00019978352502380428.\n",
      " 178/1780 [==>...........................] - ETA: 7:48 - loss: 0.2139 - accuracy: 0.9478\n",
      "Batch 03739: setting learning rate to 0.0001997830558712685.\n",
      " 179/1780 [==>...........................] - ETA: 7:52 - loss: 0.2143 - accuracy: 0.9478\n",
      "Batch 03740: setting learning rate to 0.00019978258621145264.\n",
      " 180/1780 [==>...........................] - ETA: 7:49 - loss: 0.2147 - accuracy: 0.9477\n",
      "Batch 03741: setting learning rate to 0.00019978211604435898.\n",
      " 181/1780 [==>...........................] - ETA: 7:54 - loss: 0.2140 - accuracy: 0.9479\n",
      "Batch 03742: setting learning rate to 0.00019978164536999.\n",
      " 182/1780 [==>...........................] - ETA: 7:52 - loss: 0.2141 - accuracy: 0.9481\n",
      "Batch 03743: setting learning rate to 0.0001997811741883481.\n",
      " 183/1780 [==>...........................] - ETA: 7:50 - loss: 0.2134 - accuracy: 0.9483\n",
      "Batch 03744: setting learning rate to 0.00019978070249943562.\n",
      " 184/1780 [==>...........................] - ETA: 7:48 - loss: 0.2125 - accuracy: 0.9485\n",
      "Batch 03745: setting learning rate to 0.00019978023030325497.\n",
      " 185/1780 [==>...........................] - ETA: 7:46 - loss: 0.2121 - accuracy: 0.9484\n",
      "Batch 03746: setting learning rate to 0.00019977975759980857.\n",
      " 186/1780 [==>...........................] - ETA: 7:45 - loss: 0.2117 - accuracy: 0.9485\n",
      "Batch 03747: setting learning rate to 0.00019977928438909878.\n",
      " 187/1780 [==>...........................] - ETA: 7:44 - loss: 0.2114 - accuracy: 0.9485\n",
      "Batch 03748: setting learning rate to 0.00019977881067112808.\n",
      " 188/1780 [==>...........................] - ETA: 7:42 - loss: 0.2107 - accuracy: 0.9486\n",
      "Batch 03749: setting learning rate to 0.00019977833644589885.\n",
      " 189/1780 [==>...........................] - ETA: 7:40 - loss: 0.2110 - accuracy: 0.9484\n",
      "Batch 03750: setting learning rate to 0.00019977786171341346.\n",
      " 190/1780 [==>...........................] - ETA: 7:39 - loss: 0.2109 - accuracy: 0.9484\n",
      "Batch 03751: setting learning rate to 0.00019977738647367437.\n",
      " 191/1780 [==>...........................] - ETA: 7:40 - loss: 0.2110 - accuracy: 0.9485\n",
      "Batch 03752: setting learning rate to 0.00019977691072668393.\n",
      " 192/1780 [==>...........................] - ETA: 7:38 - loss: 0.2108 - accuracy: 0.9486\n",
      "Batch 03753: setting learning rate to 0.00019977643447244468.\n",
      " 193/1780 [==>...........................] - ETA: 7:43 - loss: 0.2110 - accuracy: 0.9486\n",
      "Batch 03754: setting learning rate to 0.00019977595771095895.\n",
      " 194/1780 [==>...........................] - ETA: 7:41 - loss: 0.2109 - accuracy: 0.9487\n",
      "Batch 03755: setting learning rate to 0.00019977548044222912.\n",
      " 195/1780 [==>...........................] - ETA: 7:44 - loss: 0.2108 - accuracy: 0.9486\n",
      "Batch 03756: setting learning rate to 0.00019977500266625775.\n",
      " 196/1780 [==>...........................] - ETA: 7:42 - loss: 0.2098 - accuracy: 0.9489\n",
      "Batch 03757: setting learning rate to 0.00019977452438304716.\n",
      " 197/1780 [==>...........................] - ETA: 7:42 - loss: 0.2097 - accuracy: 0.9490\n",
      "Batch 03758: setting learning rate to 0.00019977404559259982.\n",
      " 198/1780 [==>...........................] - ETA: 7:41 - loss: 0.2096 - accuracy: 0.9489\n",
      "Batch 03759: setting learning rate to 0.0001997735662949182.\n",
      " 199/1780 [==>...........................] - ETA: 7:44 - loss: 0.2094 - accuracy: 0.9490\n",
      "Batch 03760: setting learning rate to 0.00019977308649000465.\n",
      " 200/1780 [==>...........................] - ETA: 7:42 - loss: 0.2096 - accuracy: 0.9490\n",
      "Batch 03761: setting learning rate to 0.00019977260617786167.\n",
      " 201/1780 [==>...........................] - ETA: 7:41 - loss: 0.2104 - accuracy: 0.9487\n",
      "Batch 03762: setting learning rate to 0.00019977212535849173.\n",
      " 202/1780 [==>...........................] - ETA: 7:39 - loss: 0.2110 - accuracy: 0.9485\n",
      "Batch 03763: setting learning rate to 0.0001997716440318972.\n",
      " 203/1780 [==>...........................] - ETA: 7:37 - loss: 0.2103 - accuracy: 0.9487\n",
      "Batch 03764: setting learning rate to 0.00019977116219808058.\n",
      " 204/1780 [==>...........................] - ETA: 7:36 - loss: 0.2101 - accuracy: 0.9486\n",
      "Batch 03765: setting learning rate to 0.00019977067985704427.\n",
      " 205/1780 [==>...........................] - ETA: 7:34 - loss: 0.2110 - accuracy: 0.9485\n",
      "Batch 03766: setting learning rate to 0.00019977019700879078.\n",
      " 206/1780 [==>...........................] - ETA: 7:34 - loss: 0.2108 - accuracy: 0.9485\n",
      "Batch 03767: setting learning rate to 0.00019976971365332253.\n",
      " 207/1780 [==>...........................] - ETA: 7:32 - loss: 0.2110 - accuracy: 0.9484\n",
      "Batch 03768: setting learning rate to 0.00019976922979064198.\n",
      " 208/1780 [==>...........................] - ETA: 7:34 - loss: 0.2108 - accuracy: 0.9485\n",
      "Batch 03769: setting learning rate to 0.00019976874542075162.\n",
      " 209/1780 [==>...........................] - ETA: 7:34 - loss: 0.2110 - accuracy: 0.9484\n",
      "Batch 03770: setting learning rate to 0.00019976826054365386.\n",
      " 210/1780 [==>...........................] - ETA: 7:36 - loss: 0.2106 - accuracy: 0.9486\n",
      "Batch 03771: setting learning rate to 0.00019976777515935123.\n",
      " 211/1780 [==>...........................] - ETA: 7:37 - loss: 0.2137 - accuracy: 0.9484\n",
      "Batch 03772: setting learning rate to 0.0001997672892678461.\n",
      " 212/1780 [==>...........................] - ETA: 7:38 - loss: 0.2139 - accuracy: 0.9481\n",
      "Batch 03773: setting learning rate to 0.00019976680286914105.\n",
      " 213/1780 [==>...........................] - ETA: 7:37 - loss: 0.2147 - accuracy: 0.9481\n",
      "Batch 03774: setting learning rate to 0.0001997663159632385.\n",
      " 214/1780 [==>...........................] - ETA: 7:35 - loss: 0.2155 - accuracy: 0.9476\n",
      "Batch 03775: setting learning rate to 0.00019976582855014093.\n",
      " 215/1780 [==>...........................] - ETA: 7:38 - loss: 0.2163 - accuracy: 0.9471\n",
      "Batch 03776: setting learning rate to 0.0001997653406298508.\n",
      " 216/1780 [==>...........................] - ETA: 7:36 - loss: 0.2169 - accuracy: 0.9465\n",
      "Batch 03777: setting learning rate to 0.00019976485220237064.\n",
      " 217/1780 [==>...........................] - ETA: 7:37 - loss: 0.2179 - accuracy: 0.9459\n",
      "Batch 03778: setting learning rate to 0.00019976436326770287.\n",
      " 218/1780 [==>...........................] - ETA: 7:35 - loss: 0.2177 - accuracy: 0.9458\n",
      "Batch 03779: setting learning rate to 0.00019976387382585.\n",
      " 219/1780 [==>...........................] - ETA: 7:34 - loss: 0.2178 - accuracy: 0.9457\n",
      "Batch 03780: setting learning rate to 0.00019976338387681452.\n",
      " 220/1780 [==>...........................] - ETA: 7:32 - loss: 0.2177 - accuracy: 0.9458\n",
      "Batch 03781: setting learning rate to 0.00019976289342059895.\n",
      " 221/1780 [==>...........................] - ETA: 7:31 - loss: 0.2185 - accuracy: 0.9458\n",
      "Batch 03782: setting learning rate to 0.00019976240245720575.\n",
      " 222/1780 [==>...........................] - ETA: 7:29 - loss: 0.2188 - accuracy: 0.9458\n",
      "Batch 03783: setting learning rate to 0.0001997619109866374.\n",
      " 223/1780 [==>...........................] - ETA: 7:28 - loss: 0.2194 - accuracy: 0.9458\n",
      "Batch 03784: setting learning rate to 0.00019976141900889647.\n",
      " 224/1780 [==>...........................] - ETA: 7:27 - loss: 0.2194 - accuracy: 0.9460\n",
      "Batch 03785: setting learning rate to 0.00019976092652398538.\n",
      " 225/1780 [==>...........................] - ETA: 7:26 - loss: 0.2193 - accuracy: 0.9460\n",
      "Batch 03786: setting learning rate to 0.0001997604335319067.\n",
      " 226/1780 [==>...........................] - ETA: 7:29 - loss: 0.2189 - accuracy: 0.9462\n",
      "Batch 03787: setting learning rate to 0.00019975994003266289.\n",
      " 227/1780 [==>...........................] - ETA: 7:30 - loss: 0.2191 - accuracy: 0.9462\n",
      "Batch 03788: setting learning rate to 0.00019975944602625644.\n",
      " 228/1780 [==>...........................] - ETA: 7:35 - loss: 0.2188 - accuracy: 0.9461\n",
      "Batch 03789: setting learning rate to 0.00019975895151268995.\n",
      " 229/1780 [==>...........................] - ETA: 7:33 - loss: 0.2195 - accuracy: 0.9457\n",
      "Batch 03790: setting learning rate to 0.00019975845649196586.\n",
      " 230/1780 [==>...........................] - ETA: 7:32 - loss: 0.2198 - accuracy: 0.9455\n",
      "Batch 03791: setting learning rate to 0.00019975796096408668.\n",
      " 231/1780 [==>...........................] - ETA: 7:30 - loss: 0.2195 - accuracy: 0.9456\n",
      "Batch 03792: setting learning rate to 0.00019975746492905495.\n",
      " 232/1780 [==>...........................] - ETA: 7:29 - loss: 0.2191 - accuracy: 0.9456\n",
      "Batch 03793: setting learning rate to 0.00019975696838687323.\n",
      " 233/1780 [==>...........................] - ETA: 7:30 - loss: 0.2202 - accuracy: 0.9451\n",
      "Batch 03794: setting learning rate to 0.00019975647133754402.\n",
      " 234/1780 [==>...........................] - ETA: 7:29 - loss: 0.2201 - accuracy: 0.9447\n",
      "Batch 03795: setting learning rate to 0.0001997559737810698.\n",
      " 235/1780 [==>...........................] - ETA: 7:28 - loss: 0.2202 - accuracy: 0.9446\n",
      "Batch 03796: setting learning rate to 0.00019975547571745317.\n",
      " 236/1780 [==>...........................] - ETA: 7:26 - loss: 0.2201 - accuracy: 0.9445\n",
      "Batch 03797: setting learning rate to 0.0001997549771466966.\n",
      " 237/1780 [==>...........................] - ETA: 7:25 - loss: 0.2198 - accuracy: 0.9446\n",
      "Batch 03798: setting learning rate to 0.00019975447806880268.\n",
      " 238/1780 [===>..........................] - ETA: 7:23 - loss: 0.2193 - accuracy: 0.9448\n",
      "Batch 03799: setting learning rate to 0.00019975397848377387.\n",
      " 239/1780 [===>..........................] - ETA: 7:21 - loss: 0.2195 - accuracy: 0.9450\n",
      "Batch 03800: setting learning rate to 0.0001997534783916128.\n",
      " 240/1780 [===>..........................] - ETA: 7:24 - loss: 0.2195 - accuracy: 0.9451\n",
      "Batch 03801: setting learning rate to 0.00019975297779232197.\n",
      " 241/1780 [===>..........................] - ETA: 7:23 - loss: 0.2206 - accuracy: 0.9452\n",
      "Batch 03802: setting learning rate to 0.00019975247668590395.\n",
      " 242/1780 [===>..........................] - ETA: 7:24 - loss: 0.2206 - accuracy: 0.9453\n",
      "Batch 03803: setting learning rate to 0.00019975197507236122.\n",
      " 243/1780 [===>..........................] - ETA: 7:25 - loss: 0.2208 - accuracy: 0.9453\n",
      "Batch 03804: setting learning rate to 0.0001997514729516964.\n",
      " 244/1780 [===>..........................] - ETA: 7:27 - loss: 0.2204 - accuracy: 0.9454\n",
      "Batch 03805: setting learning rate to 0.00019975097032391204.\n",
      " 245/1780 [===>..........................] - ETA: 7:26 - loss: 0.2201 - accuracy: 0.9455\n",
      "Batch 03806: setting learning rate to 0.00019975046718901065.\n",
      " 246/1780 [===>..........................] - ETA: 7:27 - loss: 0.2200 - accuracy: 0.9455\n",
      "Batch 03807: setting learning rate to 0.0001997499635469948.\n",
      " 247/1780 [===>..........................] - ETA: 7:25 - loss: 0.2200 - accuracy: 0.9455\n",
      "Batch 03808: setting learning rate to 0.0001997494593978671.\n",
      " 248/1780 [===>..........................] - ETA: 7:24 - loss: 0.2202 - accuracy: 0.9455\n",
      "Batch 03809: setting learning rate to 0.00019974895474163002.\n",
      " 249/1780 [===>..........................] - ETA: 7:23 - loss: 0.2202 - accuracy: 0.9455\n",
      "Batch 03810: setting learning rate to 0.0001997484495782862.\n",
      " 250/1780 [===>..........................] - ETA: 7:21 - loss: 0.2202 - accuracy: 0.9452\n",
      "Batch 03811: setting learning rate to 0.00019974794390783819.\n",
      " 251/1780 [===>..........................] - ETA: 7:20 - loss: 0.2203 - accuracy: 0.9453\n",
      "Batch 03812: setting learning rate to 0.00019974743773028858.\n",
      " 252/1780 [===>..........................] - ETA: 7:19 - loss: 0.2201 - accuracy: 0.9453\n",
      "Batch 03813: setting learning rate to 0.0001997469310456399.\n",
      " 253/1780 [===>..........................] - ETA: 7:20 - loss: 0.2201 - accuracy: 0.9453\n",
      "Batch 03814: setting learning rate to 0.00019974642385389477.\n",
      " 254/1780 [===>..........................] - ETA: 7:19 - loss: 0.2197 - accuracy: 0.9453\n",
      "Batch 03815: setting learning rate to 0.00019974591615505577.\n",
      " 255/1780 [===>..........................] - ETA: 7:17 - loss: 0.2194 - accuracy: 0.9454\n",
      "Batch 03816: setting learning rate to 0.0001997454079491254.\n",
      " 256/1780 [===>..........................] - ETA: 7:18 - loss: 0.2189 - accuracy: 0.9455\n",
      "Batch 03817: setting learning rate to 0.00019974489923610633.\n",
      " 257/1780 [===>..........................] - ETA: 7:17 - loss: 0.2185 - accuracy: 0.9457\n",
      "Batch 03818: setting learning rate to 0.00019974439001600112.\n",
      " 258/1780 [===>..........................] - ETA: 7:21 - loss: 0.2184 - accuracy: 0.9459\n",
      "Batch 03819: setting learning rate to 0.00019974388028881236.\n",
      " 259/1780 [===>..........................] - ETA: 7:20 - loss: 0.2182 - accuracy: 0.9459\n",
      "Batch 03820: setting learning rate to 0.00019974337005454262.\n",
      " 260/1780 [===>..........................] - ETA: 7:22 - loss: 0.2179 - accuracy: 0.9461\n",
      "Batch 03821: setting learning rate to 0.0001997428593131945.\n",
      " 261/1780 [===>..........................] - ETA: 7:21 - loss: 0.2183 - accuracy: 0.9460\n",
      "Batch 03822: setting learning rate to 0.00019974234806477065.\n",
      " 262/1780 [===>..........................] - ETA: 7:20 - loss: 0.2184 - accuracy: 0.9459\n",
      "Batch 03823: setting learning rate to 0.00019974183630927362.\n",
      " 263/1780 [===>..........................] - ETA: 7:18 - loss: 0.2184 - accuracy: 0.9458\n",
      "Batch 03824: setting learning rate to 0.000199741324046706.\n",
      " 264/1780 [===>..........................] - ETA: 7:17 - loss: 0.2184 - accuracy: 0.9458\n",
      "Batch 03825: setting learning rate to 0.0001997408112770704.\n",
      " 265/1780 [===>..........................] - ETA: 7:16 - loss: 0.2186 - accuracy: 0.9458\n",
      "Batch 03826: setting learning rate to 0.00019974029800036948.\n",
      " 266/1780 [===>..........................] - ETA: 7:16 - loss: 0.2185 - accuracy: 0.9458\n",
      "Batch 03827: setting learning rate to 0.0001997397842166058.\n",
      " 267/1780 [===>..........................] - ETA: 7:14 - loss: 0.2182 - accuracy: 0.9458\n",
      "Batch 03828: setting learning rate to 0.00019973926992578197.\n",
      " 268/1780 [===>..........................] - ETA: 7:14 - loss: 0.2180 - accuracy: 0.9458\n",
      "Batch 03829: setting learning rate to 0.00019973875512790064.\n",
      " 269/1780 [===>..........................] - ETA: 7:14 - loss: 0.2189 - accuracy: 0.9456\n",
      "Batch 03830: setting learning rate to 0.00019973823982296436.\n",
      " 270/1780 [===>..........................] - ETA: 7:12 - loss: 0.2190 - accuracy: 0.9457\n",
      "Batch 03831: setting learning rate to 0.0001997377240109758.\n",
      " 271/1780 [===>..........................] - ETA: 7:13 - loss: 0.2187 - accuracy: 0.9457\n",
      "Batch 03832: setting learning rate to 0.0001997372076919376.\n",
      " 272/1780 [===>..........................] - ETA: 7:12 - loss: 0.2186 - accuracy: 0.9458\n",
      "Batch 03833: setting learning rate to 0.00019973669086585237.\n",
      " 273/1780 [===>..........................] - ETA: 7:12 - loss: 0.2187 - accuracy: 0.9456\n",
      "Batch 03834: setting learning rate to 0.00019973617353272272.\n",
      " 274/1780 [===>..........................] - ETA: 7:16 - loss: 0.2192 - accuracy: 0.9457\n",
      "Batch 03835: setting learning rate to 0.0001997356556925513.\n",
      " 275/1780 [===>..........................] - ETA: 7:15 - loss: 0.2193 - accuracy: 0.9456\n",
      "Batch 03836: setting learning rate to 0.00019973513734534068.\n",
      " 276/1780 [===>..........................] - ETA: 7:16 - loss: 0.2195 - accuracy: 0.9455\n",
      "Batch 03837: setting learning rate to 0.0001997346184910936.\n",
      " 277/1780 [===>..........................] - ETA: 7:15 - loss: 0.2195 - accuracy: 0.9455\n",
      "Batch 03838: setting learning rate to 0.0001997340991298126.\n",
      " 278/1780 [===>..........................] - ETA: 7:13 - loss: 0.2196 - accuracy: 0.9454\n",
      "Batch 03839: setting learning rate to 0.00019973357926150037.\n",
      " 279/1780 [===>..........................] - ETA: 7:12 - loss: 0.2192 - accuracy: 0.9454\n",
      "Batch 03840: setting learning rate to 0.00019973305888615957.\n",
      " 280/1780 [===>..........................] - ETA: 7:11 - loss: 0.2189 - accuracy: 0.9455\n",
      "Batch 03841: setting learning rate to 0.00019973253800379282.\n",
      " 281/1780 [===>..........................] - ETA: 7:10 - loss: 0.2187 - accuracy: 0.9455\n",
      "Batch 03842: setting learning rate to 0.00019973201661440277.\n",
      " 282/1780 [===>..........................] - ETA: 7:11 - loss: 0.2184 - accuracy: 0.9455\n",
      "Batch 03843: setting learning rate to 0.00019973149471799205.\n",
      " 283/1780 [===>..........................] - ETA: 7:10 - loss: 0.2181 - accuracy: 0.9456\n",
      "Batch 03844: setting learning rate to 0.00019973097231456338.\n",
      " 284/1780 [===>..........................] - ETA: 7:11 - loss: 0.2180 - accuracy: 0.9455\n",
      "Batch 03845: setting learning rate to 0.00019973044940411932.\n",
      " 285/1780 [===>..........................] - ETA: 7:10 - loss: 0.2183 - accuracy: 0.9454\n",
      "Batch 03846: setting learning rate to 0.0001997299259866626.\n",
      " 286/1780 [===>..........................] - ETA: 7:09 - loss: 0.2181 - accuracy: 0.9455\n",
      "Batch 03847: setting learning rate to 0.00019972940206219584.\n",
      " 287/1780 [===>..........................] - ETA: 7:10 - loss: 0.2176 - accuracy: 0.9456\n",
      "Batch 03848: setting learning rate to 0.00019972887763072174.\n",
      " 288/1780 [===>..........................] - ETA: 7:08 - loss: 0.2173 - accuracy: 0.9457\n",
      "Batch 03849: setting learning rate to 0.00019972835269224296.\n",
      " 289/1780 [===>..........................] - ETA: 7:07 - loss: 0.2172 - accuracy: 0.9456\n",
      "Batch 03850: setting learning rate to 0.00019972782724676216.\n",
      " 290/1780 [===>..........................] - ETA: 7:10 - loss: 0.2166 - accuracy: 0.9457\n",
      "Batch 03851: setting learning rate to 0.000199727301294282.\n",
      " 291/1780 [===>..........................] - ETA: 7:12 - loss: 0.2162 - accuracy: 0.9458\n",
      "Batch 03852: setting learning rate to 0.00019972677483480515.\n",
      " 292/1780 [===>..........................] - ETA: 7:11 - loss: 0.2173 - accuracy: 0.9455\n",
      "Batch 03853: setting learning rate to 0.0001997262478683343.\n",
      " 293/1780 [===>..........................] - ETA: 7:10 - loss: 0.2179 - accuracy: 0.9456\n",
      "Batch 03854: setting learning rate to 0.00019972572039487214.\n",
      " 294/1780 [===>..........................] - ETA: 7:09 - loss: 0.2176 - accuracy: 0.9456\n",
      "Batch 03855: setting learning rate to 0.00019972519241442135.\n",
      " 295/1780 [===>..........................] - ETA: 7:07 - loss: 0.2172 - accuracy: 0.9457\n",
      "Batch 03856: setting learning rate to 0.00019972466392698457.\n",
      " 296/1780 [===>..........................] - ETA: 7:06 - loss: 0.2178 - accuracy: 0.9456\n",
      "Batch 03857: setting learning rate to 0.00019972413493256455.\n",
      " 297/1780 [====>.........................] - ETA: 7:05 - loss: 0.2175 - accuracy: 0.9456\n",
      "Batch 03858: setting learning rate to 0.00019972360543116395.\n",
      " 298/1780 [====>.........................] - ETA: 7:04 - loss: 0.2174 - accuracy: 0.9455\n",
      "Batch 03859: setting learning rate to 0.00019972307542278546.\n",
      " 299/1780 [====>.........................] - ETA: 7:04 - loss: 0.2172 - accuracy: 0.9455\n",
      "Batch 03860: setting learning rate to 0.00019972254490743175.\n",
      " 300/1780 [====>.........................] - ETA: 7:03 - loss: 0.2171 - accuracy: 0.9455\n",
      "Batch 03861: setting learning rate to 0.00019972201388510557.\n",
      " 301/1780 [====>.........................] - ETA: 7:05 - loss: 0.2171 - accuracy: 0.9455\n",
      "Batch 03862: setting learning rate to 0.0001997214823558096.\n",
      " 302/1780 [====>.........................] - ETA: 7:04 - loss: 0.2174 - accuracy: 0.9454\n",
      "Batch 03863: setting learning rate to 0.00019972095031954652.\n",
      " 303/1780 [====>.........................] - ETA: 7:05 - loss: 0.2171 - accuracy: 0.9455\n",
      "Batch 03864: setting learning rate to 0.00019972041777631904.\n",
      " 304/1780 [====>.........................] - ETA: 7:05 - loss: 0.2166 - accuracy: 0.9457\n",
      "Batch 03865: setting learning rate to 0.0001997198847261299.\n",
      " 305/1780 [====>.........................] - ETA: 7:04 - loss: 0.2165 - accuracy: 0.9456\n",
      "Batch 03866: setting learning rate to 0.00019971935116898178.\n",
      " 306/1780 [====>.........................] - ETA: 7:06 - loss: 0.2163 - accuracy: 0.9458\n",
      "Batch 03867: setting learning rate to 0.00019971881710487743.\n",
      " 307/1780 [====>.........................] - ETA: 7:07 - loss: 0.2160 - accuracy: 0.9458\n",
      "Batch 03868: setting learning rate to 0.0001997182825338195.\n",
      " 308/1780 [====>.........................] - ETA: 7:06 - loss: 0.2156 - accuracy: 0.9460\n",
      "Batch 03869: setting learning rate to 0.00019971774745581073.\n",
      " 309/1780 [====>.........................] - ETA: 7:05 - loss: 0.2156 - accuracy: 0.9460\n",
      "Batch 03870: setting learning rate to 0.0001997172118708539.\n",
      " 310/1780 [====>.........................] - ETA: 7:04 - loss: 0.2161 - accuracy: 0.9461\n",
      "Batch 03871: setting learning rate to 0.00019971667577895163.\n",
      " 311/1780 [====>.........................] - ETA: 7:03 - loss: 0.2161 - accuracy: 0.9460\n",
      "Batch 03872: setting learning rate to 0.00019971613918010674.\n",
      " 312/1780 [====>.........................] - ETA: 7:02 - loss: 0.2164 - accuracy: 0.9458\n",
      "Batch 03873: setting learning rate to 0.00019971560207432192.\n",
      " 313/1780 [====>.........................] - ETA: 7:01 - loss: 0.2161 - accuracy: 0.9459\n",
      "Batch 03874: setting learning rate to 0.0001997150644615999.\n",
      " 314/1780 [====>.........................] - ETA: 6:59 - loss: 0.2159 - accuracy: 0.9459\n",
      "Batch 03875: setting learning rate to 0.00019971452634194337.\n",
      " 315/1780 [====>.........................] - ETA: 6:59 - loss: 0.2157 - accuracy: 0.9459\n",
      "Batch 03876: setting learning rate to 0.00019971398771535513.\n",
      " 316/1780 [====>.........................] - ETA: 6:58 - loss: 0.2156 - accuracy: 0.9460\n",
      "Batch 03877: setting learning rate to 0.00019971344858183793.\n",
      " 317/1780 [====>.........................] - ETA: 6:58 - loss: 0.2157 - accuracy: 0.9458\n",
      "Batch 03878: setting learning rate to 0.00019971290894139446.\n",
      " 318/1780 [====>.........................] - ETA: 6:58 - loss: 0.2158 - accuracy: 0.9458\n",
      "Batch 03879: setting learning rate to 0.00019971236879402746.\n",
      " 319/1780 [====>.........................] - ETA: 6:57 - loss: 0.2160 - accuracy: 0.9457\n",
      "Batch 03880: setting learning rate to 0.00019971182813973968.\n",
      " 320/1780 [====>.........................] - ETA: 7:00 - loss: 0.2166 - accuracy: 0.9456\n",
      "Batch 03881: setting learning rate to 0.00019971128697853394.\n",
      " 321/1780 [====>.........................] - ETA: 6:58 - loss: 0.2167 - accuracy: 0.9456\n",
      "Batch 03882: setting learning rate to 0.0001997107453104129.\n",
      " 322/1780 [====>.........................] - ETA: 7:00 - loss: 0.2168 - accuracy: 0.9456\n",
      "Batch 03883: setting learning rate to 0.0001997102031353794.\n",
      " 323/1780 [====>.........................] - ETA: 7:01 - loss: 0.2169 - accuracy: 0.9456\n",
      "Batch 03884: setting learning rate to 0.00019970966045343612.\n",
      " 324/1780 [====>.........................] - ETA: 7:00 - loss: 0.2164 - accuracy: 0.9457\n",
      "Batch 03885: setting learning rate to 0.00019970911726458583.\n",
      " 325/1780 [====>.........................] - ETA: 7:00 - loss: 0.2167 - accuracy: 0.9456\n",
      "Batch 03886: setting learning rate to 0.0001997085735688313.\n",
      " 326/1780 [====>.........................] - ETA: 6:58 - loss: 0.2161 - accuracy: 0.9457\n",
      "Batch 03887: setting learning rate to 0.0001997080293661753.\n",
      " 327/1780 [====>.........................] - ETA: 6:57 - loss: 0.2158 - accuracy: 0.9458\n",
      "Batch 03888: setting learning rate to 0.00019970748465662067.\n",
      " 328/1780 [====>.........................] - ETA: 6:56 - loss: 0.2155 - accuracy: 0.9458\n",
      "Batch 03889: setting learning rate to 0.00019970693944017004.\n",
      " 329/1780 [====>.........................] - ETA: 6:55 - loss: 0.2150 - accuracy: 0.9460\n",
      "Batch 03890: setting learning rate to 0.00019970639371682625.\n",
      " 330/1780 [====>.........................] - ETA: 6:54 - loss: 0.2146 - accuracy: 0.9461\n",
      "Batch 03891: setting learning rate to 0.00019970584748659208.\n",
      " 331/1780 [====>.........................] - ETA: 6:53 - loss: 0.2148 - accuracy: 0.9460\n",
      "Batch 03892: setting learning rate to 0.00019970530074947033.\n",
      " 332/1780 [====>.........................] - ETA: 6:52 - loss: 0.2146 - accuracy: 0.9461\n",
      "Batch 03893: setting learning rate to 0.00019970475350546374.\n",
      " 333/1780 [====>.........................] - ETA: 6:53 - loss: 0.2143 - accuracy: 0.9461\n",
      "Batch 03894: setting learning rate to 0.00019970420575457506.\n",
      " 334/1780 [====>.........................] - ETA: 6:52 - loss: 0.2144 - accuracy: 0.9461\n",
      "Batch 03895: setting learning rate to 0.00019970365749680716.\n",
      " 335/1780 [====>.........................] - ETA: 6:53 - loss: 0.2141 - accuracy: 0.9462\n",
      "Batch 03896: setting learning rate to 0.00019970310873216275.\n",
      " 336/1780 [====>.........................] - ETA: 6:56 - loss: 0.2151 - accuracy: 0.9460\n",
      "Batch 03897: setting learning rate to 0.0001997025594606447.\n",
      " 337/1780 [====>.........................] - ETA: 6:55 - loss: 0.2149 - accuracy: 0.9461\n",
      "Batch 03898: setting learning rate to 0.0001997020096822557.\n",
      " 338/1780 [====>.........................] - ETA: 6:56 - loss: 0.2151 - accuracy: 0.9460\n",
      "Batch 03899: setting learning rate to 0.00019970145939699868.\n",
      " 339/1780 [====>.........................] - ETA: 6:55 - loss: 0.2152 - accuracy: 0.9459\n",
      "Batch 03900: setting learning rate to 0.0001997009086048763.\n",
      " 340/1780 [====>.........................] - ETA: 6:54 - loss: 0.2153 - accuracy: 0.9457\n",
      "Batch 03901: setting learning rate to 0.00019970035730589142.\n",
      " 341/1780 [====>.........................] - ETA: 6:53 - loss: 0.2153 - accuracy: 0.9456\n",
      "Batch 03902: setting learning rate to 0.00019969980550004686.\n",
      " 342/1780 [====>.........................] - ETA: 6:52 - loss: 0.2153 - accuracy: 0.9455\n",
      "Batch 03903: setting learning rate to 0.0001996992531873454.\n",
      " 343/1780 [====>.........................] - ETA: 6:52 - loss: 0.2152 - accuracy: 0.9455\n",
      "Batch 03904: setting learning rate to 0.00019969870036778983.\n",
      " 344/1780 [====>.........................] - ETA: 6:52 - loss: 0.2150 - accuracy: 0.9455\n",
      "Batch 03905: setting learning rate to 0.00019969814704138303.\n",
      " 345/1780 [====>.........................] - ETA: 6:51 - loss: 0.2149 - accuracy: 0.9455\n",
      "Batch 03906: setting learning rate to 0.00019969759320812776.\n",
      " 346/1780 [====>.........................] - ETA: 6:50 - loss: 0.2147 - accuracy: 0.9455\n",
      "Batch 03907: setting learning rate to 0.0001996970388680268.\n",
      " 347/1780 [====>.........................] - ETA: 6:49 - loss: 0.2148 - accuracy: 0.9455\n",
      "Batch 03908: setting learning rate to 0.00019969648402108305.\n",
      " 348/1780 [====>.........................] - ETA: 6:48 - loss: 0.2145 - accuracy: 0.9456\n",
      "Batch 03909: setting learning rate to 0.0001996959286672993.\n",
      " 349/1780 [====>.........................] - ETA: 6:47 - loss: 0.2143 - accuracy: 0.9456\n",
      "Batch 03910: setting learning rate to 0.00019969537280667833.\n",
      " 350/1780 [====>.........................] - ETA: 6:47 - loss: 0.2143 - accuracy: 0.9456\n",
      "Batch 03911: setting learning rate to 0.00019969481643922302.\n",
      " 351/1780 [====>.........................] - ETA: 6:49 - loss: 0.2139 - accuracy: 0.9457\n",
      "Batch 03912: setting learning rate to 0.0001996942595649362.\n",
      " 352/1780 [====>.........................] - ETA: 6:50 - loss: 0.2134 - accuracy: 0.9459\n",
      "Batch 03913: setting learning rate to 0.0001996937021838207.\n",
      " 353/1780 [====>.........................] - ETA: 6:49 - loss: 0.2137 - accuracy: 0.9459\n",
      "Batch 03914: setting learning rate to 0.00019969314429587928.\n",
      " 354/1780 [====>.........................] - ETA: 6:49 - loss: 0.2136 - accuracy: 0.9458\n",
      "Batch 03915: setting learning rate to 0.00019969258590111485.\n",
      " 355/1780 [====>.........................] - ETA: 6:49 - loss: 0.2134 - accuracy: 0.9459\n",
      "Batch 03916: setting learning rate to 0.00019969202699953025.\n",
      " 356/1780 [=====>........................] - ETA: 6:48 - loss: 0.2132 - accuracy: 0.9459\n",
      "Batch 03917: setting learning rate to 0.00019969146759112828.\n",
      " 357/1780 [=====>........................] - ETA: 6:47 - loss: 0.2131 - accuracy: 0.9458\n",
      "Batch 03918: setting learning rate to 0.0001996909076759118.\n",
      " 358/1780 [=====>........................] - ETA: 6:48 - loss: 0.2130 - accuracy: 0.9457\n",
      "Batch 03919: setting learning rate to 0.00019969034725388367.\n",
      " 359/1780 [=====>........................] - ETA: 6:47 - loss: 0.2128 - accuracy: 0.9458\n",
      "Batch 03920: setting learning rate to 0.00019968978632504673.\n",
      " 360/1780 [=====>........................] - ETA: 6:46 - loss: 0.2125 - accuracy: 0.9458\n",
      "Batch 03921: setting learning rate to 0.00019968922488940383.\n",
      " 361/1780 [=====>........................] - ETA: 6:45 - loss: 0.2125 - accuracy: 0.9458\n",
      "Batch 03922: setting learning rate to 0.00019968866294695783.\n",
      " 362/1780 [=====>........................] - ETA: 6:44 - loss: 0.2124 - accuracy: 0.9458\n",
      "Batch 03923: setting learning rate to 0.00019968810049771158.\n",
      " 363/1780 [=====>........................] - ETA: 6:44 - loss: 0.2123 - accuracy: 0.9459\n",
      "Batch 03924: setting learning rate to 0.00019968753754166798.\n",
      " 364/1780 [=====>........................] - ETA: 6:42 - loss: 0.2124 - accuracy: 0.9458\n",
      "Batch 03925: setting learning rate to 0.0001996869740788298.\n",
      " 365/1780 [=====>........................] - ETA: 6:42 - loss: 0.2120 - accuracy: 0.9459\n",
      "Batch 03926: setting learning rate to 0.0001996864101092.\n",
      " 366/1780 [=====>........................] - ETA: 6:42 - loss: 0.2120 - accuracy: 0.9459\n",
      "Batch 03927: setting learning rate to 0.0001996858456327814.\n",
      " 367/1780 [=====>........................] - ETA: 6:44 - loss: 0.2123 - accuracy: 0.9458\n",
      "Batch 03928: setting learning rate to 0.00019968528064957686.\n",
      " 368/1780 [=====>........................] - ETA: 6:44 - loss: 0.2123 - accuracy: 0.9458\n",
      "Batch 03929: setting learning rate to 0.00019968471515958926.\n",
      " 369/1780 [=====>........................] - ETA: 6:44 - loss: 0.2121 - accuracy: 0.9459\n",
      "Batch 03930: setting learning rate to 0.0001996841491628215.\n",
      " 370/1780 [=====>........................] - ETA: 6:44 - loss: 0.2117 - accuracy: 0.9460\n",
      "Batch 03931: setting learning rate to 0.00019968358265927644.\n",
      " 371/1780 [=====>........................] - ETA: 6:43 - loss: 0.2116 - accuracy: 0.9460\n",
      "Batch 03932: setting learning rate to 0.000199683015648957.\n",
      " 372/1780 [=====>........................] - ETA: 6:42 - loss: 0.2117 - accuracy: 0.9460\n",
      "Batch 03933: setting learning rate to 0.00019968244813186597.\n",
      " 373/1780 [=====>........................] - ETA: 6:41 - loss: 0.2121 - accuracy: 0.9460\n",
      "Batch 03934: setting learning rate to 0.0001996818801080063.\n",
      " 374/1780 [=====>........................] - ETA: 6:42 - loss: 0.2119 - accuracy: 0.9460\n",
      "Batch 03935: setting learning rate to 0.00019968131157738087.\n",
      " 375/1780 [=====>........................] - ETA: 6:42 - loss: 0.2119 - accuracy: 0.9459\n",
      "Batch 03936: setting learning rate to 0.00019968074253999256.\n",
      " 376/1780 [=====>........................] - ETA: 6:41 - loss: 0.2118 - accuracy: 0.9459\n",
      "Batch 03937: setting learning rate to 0.0001996801729958443.\n",
      " 377/1780 [=====>........................] - ETA: 6:40 - loss: 0.2125 - accuracy: 0.9459\n",
      "Batch 03938: setting learning rate to 0.00019967960294493893.\n",
      " 378/1780 [=====>........................] - ETA: 6:39 - loss: 0.2123 - accuracy: 0.9458\n",
      "Batch 03939: setting learning rate to 0.00019967903238727938.\n",
      " 379/1780 [=====>........................] - ETA: 6:38 - loss: 0.2124 - accuracy: 0.9459\n",
      "Batch 03940: setting learning rate to 0.00019967846132286853.\n",
      " 380/1780 [=====>........................] - ETA: 6:39 - loss: 0.2125 - accuracy: 0.9458\n",
      "Batch 03941: setting learning rate to 0.00019967788975170931.\n",
      " 381/1780 [=====>........................] - ETA: 6:38 - loss: 0.2124 - accuracy: 0.9459\n",
      "Batch 03942: setting learning rate to 0.00019967731767380463.\n",
      " 382/1780 [=====>........................] - ETA: 6:37 - loss: 0.2122 - accuracy: 0.9460\n",
      "Batch 03943: setting learning rate to 0.00019967674508915734.\n",
      " 383/1780 [=====>........................] - ETA: 6:40 - loss: 0.2129 - accuracy: 0.9458\n",
      "Batch 03944: setting learning rate to 0.00019967617199777041.\n",
      " 384/1780 [=====>........................] - ETA: 6:40 - loss: 0.2132 - accuracy: 0.9458\n",
      "Batch 03945: setting learning rate to 0.00019967559839964674.\n",
      " 385/1780 [=====>........................] - ETA: 6:39 - loss: 0.2132 - accuracy: 0.9458\n",
      "Batch 03946: setting learning rate to 0.00019967502429478926.\n",
      " 386/1780 [=====>........................] - ETA: 6:39 - loss: 0.2134 - accuracy: 0.9457\n",
      "Batch 03947: setting learning rate to 0.00019967444968320086.\n",
      " 387/1780 [=====>........................] - ETA: 6:39 - loss: 0.2132 - accuracy: 0.9458\n",
      "Batch 03948: setting learning rate to 0.00019967387456488447.\n",
      " 388/1780 [=====>........................] - ETA: 6:39 - loss: 0.2131 - accuracy: 0.9458\n",
      "Batch 03949: setting learning rate to 0.00019967329893984302.\n",
      " 389/1780 [=====>........................] - ETA: 6:38 - loss: 0.2131 - accuracy: 0.9458\n",
      "Batch 03950: setting learning rate to 0.00019967272280807942.\n",
      " 390/1780 [=====>........................] - ETA: 6:38 - loss: 0.2130 - accuracy: 0.9458\n",
      "Batch 03951: setting learning rate to 0.0001996721461695966.\n",
      " 391/1780 [=====>........................] - ETA: 6:37 - loss: 0.2129 - accuracy: 0.9458\n",
      "Batch 03952: setting learning rate to 0.00019967156902439749.\n",
      " 392/1780 [=====>........................] - ETA: 6:36 - loss: 0.2132 - accuracy: 0.9458\n",
      "Batch 03953: setting learning rate to 0.00019967099137248507.\n",
      " 393/1780 [=====>........................] - ETA: 6:35 - loss: 0.2132 - accuracy: 0.9457\n",
      "Batch 03954: setting learning rate to 0.00019967041321386223.\n",
      " 394/1780 [=====>........................] - ETA: 6:34 - loss: 0.2133 - accuracy: 0.9458\n",
      "Batch 03955: setting learning rate to 0.00019966983454853195.\n",
      " 395/1780 [=====>........................] - ETA: 6:33 - loss: 0.2133 - accuracy: 0.9458\n",
      "Batch 03956: setting learning rate to 0.00019966925537649713.\n",
      " 396/1780 [=====>........................] - ETA: 6:33 - loss: 0.2130 - accuracy: 0.9458\n",
      "Batch 03957: setting learning rate to 0.0001996686756977607.\n",
      " 397/1780 [=====>........................] - ETA: 6:32 - loss: 0.2134 - accuracy: 0.9457\n",
      "Batch 03958: setting learning rate to 0.00019966809551232566.\n",
      " 398/1780 [=====>........................] - ETA: 6:32 - loss: 0.2136 - accuracy: 0.9456\n",
      "Batch 03959: setting learning rate to 0.00019966751482019494.\n",
      " 399/1780 [=====>........................] - ETA: 6:35 - loss: 0.2136 - accuracy: 0.9456\n",
      "Batch 03960: setting learning rate to 0.00019966693362137147.\n",
      " 400/1780 [=====>........................] - ETA: 6:34 - loss: 0.2134 - accuracy: 0.9455\n",
      "Batch 03961: setting learning rate to 0.00019966635191585823.\n",
      " 401/1780 [=====>........................] - ETA: 6:34 - loss: 0.2131 - accuracy: 0.9456\n",
      "Batch 03962: setting learning rate to 0.00019966576970365814.\n",
      " 402/1780 [=====>........................] - ETA: 6:33 - loss: 0.2129 - accuracy: 0.9457\n",
      "Batch 03963: setting learning rate to 0.00019966518698477423.\n",
      " 403/1780 [=====>........................] - ETA: 6:32 - loss: 0.2132 - accuracy: 0.9457\n",
      "Batch 03964: setting learning rate to 0.0001996646037592094.\n",
      " 404/1780 [=====>........................] - ETA: 6:33 - loss: 0.2128 - accuracy: 0.9459\n",
      "Batch 03965: setting learning rate to 0.00019966402002696666.\n",
      " 405/1780 [=====>........................] - ETA: 6:34 - loss: 0.2125 - accuracy: 0.9459\n",
      "Batch 03966: setting learning rate to 0.00019966343578804895.\n",
      " 406/1780 [=====>........................] - ETA: 6:34 - loss: 0.2124 - accuracy: 0.9460\n",
      "Batch 03967: setting learning rate to 0.00019966285104245924.\n",
      " 407/1780 [=====>........................] - ETA: 6:33 - loss: 0.2122 - accuracy: 0.9461\n",
      "Batch 03968: setting learning rate to 0.00019966226579020048.\n",
      " 408/1780 [=====>........................] - ETA: 6:33 - loss: 0.2121 - accuracy: 0.9461\n",
      "Batch 03969: setting learning rate to 0.0001996616800312757.\n",
      " 409/1780 [=====>........................] - ETA: 6:32 - loss: 0.2121 - accuracy: 0.9461\n",
      "Batch 03970: setting learning rate to 0.00019966109376568785.\n",
      " 410/1780 [=====>........................] - ETA: 6:31 - loss: 0.2124 - accuracy: 0.9461\n",
      "Batch 03971: setting learning rate to 0.0001996605069934399.\n",
      " 411/1780 [=====>........................] - ETA: 6:30 - loss: 0.2122 - accuracy: 0.9462\n",
      "Batch 03972: setting learning rate to 0.00019965991971453487.\n",
      " 412/1780 [=====>........................] - ETA: 6:29 - loss: 0.2123 - accuracy: 0.9461\n",
      "Batch 03973: setting learning rate to 0.00019965933192897569.\n",
      " 413/1780 [=====>........................] - ETA: 6:28 - loss: 0.2120 - accuracy: 0.9461\n",
      "Batch 03974: setting learning rate to 0.0001996587436367654.\n",
      " 414/1780 [=====>........................] - ETA: 6:27 - loss: 0.2119 - accuracy: 0.9461\n",
      "Batch 03975: setting learning rate to 0.00019965815483790692.\n",
      " 415/1780 [=====>........................] - ETA: 6:29 - loss: 0.2121 - accuracy: 0.9461\n",
      "Batch 03976: setting learning rate to 0.00019965756553240333.\n",
      " 416/1780 [======>.......................] - ETA: 6:28 - loss: 0.2118 - accuracy: 0.9461\n",
      "Batch 03977: setting learning rate to 0.0001996569757202576.\n",
      " 417/1780 [======>.......................] - ETA: 6:28 - loss: 0.2118 - accuracy: 0.9461\n",
      "Batch 03978: setting learning rate to 0.0001996563854014727.\n",
      " 418/1780 [======>.......................] - ETA: 6:27 - loss: 0.2117 - accuracy: 0.9460\n",
      "Batch 03979: setting learning rate to 0.00019965579457605165.\n",
      " 419/1780 [======>.......................] - ETA: 6:27 - loss: 0.2116 - accuracy: 0.9460\n",
      "Batch 03980: setting learning rate to 0.00019965520324399748.\n",
      " 420/1780 [======>.......................] - ETA: 6:30 - loss: 0.2117 - accuracy: 0.9458\n",
      "Batch 03981: setting learning rate to 0.0001996546114053131.\n",
      " 421/1780 [======>.......................] - ETA: 6:29 - loss: 0.2116 - accuracy: 0.9457\n",
      "Batch 03982: setting learning rate to 0.00019965401906000163.\n",
      " 422/1780 [======>.......................] - ETA: 6:28 - loss: 0.2120 - accuracy: 0.9455\n",
      "Batch 03983: setting learning rate to 0.00019965342620806604.\n",
      " 423/1780 [======>.......................] - ETA: 6:28 - loss: 0.2118 - accuracy: 0.9455\n",
      "Batch 03984: setting learning rate to 0.00019965283284950933.\n",
      " 424/1780 [======>.......................] - ETA: 6:27 - loss: 0.2118 - accuracy: 0.9454\n",
      "Batch 03985: setting learning rate to 0.00019965223898433455.\n",
      " 425/1780 [======>.......................] - ETA: 6:26 - loss: 0.2115 - accuracy: 0.9455\n",
      "Batch 03986: setting learning rate to 0.00019965164461254467.\n",
      " 426/1780 [======>.......................] - ETA: 6:25 - loss: 0.2115 - accuracy: 0.9455\n",
      "Batch 03987: setting learning rate to 0.00019965104973414274.\n",
      " 427/1780 [======>.......................] - ETA: 6:24 - loss: 0.2117 - accuracy: 0.9456\n",
      "Batch 03988: setting learning rate to 0.00019965045434913178.\n",
      " 428/1780 [======>.......................] - ETA: 6:24 - loss: 0.2124 - accuracy: 0.9456\n",
      "Batch 03989: setting learning rate to 0.00019964985845751483.\n",
      " 429/1780 [======>.......................] - ETA: 6:25 - loss: 0.2121 - accuracy: 0.9456\n",
      "Batch 03990: setting learning rate to 0.0001996492620592949.\n",
      " 430/1780 [======>.......................] - ETA: 6:24 - loss: 0.2119 - accuracy: 0.9457\n",
      "Batch 03991: setting learning rate to 0.00019964866515447503.\n",
      " 431/1780 [======>.......................] - ETA: 6:23 - loss: 0.2117 - accuracy: 0.9458\n",
      "Batch 03992: setting learning rate to 0.00019964806774305828.\n",
      " 432/1780 [======>.......................] - ETA: 6:23 - loss: 0.2114 - accuracy: 0.9458\n",
      "Batch 03993: setting learning rate to 0.00019964746982504766.\n",
      " 433/1780 [======>.......................] - ETA: 6:23 - loss: 0.2112 - accuracy: 0.9459\n",
      "Batch 03994: setting learning rate to 0.0001996468714004462.\n",
      " 434/1780 [======>.......................] - ETA: 6:23 - loss: 0.2114 - accuracy: 0.9458\n",
      "Batch 03995: setting learning rate to 0.00019964627246925695.\n",
      " 435/1780 [======>.......................] - ETA: 6:22 - loss: 0.2112 - accuracy: 0.9457\n",
      "Batch 03996: setting learning rate to 0.00019964567303148295.\n",
      " 436/1780 [======>.......................] - ETA: 6:23 - loss: 0.2111 - accuracy: 0.9457\n",
      "Batch 03997: setting learning rate to 0.00019964507308712728.\n",
      " 437/1780 [======>.......................] - ETA: 6:23 - loss: 0.2109 - accuracy: 0.9458\n",
      "Batch 03998: setting learning rate to 0.00019964447263619297.\n",
      " 438/1780 [======>.......................] - ETA: 6:22 - loss: 0.2114 - accuracy: 0.9457\n",
      "Batch 03999: setting learning rate to 0.00019964387167868306.\n",
      " 439/1780 [======>.......................] - ETA: 6:21 - loss: 0.2115 - accuracy: 0.9457\n",
      "Batch 04000: setting learning rate to 0.0001996432702146006.\n",
      " 440/1780 [======>.......................] - ETA: 6:21 - loss: 0.2117 - accuracy: 0.9457\n",
      "Batch 04001: setting learning rate to 0.00019964266824394868.\n",
      " 441/1780 [======>.......................] - ETA: 6:20 - loss: 0.2117 - accuracy: 0.9457\n",
      "Batch 04002: setting learning rate to 0.00019964206576673034.\n",
      " 442/1780 [======>.......................] - ETA: 6:19 - loss: 0.2118 - accuracy: 0.9456\n",
      "Batch 04003: setting learning rate to 0.00019964146278294865.\n",
      " 443/1780 [======>.......................] - ETA: 6:18 - loss: 0.2118 - accuracy: 0.9456\n",
      "Batch 04004: setting learning rate to 0.00019964085929260667.\n",
      " 444/1780 [======>.......................] - ETA: 6:18 - loss: 0.2116 - accuracy: 0.9456\n",
      "Batch 04005: setting learning rate to 0.00019964025529570746.\n",
      " 445/1780 [======>.......................] - ETA: 6:18 - loss: 0.2115 - accuracy: 0.9455\n",
      "Batch 04006: setting learning rate to 0.00019963965079225414.\n",
      " 446/1780 [======>.......................] - ETA: 6:18 - loss: 0.2116 - accuracy: 0.9456\n",
      "Batch 04007: setting learning rate to 0.00019963904578224967.\n",
      " 447/1780 [======>.......................] - ETA: 6:18 - loss: 0.2114 - accuracy: 0.9456\n",
      "Batch 04008: setting learning rate to 0.00019963844026569727.\n",
      " 448/1780 [======>.......................] - ETA: 6:17 - loss: 0.2115 - accuracy: 0.9455\n",
      "Batch 04009: setting learning rate to 0.00019963783424259993.\n",
      " 449/1780 [======>.......................] - ETA: 6:19 - loss: 0.2115 - accuracy: 0.9455\n",
      "Batch 04010: setting learning rate to 0.0001996372277129607.\n",
      " 450/1780 [======>.......................] - ETA: 6:19 - loss: 0.2117 - accuracy: 0.9455\n",
      "Batch 04011: setting learning rate to 0.00019963662067678275.\n",
      " 451/1780 [======>.......................] - ETA: 6:18 - loss: 0.2116 - accuracy: 0.9456\n",
      "Batch 04012: setting learning rate to 0.00019963601313406913.\n",
      " 452/1780 [======>.......................] - ETA: 6:18 - loss: 0.2132 - accuracy: 0.9454\n",
      "Batch 04013: setting learning rate to 0.00019963540508482293.\n",
      " 453/1780 [======>.......................] - ETA: 6:20 - loss: 0.2130 - accuracy: 0.9455\n",
      "Batch 04014: setting learning rate to 0.00019963479652904722.\n",
      " 454/1780 [======>.......................] - ETA: 6:20 - loss: 0.2129 - accuracy: 0.9456\n",
      "Batch 04015: setting learning rate to 0.0001996341874667451.\n",
      " 455/1780 [======>.......................] - ETA: 6:19 - loss: 0.2128 - accuracy: 0.9455\n",
      "Batch 04016: setting learning rate to 0.0001996335778979197.\n",
      " 456/1780 [======>.......................] - ETA: 6:18 - loss: 0.2127 - accuracy: 0.9454\n",
      "Batch 04017: setting learning rate to 0.00019963296782257413.\n",
      " 457/1780 [======>.......................] - ETA: 6:17 - loss: 0.2128 - accuracy: 0.9454\n",
      "Batch 04018: setting learning rate to 0.00019963235724071142.\n",
      " 458/1780 [======>.......................] - ETA: 6:17 - loss: 0.2129 - accuracy: 0.9452\n",
      "Batch 04019: setting learning rate to 0.00019963174615233472.\n",
      " 459/1780 [======>.......................] - ETA: 6:16 - loss: 0.2136 - accuracy: 0.9450\n",
      "Batch 04020: setting learning rate to 0.00019963113455744713.\n",
      " 460/1780 [======>.......................] - ETA: 6:15 - loss: 0.2136 - accuracy: 0.9449\n",
      "Batch 04021: setting learning rate to 0.00019963052245605174.\n",
      " 461/1780 [======>.......................] - ETA: 6:16 - loss: 0.2135 - accuracy: 0.9448\n",
      "Batch 04022: setting learning rate to 0.00019962990984815168.\n",
      " 462/1780 [======>.......................] - ETA: 6:15 - loss: 0.2134 - accuracy: 0.9448\n",
      "Batch 04023: setting learning rate to 0.0001996292967337501.\n",
      " 463/1780 [======>.......................] - ETA: 6:15 - loss: 0.2133 - accuracy: 0.9449\n",
      "Batch 04024: setting learning rate to 0.00019962868311285007.\n",
      " 464/1780 [======>.......................] - ETA: 6:15 - loss: 0.2132 - accuracy: 0.9449\n",
      "Batch 04025: setting learning rate to 0.0001996280689854547.\n",
      " 465/1780 [======>.......................] - ETA: 6:15 - loss: 0.2134 - accuracy: 0.9450\n",
      "Batch 04026: setting learning rate to 0.00019962745435156713.\n",
      " 466/1780 [======>.......................] - ETA: 6:16 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04027: setting learning rate to 0.00019962683921119053.\n",
      " 467/1780 [======>.......................] - ETA: 6:15 - loss: 0.2133 - accuracy: 0.9451\n",
      "Batch 04028: setting learning rate to 0.00019962622356432794.\n",
      " 468/1780 [======>.......................] - ETA: 6:15 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04029: setting learning rate to 0.00019962560741098258.\n",
      " 469/1780 [======>.......................] - ETA: 6:14 - loss: 0.2132 - accuracy: 0.9450\n",
      "Batch 04030: setting learning rate to 0.0001996249907511575.\n",
      " 470/1780 [======>.......................] - ETA: 6:14 - loss: 0.2130 - accuracy: 0.9450\n",
      "Batch 04031: setting learning rate to 0.0001996243735848559.\n",
      " 471/1780 [======>.......................] - ETA: 6:13 - loss: 0.2130 - accuracy: 0.9450\n",
      "Batch 04032: setting learning rate to 0.00019962375591208085.\n",
      " 472/1780 [======>.......................] - ETA: 6:13 - loss: 0.2131 - accuracy: 0.9449\n",
      "Batch 04033: setting learning rate to 0.00019962313773283556.\n",
      " 473/1780 [======>.......................] - ETA: 6:12 - loss: 0.2130 - accuracy: 0.9450\n",
      "Batch 04034: setting learning rate to 0.00019962251904712312.\n",
      " 474/1780 [======>.......................] - ETA: 6:11 - loss: 0.2129 - accuracy: 0.9450\n",
      "Batch 04035: setting learning rate to 0.0001996218998549467.\n",
      " 475/1780 [=======>......................] - ETA: 6:11 - loss: 0.2128 - accuracy: 0.9450\n",
      "Batch 04036: setting learning rate to 0.00019962128015630948.\n",
      " 476/1780 [=======>......................] - ETA: 6:10 - loss: 0.2128 - accuracy: 0.9450\n",
      "Batch 04037: setting learning rate to 0.00019962065995121457.\n",
      " 477/1780 [=======>......................] - ETA: 6:11 - loss: 0.2128 - accuracy: 0.9450\n",
      "Batch 04038: setting learning rate to 0.0001996200392396651.\n",
      " 478/1780 [=======>......................] - ETA: 6:10 - loss: 0.2126 - accuracy: 0.9451\n",
      "Batch 04039: setting learning rate to 0.00019961941802166427.\n",
      " 479/1780 [=======>......................] - ETA: 6:11 - loss: 0.2130 - accuracy: 0.9450\n",
      "Batch 04040: setting learning rate to 0.00019961879629721522.\n",
      " 480/1780 [=======>......................] - ETA: 6:10 - loss: 0.2132 - accuracy: 0.9450\n",
      "Batch 04041: setting learning rate to 0.00019961817406632113.\n",
      " 481/1780 [=======>......................] - ETA: 6:12 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04042: setting learning rate to 0.00019961755132898512.\n",
      " 482/1780 [=======>......................] - ETA: 6:11 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04043: setting learning rate to 0.00019961692808521042.\n",
      " 483/1780 [=======>......................] - ETA: 6:11 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04044: setting learning rate to 0.00019961630433500013.\n",
      " 484/1780 [=======>......................] - ETA: 6:10 - loss: 0.2130 - accuracy: 0.9451\n",
      "Batch 04045: setting learning rate to 0.00019961568007835747.\n",
      " 485/1780 [=======>......................] - ETA: 6:11 - loss: 0.2129 - accuracy: 0.9451\n",
      "Batch 04046: setting learning rate to 0.0001996150553152856.\n",
      " 486/1780 [=======>......................] - ETA: 6:10 - loss: 0.2130 - accuracy: 0.9451\n",
      "Batch 04047: setting learning rate to 0.00019961443004578767.\n",
      " 487/1780 [=======>......................] - ETA: 6:09 - loss: 0.2131 - accuracy: 0.9450\n",
      "Batch 04048: setting learning rate to 0.0001996138042698669.\n",
      " 488/1780 [=======>......................] - ETA: 6:09 - loss: 0.2130 - accuracy: 0.9451\n",
      "Batch 04049: setting learning rate to 0.00019961317798752645.\n",
      " 489/1780 [=======>......................] - ETA: 6:08 - loss: 0.2134 - accuracy: 0.9450\n",
      "Batch 04050: setting learning rate to 0.0001996125511987695.\n",
      " 490/1780 [=======>......................] - ETA: 6:07 - loss: 0.2132 - accuracy: 0.9450\n",
      "Batch 04051: setting learning rate to 0.00019961192390359925.\n",
      " 491/1780 [=======>......................] - ETA: 6:06 - loss: 0.2134 - accuracy: 0.9450\n",
      "Batch 04052: setting learning rate to 0.00019961129610201886.\n",
      " 492/1780 [=======>......................] - ETA: 6:06 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04053: setting learning rate to 0.00019961066779403157.\n",
      " 493/1780 [=======>......................] - ETA: 6:06 - loss: 0.2136 - accuracy: 0.9449\n",
      "Batch 04054: setting learning rate to 0.00019961003897964055.\n",
      " 494/1780 [=======>......................] - ETA: 6:06 - loss: 0.2133 - accuracy: 0.9450\n",
      "Batch 04055: setting learning rate to 0.00019960940965884898.\n",
      " 495/1780 [=======>......................] - ETA: 6:09 - loss: 0.2132 - accuracy: 0.9449\n",
      "Batch 04056: setting learning rate to 0.00019960877983166007.\n",
      " 496/1780 [=======>......................] - ETA: 6:08 - loss: 0.2131 - accuracy: 0.9450\n",
      "Batch 04057: setting learning rate to 0.00019960814949807705.\n",
      " 497/1780 [=======>......................] - ETA: 6:08 - loss: 0.2130 - accuracy: 0.9450\n",
      "Batch 04058: setting learning rate to 0.00019960751865810307.\n",
      " 498/1780 [=======>......................] - ETA: 6:07 - loss: 0.2128 - accuracy: 0.9451\n",
      "Batch 04059: setting learning rate to 0.00019960688731174137.\n",
      " 499/1780 [=======>......................] - ETA: 6:07 - loss: 0.2127 - accuracy: 0.9451\n",
      "Batch 04060: setting learning rate to 0.00019960625545899517.\n",
      " 500/1780 [=======>......................] - ETA: 6:06 - loss: 0.2130 - accuracy: 0.9450\n",
      "Batch 04061: setting learning rate to 0.0001996056230998677.\n",
      " 501/1780 [=======>......................] - ETA: 6:05 - loss: 0.2128 - accuracy: 0.9451\n",
      "Batch 04062: setting learning rate to 0.0001996049902343621.\n",
      " 502/1780 [=======>......................] - ETA: 6:04 - loss: 0.2125 - accuracy: 0.9452\n",
      "Batch 04063: setting learning rate to 0.00019960435686248166.\n",
      " 503/1780 [=======>......................] - ETA: 6:05 - loss: 0.2123 - accuracy: 0.9452\n",
      "Batch 04064: setting learning rate to 0.00019960372298422952.\n",
      " 504/1780 [=======>......................] - ETA: 6:04 - loss: 0.2121 - accuracy: 0.9453\n",
      "Batch 04065: setting learning rate to 0.000199603088599609.\n",
      " 505/1780 [=======>......................] - ETA: 6:04 - loss: 0.2123 - accuracy: 0.9453\n",
      "Batch 04066: setting learning rate to 0.00019960245370862327.\n",
      " 506/1780 [=======>......................] - ETA: 6:03 - loss: 0.2125 - accuracy: 0.9452\n",
      "Batch 04067: setting learning rate to 0.00019960181831127558.\n",
      " 507/1780 [=======>......................] - ETA: 6:02 - loss: 0.2123 - accuracy: 0.9453\n",
      "Batch 04068: setting learning rate to 0.00019960118240756913.\n",
      " 508/1780 [=======>......................] - ETA: 6:02 - loss: 0.2123 - accuracy: 0.9453\n",
      "Batch 04069: setting learning rate to 0.00019960054599750716.\n",
      " 509/1780 [=======>......................] - ETA: 6:02 - loss: 0.2121 - accuracy: 0.9453\n",
      "Batch 04070: setting learning rate to 0.00019959990908109294.\n",
      " 510/1780 [=======>......................] - ETA: 6:02 - loss: 0.2122 - accuracy: 0.9452\n",
      "Batch 04071: setting learning rate to 0.0001995992716583297.\n",
      " 511/1780 [=======>......................] - ETA: 6:01 - loss: 0.2123 - accuracy: 0.9451\n",
      "Batch 04072: setting learning rate to 0.0001995986337292206.\n",
      " 512/1780 [=======>......................] - ETA: 6:03 - loss: 0.2123 - accuracy: 0.9451\n",
      "Batch 04073: setting learning rate to 0.00019959799529376899.\n",
      " 513/1780 [=======>......................] - ETA: 6:03 - loss: 0.2122 - accuracy: 0.9451\n",
      "Batch 04074: setting learning rate to 0.0001995973563519781.\n",
      " 514/1780 [=======>......................] - ETA: 6:02 - loss: 0.2125 - accuracy: 0.9451\n",
      "Batch 04075: setting learning rate to 0.0001995967169038511.\n",
      " 515/1780 [=======>......................] - ETA: 6:02 - loss: 0.2123 - accuracy: 0.9452\n",
      "Batch 04076: setting learning rate to 0.00019959607694939132.\n",
      " 516/1780 [=======>......................] - ETA: 6:01 - loss: 0.2125 - accuracy: 0.9451\n",
      "Batch 04077: setting learning rate to 0.00019959543648860197.\n",
      " 517/1780 [=======>......................] - ETA: 6:01 - loss: 0.2124 - accuracy: 0.9451\n",
      "Batch 04078: setting learning rate to 0.00019959479552148632.\n",
      " 518/1780 [=======>......................] - ETA: 6:00 - loss: 0.2125 - accuracy: 0.9452\n",
      "Batch 04079: setting learning rate to 0.00019959415404804763.\n",
      " 519/1780 [=======>......................] - ETA: 6:00 - loss: 0.2128 - accuracy: 0.9451\n",
      "Batch 04080: setting learning rate to 0.0001995935120682892.\n",
      " 520/1780 [=======>......................] - ETA: 5:59 - loss: 0.2126 - accuracy: 0.9451\n",
      "Batch 04081: setting learning rate to 0.0001995928695822142.\n",
      " 521/1780 [=======>......................] - ETA: 6:00 - loss: 0.2123 - accuracy: 0.9452\n",
      "Batch 04082: setting learning rate to 0.00019959222658982596.\n",
      " 522/1780 [=======>......................] - ETA: 5:59 - loss: 0.2124 - accuracy: 0.9452\n",
      "Batch 04083: setting learning rate to 0.00019959158309112778.\n",
      " 523/1780 [=======>......................] - ETA: 5:58 - loss: 0.2122 - accuracy: 0.9453\n",
      "Batch 04084: setting learning rate to 0.00019959093908612286.\n",
      " 524/1780 [=======>......................] - ETA: 5:57 - loss: 0.2122 - accuracy: 0.9453\n",
      "Batch 04085: setting learning rate to 0.0001995902945748145.\n",
      " 525/1780 [=======>......................] - ETA: 5:58 - loss: 0.2120 - accuracy: 0.9454\n",
      "Batch 04086: setting learning rate to 0.000199589649557206.\n",
      " 526/1780 [=======>......................] - ETA: 5:57 - loss: 0.2119 - accuracy: 0.9453\n",
      "Batch 04087: setting learning rate to 0.00019958900403330062.\n",
      " 527/1780 [=======>......................] - ETA: 5:57 - loss: 0.2119 - accuracy: 0.9453\n",
      "Batch 04088: setting learning rate to 0.00019958835800310164.\n",
      " 528/1780 [=======>......................] - ETA: 5:58 - loss: 0.2117 - accuracy: 0.9453\n",
      "Batch 04089: setting learning rate to 0.00019958771146661233.\n",
      " 529/1780 [=======>......................] - ETA: 5:59 - loss: 0.2114 - accuracy: 0.9453\n",
      "Batch 04090: setting learning rate to 0.00019958706442383602.\n",
      " 530/1780 [=======>......................] - ETA: 5:58 - loss: 0.2115 - accuracy: 0.9453\n",
      "Batch 04091: setting learning rate to 0.00019958641687477596.\n",
      " 531/1780 [=======>......................] - ETA: 5:58 - loss: 0.2115 - accuracy: 0.9453\n",
      "Batch 04092: setting learning rate to 0.00019958576881943545.\n",
      " 532/1780 [=======>......................] - ETA: 5:58 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04093: setting learning rate to 0.0001995851202578178.\n",
      " 533/1780 [=======>......................] - ETA: 5:57 - loss: 0.2115 - accuracy: 0.9452\n",
      "Batch 04094: setting learning rate to 0.00019958447118992626.\n",
      " 534/1780 [========>.....................] - ETA: 5:56 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04095: setting learning rate to 0.00019958382161576422.\n",
      " 535/1780 [========>.....................] - ETA: 5:56 - loss: 0.2109 - accuracy: 0.9454\n",
      "Batch 04096: setting learning rate to 0.0001995831715353349.\n",
      " 536/1780 [========>.....................] - ETA: 5:55 - loss: 0.2109 - accuracy: 0.9453\n",
      "Batch 04097: setting learning rate to 0.00019958252094864166.\n",
      " 537/1780 [========>.....................] - ETA: 5:54 - loss: 0.2111 - accuracy: 0.9453\n",
      "Batch 04098: setting learning rate to 0.00019958186985568778.\n",
      " 538/1780 [========>.....................] - ETA: 5:54 - loss: 0.2111 - accuracy: 0.9453\n",
      "Batch 04099: setting learning rate to 0.00019958121825647656.\n",
      " 539/1780 [========>.....................] - ETA: 5:54 - loss: 0.2111 - accuracy: 0.9453\n",
      "Batch 04100: setting learning rate to 0.0001995805661510113.\n",
      " 540/1780 [========>.....................] - ETA: 5:53 - loss: 0.2111 - accuracy: 0.9453\n",
      "Batch 04101: setting learning rate to 0.00019957991353929537.\n",
      " 541/1780 [========>.....................] - ETA: 5:54 - loss: 0.2113 - accuracy: 0.9452\n",
      "Batch 04102: setting learning rate to 0.00019957926042133206.\n",
      " 542/1780 [========>.....................] - ETA: 5:53 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04103: setting learning rate to 0.00019957860679712465.\n",
      " 543/1780 [========>.....................] - ETA: 5:54 - loss: 0.2110 - accuracy: 0.9453\n",
      "Batch 04104: setting learning rate to 0.00019957795266667655.\n",
      " 544/1780 [========>.....................] - ETA: 5:53 - loss: 0.2110 - accuracy: 0.9453\n",
      "Batch 04105: setting learning rate to 0.00019957729802999098.\n",
      " 545/1780 [========>.....................] - ETA: 5:54 - loss: 0.2110 - accuracy: 0.9453\n",
      "Batch 04106: setting learning rate to 0.00019957664288707138.\n",
      " 546/1780 [========>.....................] - ETA: 5:53 - loss: 0.2114 - accuracy: 0.9451\n",
      "Batch 04107: setting learning rate to 0.00019957598723792098.\n",
      " 547/1780 [========>.....................] - ETA: 5:53 - loss: 0.2111 - accuracy: 0.9452\n",
      "Batch 04108: setting learning rate to 0.0001995753310825432.\n",
      " 548/1780 [========>.....................] - ETA: 5:52 - loss: 0.2108 - accuracy: 0.9453\n",
      "Batch 04109: setting learning rate to 0.00019957467442094132.\n",
      " 549/1780 [========>.....................] - ETA: 5:52 - loss: 0.2106 - accuracy: 0.9454\n",
      "Batch 04110: setting learning rate to 0.0001995740172531187.\n",
      " 550/1780 [========>.....................] - ETA: 5:52 - loss: 0.2110 - accuracy: 0.9453\n",
      "Batch 04111: setting learning rate to 0.00019957335957907865.\n",
      " 551/1780 [========>.....................] - ETA: 5:51 - loss: 0.2111 - accuracy: 0.9454\n",
      "Batch 04112: setting learning rate to 0.00019957270139882454.\n",
      " 552/1780 [========>.....................] - ETA: 5:50 - loss: 0.2110 - accuracy: 0.9454\n",
      "Batch 04113: setting learning rate to 0.0001995720427123597.\n",
      " 553/1780 [========>.....................] - ETA: 5:50 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04114: setting learning rate to 0.00019957138351968753.\n",
      " 554/1780 [========>.....................] - ETA: 5:49 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04115: setting learning rate to 0.00019957072382081133.\n",
      " 555/1780 [========>.....................] - ETA: 5:48 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04116: setting learning rate to 0.00019957006361573444.\n",
      " 556/1780 [========>.....................] - ETA: 5:49 - loss: 0.2113 - accuracy: 0.9453\n",
      "Batch 04117: setting learning rate to 0.00019956940290446026.\n",
      " 557/1780 [========>.....................] - ETA: 5:48 - loss: 0.2114 - accuracy: 0.9452\n",
      "Batch 04118: setting learning rate to 0.00019956874168699213.\n",
      " 558/1780 [========>.....................] - ETA: 5:48 - loss: 0.2116 - accuracy: 0.9452\n",
      "Batch 04119: setting learning rate to 0.0001995680799633334.\n",
      " 559/1780 [========>.....................] - ETA: 5:49 - loss: 0.2117 - accuracy: 0.9451\n",
      "Batch 04120: setting learning rate to 0.00019956741773348747.\n",
      " 560/1780 [========>.....................] - ETA: 5:48 - loss: 0.2117 - accuracy: 0.9450\n",
      "Batch 04121: setting learning rate to 0.0001995667549974577.\n",
      " 561/1780 [========>.....................] - ETA: 5:49 - loss: 0.2115 - accuracy: 0.9451\n",
      "Batch 04122: setting learning rate to 0.0001995660917552474.\n",
      " 562/1780 [========>.....................] - ETA: 5:48 - loss: 0.2115 - accuracy: 0.9451\n",
      "Batch 04123: setting learning rate to 0.00019956542800686003.\n",
      " 563/1780 [========>.....................] - ETA: 5:48 - loss: 0.2115 - accuracy: 0.9451\n",
      "Batch 04124: setting learning rate to 0.0001995647637522989.\n",
      " 564/1780 [========>.....................] - ETA: 5:48 - loss: 0.2115 - accuracy: 0.9451\n",
      "Batch 04125: setting learning rate to 0.0001995640989915674.\n",
      " 565/1780 [========>.....................] - ETA: 5:47 - loss: 0.2116 - accuracy: 0.9451\n",
      "Batch 04126: setting learning rate to 0.0001995634337246689.\n",
      " 566/1780 [========>.....................] - ETA: 5:46 - loss: 0.2115 - accuracy: 0.9452\n",
      "Batch 04127: setting learning rate to 0.00019956276795160682.\n",
      " 567/1780 [========>.....................] - ETA: 5:46 - loss: 0.2114 - accuracy: 0.9452\n",
      "Batch 04128: setting learning rate to 0.0001995621016723845.\n",
      " 568/1780 [========>.....................] - ETA: 5:45 - loss: 0.2114 - accuracy: 0.9451\n",
      "Batch 04129: setting learning rate to 0.00019956143488700538.\n",
      " 569/1780 [========>.....................] - ETA: 5:44 - loss: 0.2112 - accuracy: 0.9451\n",
      "Batch 04130: setting learning rate to 0.0001995607675954728.\n",
      " 570/1780 [========>.....................] - ETA: 5:44 - loss: 0.2110 - accuracy: 0.9452\n",
      "Batch 04131: setting learning rate to 0.00019956009979779017.\n",
      " 571/1780 [========>.....................] - ETA: 5:44 - loss: 0.2110 - accuracy: 0.9452\n",
      "Batch 04132: setting learning rate to 0.00019955943149396093.\n",
      " 572/1780 [========>.....................] - ETA: 5:44 - loss: 0.2111 - accuracy: 0.9452\n",
      "Batch 04133: setting learning rate to 0.00019955876268398838.\n",
      " 573/1780 [========>.....................] - ETA: 5:45 - loss: 0.2111 - accuracy: 0.9452\n",
      "Batch 04134: setting learning rate to 0.000199558093367876.\n",
      " 574/1780 [========>.....................] - ETA: 5:44 - loss: 0.2113 - accuracy: 0.9451\n",
      "Batch 04135: setting learning rate to 0.00019955742354562717.\n",
      " 575/1780 [========>.....................] - ETA: 5:45 - loss: 0.2116 - accuracy: 0.9451\n",
      "Batch 04136: setting learning rate to 0.00019955675321724526.\n",
      " 576/1780 [========>.....................] - ETA: 5:44 - loss: 0.2114 - accuracy: 0.9452\n",
      "Batch 04137: setting learning rate to 0.00019955608238273376.\n",
      " 577/1780 [========>.....................] - ETA: 5:44 - loss: 0.2113 - accuracy: 0.9452\n",
      "Batch 04138: setting learning rate to 0.00019955541104209597.\n",
      " 578/1780 [========>.....................] - ETA: 5:44 - loss: 0.2115 - accuracy: 0.9451\n",
      "Batch 04139: setting learning rate to 0.00019955473919533543.\n",
      " 579/1780 [========>.....................] - ETA: 5:43 - loss: 0.2117 - accuracy: 0.9449\n",
      "Batch 04140: setting learning rate to 0.00019955406684245546.\n",
      " 580/1780 [========>.....................] - ETA: 5:42 - loss: 0.2118 - accuracy: 0.9449\n",
      "Batch 04141: setting learning rate to 0.0001995533939834595.\n",
      " 581/1780 [========>.....................] - ETA: 5:42 - loss: 0.2118 - accuracy: 0.9448\n",
      "Batch 04142: setting learning rate to 0.000199552720618351.\n",
      " 582/1780 [========>.....................] - ETA: 5:41 - loss: 0.2117 - accuracy: 0.9449\n",
      "Batch 04143: setting learning rate to 0.00019955204674713332.\n",
      " 583/1780 [========>.....................] - ETA: 5:41 - loss: 0.2118 - accuracy: 0.9449\n",
      "Batch 04144: setting learning rate to 0.00019955137236980997.\n",
      " 584/1780 [========>.....................] - ETA: 5:40 - loss: 0.2120 - accuracy: 0.9449\n",
      "Batch 04145: setting learning rate to 0.00019955069748638432.\n",
      " 585/1780 [========>.....................] - ETA: 5:39 - loss: 0.2120 - accuracy: 0.9448\n",
      "Batch 04146: setting learning rate to 0.00019955002209685982.\n",
      " 586/1780 [========>.....................] - ETA: 5:39 - loss: 0.2118 - accuracy: 0.9449\n",
      "Batch 04147: setting learning rate to 0.0001995493462012399.\n",
      " 587/1780 [========>.....................] - ETA: 5:41 - loss: 0.2118 - accuracy: 0.9449\n",
      "Batch 04148: setting learning rate to 0.000199548669799528.\n",
      " 588/1780 [========>.....................] - ETA: 5:40 - loss: 0.2117 - accuracy: 0.9449\n",
      "Batch 04149: setting learning rate to 0.00019954799289172755.\n",
      " 589/1780 [========>.....................] - ETA: 5:39 - loss: 0.2117 - accuracy: 0.9449\n",
      "Batch 04150: setting learning rate to 0.000199547315477842.\n",
      " 590/1780 [========>.....................] - ETA: 5:39 - loss: 0.2117 - accuracy: 0.9449\n",
      "Batch 04151: setting learning rate to 0.00019954663755787478.\n",
      " 591/1780 [========>.....................] - ETA: 5:41 - loss: 0.2116 - accuracy: 0.9449\n",
      "Batch 04152: setting learning rate to 0.00019954595913182935.\n",
      " 592/1780 [========>.....................] - ETA: 5:40 - loss: 0.2114 - accuracy: 0.9449\n",
      "Batch 04153: setting learning rate to 0.00019954528019970917.\n",
      " 593/1780 [========>.....................] - ETA: 5:40 - loss: 0.2113 - accuracy: 0.9450\n",
      "Batch 04154: setting learning rate to 0.00019954460076151764.\n",
      " 594/1780 [=========>....................] - ETA: 5:39 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04155: setting learning rate to 0.0001995439208172583.\n",
      " 595/1780 [=========>....................] - ETA: 5:39 - loss: 0.2110 - accuracy: 0.9450\n",
      "Batch 04156: setting learning rate to 0.00019954324036693452.\n",
      " 596/1780 [=========>....................] - ETA: 5:38 - loss: 0.2109 - accuracy: 0.9450\n",
      "Batch 04157: setting learning rate to 0.00019954255941054982.\n",
      " 597/1780 [=========>....................] - ETA: 5:37 - loss: 0.2108 - accuracy: 0.9451\n",
      "Batch 04158: setting learning rate to 0.00019954187794810763.\n",
      " 598/1780 [=========>....................] - ETA: 5:37 - loss: 0.2110 - accuracy: 0.9451\n",
      "Batch 04159: setting learning rate to 0.00019954119597961144.\n",
      " 599/1780 [=========>....................] - ETA: 5:36 - loss: 0.2109 - accuracy: 0.9451\n",
      "Batch 04160: setting learning rate to 0.00019954051350506467.\n",
      " 600/1780 [=========>....................] - ETA: 5:35 - loss: 0.2112 - accuracy: 0.9451\n",
      "Batch 04161: setting learning rate to 0.00019953983052447085.\n",
      " 601/1780 [=========>....................] - ETA: 5:35 - loss: 0.2110 - accuracy: 0.9451\n",
      "Batch 04162: setting learning rate to 0.0001995391470378334.\n",
      " 602/1780 [=========>....................] - ETA: 5:36 - loss: 0.2111 - accuracy: 0.9451\n",
      "Batch 04163: setting learning rate to 0.00019953846304515582.\n",
      " 603/1780 [=========>....................] - ETA: 5:36 - loss: 0.2111 - accuracy: 0.9451\n",
      "Batch 04164: setting learning rate to 0.00019953777854644163.\n",
      " 604/1780 [=========>....................] - ETA: 5:35 - loss: 0.2111 - accuracy: 0.9451\n",
      "Batch 04165: setting learning rate to 0.0001995370935416942.\n",
      " 605/1780 [=========>....................] - ETA: 5:36 - loss: 0.2112 - accuracy: 0.9451\n",
      "Batch 04166: setting learning rate to 0.0001995364080309171.\n",
      " 606/1780 [=========>....................] - ETA: 5:35 - loss: 0.2112 - accuracy: 0.9450\n",
      "Batch 04167: setting learning rate to 0.0001995357220141138.\n",
      " 607/1780 [=========>....................] - ETA: 5:35 - loss: 0.2111 - accuracy: 0.9451\n",
      "Batch 04168: setting learning rate to 0.00019953503549128779.\n",
      " 608/1780 [=========>....................] - ETA: 5:35 - loss: 0.2108 - accuracy: 0.9451\n",
      "Batch 04169: setting learning rate to 0.0001995343484624425.\n",
      " 609/1780 [=========>....................] - ETA: 5:35 - loss: 0.2107 - accuracy: 0.9451\n",
      "Batch 04170: setting learning rate to 0.00019953366092758152.\n",
      " 610/1780 [=========>....................] - ETA: 5:34 - loss: 0.2109 - accuracy: 0.9451\n",
      "Batch 04171: setting learning rate to 0.0001995329728867083.\n",
      " 611/1780 [=========>....................] - ETA: 5:33 - loss: 0.2107 - accuracy: 0.9452\n",
      "Batch 04172: setting learning rate to 0.0001995322843398263.\n",
      " 612/1780 [=========>....................] - ETA: 5:33 - loss: 0.2107 - accuracy: 0.9452\n",
      "Batch 04173: setting learning rate to 0.0001995315952869391.\n",
      " 613/1780 [=========>....................] - ETA: 5:33 - loss: 0.2106 - accuracy: 0.9452\n",
      "Batch 04174: setting learning rate to 0.00019953090572805013.\n",
      " 614/1780 [=========>....................] - ETA: 5:34 - loss: 0.2107 - accuracy: 0.9452\n",
      "Batch 04175: setting learning rate to 0.00019953021566316293.\n",
      " 615/1780 [=========>....................] - ETA: 5:33 - loss: 0.2108 - accuracy: 0.9451\n",
      "Batch 04176: setting learning rate to 0.000199529525092281.\n",
      " 616/1780 [=========>....................] - ETA: 5:32 - loss: 0.2108 - accuracy: 0.9451\n",
      "Batch 04177: setting learning rate to 0.00019952883401540787.\n",
      " 617/1780 [=========>....................] - ETA: 5:32 - loss: 0.2107 - accuracy: 0.9451\n",
      "Batch 04178: setting learning rate to 0.00019952814243254702.\n",
      " 618/1780 [=========>....................] - ETA: 5:32 - loss: 0.2106 - accuracy: 0.9451\n",
      "Batch 04179: setting learning rate to 0.000199527450343702.\n",
      " 619/1780 [=========>....................] - ETA: 5:32 - loss: 0.2106 - accuracy: 0.9451\n",
      "Batch 04180: setting learning rate to 0.0001995267577488763.\n",
      " 620/1780 [=========>....................] - ETA: 5:31 - loss: 0.2105 - accuracy: 0.9450\n",
      "Batch 04181: setting learning rate to 0.00019952606464807343.\n",
      " 621/1780 [=========>....................] - ETA: 5:30 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04182: setting learning rate to 0.00019952537104129698.\n",
      " 622/1780 [=========>....................] - ETA: 5:31 - loss: 0.2110 - accuracy: 0.9448\n",
      "Batch 04183: setting learning rate to 0.0001995246769285504.\n",
      " 623/1780 [=========>....................] - ETA: 5:30 - loss: 0.2110 - accuracy: 0.9448\n",
      "Batch 04184: setting learning rate to 0.00019952398230983727.\n",
      " 624/1780 [=========>....................] - ETA: 5:30 - loss: 0.2110 - accuracy: 0.9448\n",
      "Batch 04185: setting learning rate to 0.00019952328718516108.\n",
      " 625/1780 [=========>....................] - ETA: 5:30 - loss: 0.2108 - accuracy: 0.9448\n",
      "Batch 04186: setting learning rate to 0.0001995225915545254.\n",
      " 626/1780 [=========>....................] - ETA: 5:29 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04187: setting learning rate to 0.00019952189541793376.\n",
      " 627/1780 [=========>....................] - ETA: 5:29 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04188: setting learning rate to 0.00019952119877538968.\n",
      " 628/1780 [=========>....................] - ETA: 5:28 - loss: 0.2106 - accuracy: 0.9450\n",
      "Batch 04189: setting learning rate to 0.00019952050162689672.\n",
      " 629/1780 [=========>....................] - ETA: 5:28 - loss: 0.2108 - accuracy: 0.9450\n",
      "Batch 04190: setting learning rate to 0.0001995198039724584.\n",
      " 630/1780 [=========>....................] - ETA: 5:28 - loss: 0.2106 - accuracy: 0.9450\n",
      "Batch 04191: setting learning rate to 0.0001995191058120783.\n",
      " 631/1780 [=========>....................] - ETA: 5:27 - loss: 0.2109 - accuracy: 0.9450\n",
      "Batch 04192: setting learning rate to 0.00019951840714575996.\n",
      " 632/1780 [=========>....................] - ETA: 5:27 - loss: 0.2110 - accuracy: 0.9450\n",
      "Batch 04193: setting learning rate to 0.0001995177079735069.\n",
      " 633/1780 [=========>....................] - ETA: 5:27 - loss: 0.2109 - accuracy: 0.9450\n",
      "Batch 04194: setting learning rate to 0.00019951700829532273.\n",
      " 634/1780 [=========>....................] - ETA: 5:28 - loss: 0.2110 - accuracy: 0.9449\n",
      "Batch 04195: setting learning rate to 0.00019951630811121095.\n",
      " 635/1780 [=========>....................] - ETA: 5:27 - loss: 0.2110 - accuracy: 0.9449\n",
      "Batch 04196: setting learning rate to 0.00019951560742117517.\n",
      " 636/1780 [=========>....................] - ETA: 5:26 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04197: setting learning rate to 0.00019951490622521894.\n",
      " 637/1780 [=========>....................] - ETA: 5:26 - loss: 0.2108 - accuracy: 0.9449\n",
      "Batch 04198: setting learning rate to 0.00019951420452334577.\n",
      " 638/1780 [=========>....................] - ETA: 5:26 - loss: 0.2107 - accuracy: 0.9450\n",
      "Batch 04199: setting learning rate to 0.0001995135023155593.\n",
      " 639/1780 [=========>....................] - ETA: 5:26 - loss: 0.2108 - accuracy: 0.9449\n",
      "Batch 04200: setting learning rate to 0.00019951279960186307.\n",
      " 640/1780 [=========>....................] - ETA: 5:26 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04201: setting learning rate to 0.00019951209638226062.\n",
      " 641/1780 [=========>....................] - ETA: 5:26 - loss: 0.2107 - accuracy: 0.9450\n",
      "Batch 04202: setting learning rate to 0.00019951139265675562.\n",
      " 642/1780 [=========>....................] - ETA: 5:25 - loss: 0.2107 - accuracy: 0.9450\n",
      "Batch 04203: setting learning rate to 0.00019951068842535152.\n",
      " 643/1780 [=========>....................] - ETA: 5:25 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04204: setting learning rate to 0.00019950998368805202.\n",
      " 644/1780 [=========>....................] - ETA: 5:24 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04205: setting learning rate to 0.00019950927844486062.\n",
      " 645/1780 [=========>....................] - ETA: 5:24 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04206: setting learning rate to 0.00019950857269578095.\n",
      " 646/1780 [=========>....................] - ETA: 5:24 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04207: setting learning rate to 0.00019950786644081655.\n",
      " 647/1780 [=========>....................] - ETA: 5:24 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04208: setting learning rate to 0.0001995071596799711.\n",
      " 648/1780 [=========>....................] - ETA: 5:23 - loss: 0.2105 - accuracy: 0.9450\n",
      "Batch 04209: setting learning rate to 0.00019950645241324807.\n",
      " 649/1780 [=========>....................] - ETA: 5:23 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04210: setting learning rate to 0.00019950574464065114.\n",
      " 650/1780 [=========>....................] - ETA: 5:23 - loss: 0.2103 - accuracy: 0.9450\n",
      "Batch 04211: setting learning rate to 0.00019950503636218393.\n",
      " 651/1780 [=========>....................] - ETA: 5:22 - loss: 0.2102 - accuracy: 0.9450\n",
      "Batch 04212: setting learning rate to 0.00019950432757784997.\n",
      " 652/1780 [=========>....................] - ETA: 5:22 - loss: 0.2102 - accuracy: 0.9450\n",
      "Batch 04213: setting learning rate to 0.0001995036182876529.\n",
      " 653/1780 [==========>...................] - ETA: 5:22 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04214: setting learning rate to 0.0001995029084915963.\n",
      " 654/1780 [==========>...................] - ETA: 5:22 - loss: 0.2103 - accuracy: 0.9450\n",
      "Batch 04215: setting learning rate to 0.0001995021981896838.\n",
      " 655/1780 [==========>...................] - ETA: 5:21 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04216: setting learning rate to 0.00019950148738191902.\n",
      " 656/1780 [==========>...................] - ETA: 5:21 - loss: 0.2102 - accuracy: 0.9451\n",
      "Batch 04217: setting learning rate to 0.0001995007760683056.\n",
      " 657/1780 [==========>...................] - ETA: 5:21 - loss: 0.2101 - accuracy: 0.9451\n",
      "Batch 04218: setting learning rate to 0.00019950006424884707.\n",
      " 658/1780 [==========>...................] - ETA: 5:21 - loss: 0.2100 - accuracy: 0.9452\n",
      "Batch 04219: setting learning rate to 0.00019949935192354712.\n",
      " 659/1780 [==========>...................] - ETA: 5:20 - loss: 0.2099 - accuracy: 0.9451\n",
      "Batch 04220: setting learning rate to 0.0001994986390924093.\n",
      " 660/1780 [==========>...................] - ETA: 5:19 - loss: 0.2098 - accuracy: 0.9452\n",
      "Batch 04221: setting learning rate to 0.00019949792575543732.\n",
      " 661/1780 [==========>...................] - ETA: 5:19 - loss: 0.2104 - accuracy: 0.9452\n",
      "Batch 04222: setting learning rate to 0.0001994972119126348.\n",
      " 662/1780 [==========>...................] - ETA: 5:19 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04223: setting learning rate to 0.0001994964975640053.\n",
      " 663/1780 [==========>...................] - ETA: 5:19 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04224: setting learning rate to 0.0001994957827095525.\n",
      " 664/1780 [==========>...................] - ETA: 5:18 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04225: setting learning rate to 0.00019949506734928002.\n",
      " 665/1780 [==========>...................] - ETA: 5:19 - loss: 0.2100 - accuracy: 0.9451\n",
      "Batch 04226: setting learning rate to 0.00019949435148319152.\n",
      " 666/1780 [==========>...................] - ETA: 5:18 - loss: 0.2102 - accuracy: 0.9451\n",
      "Batch 04227: setting learning rate to 0.0001994936351112906.\n",
      " 667/1780 [==========>...................] - ETA: 5:18 - loss: 0.2103 - accuracy: 0.9450\n",
      "Batch 04228: setting learning rate to 0.00019949291823358092.\n",
      " 668/1780 [==========>...................] - ETA: 5:18 - loss: 0.2103 - accuracy: 0.9450\n",
      "Batch 04229: setting learning rate to 0.00019949220085006616.\n",
      " 669/1780 [==========>...................] - ETA: 5:17 - loss: 0.2102 - accuracy: 0.9450\n",
      "Batch 04230: setting learning rate to 0.0001994914829607499.\n",
      " 670/1780 [==========>...................] - ETA: 5:17 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04231: setting learning rate to 0.00019949076456563582.\n",
      " 671/1780 [==========>...................] - ETA: 5:16 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04232: setting learning rate to 0.0001994900456647276.\n",
      " 672/1780 [==========>...................] - ETA: 5:17 - loss: 0.2104 - accuracy: 0.9451\n",
      "Batch 04233: setting learning rate to 0.00019948932625802885.\n",
      " 673/1780 [==========>...................] - ETA: 5:17 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04234: setting learning rate to 0.00019948860634554327.\n",
      " 674/1780 [==========>...................] - ETA: 5:16 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04235: setting learning rate to 0.00019948788592727445.\n",
      " 675/1780 [==========>...................] - ETA: 5:15 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04236: setting learning rate to 0.00019948716500322615.\n",
      " 676/1780 [==========>...................] - ETA: 5:15 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04237: setting learning rate to 0.000199486443573402.\n",
      " 677/1780 [==========>...................] - ETA: 5:14 - loss: 0.2102 - accuracy: 0.9451\n",
      "Batch 04238: setting learning rate to 0.0001994857216378056.\n",
      " 678/1780 [==========>...................] - ETA: 5:14 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04239: setting learning rate to 0.0001994849991964407.\n",
      " 679/1780 [==========>...................] - ETA: 5:14 - loss: 0.2103 - accuracy: 0.9449\n",
      "Batch 04240: setting learning rate to 0.00019948427624931093.\n",
      " 680/1780 [==========>...................] - ETA: 5:14 - loss: 0.2104 - accuracy: 0.9449\n",
      "Batch 04241: setting learning rate to 0.00019948355279642.\n",
      " 681/1780 [==========>...................] - ETA: 5:14 - loss: 0.2103 - accuracy: 0.9449\n",
      "Batch 04242: setting learning rate to 0.00019948282883777157.\n",
      " 682/1780 [==========>...................] - ETA: 5:14 - loss: 0.2104 - accuracy: 0.9448\n",
      "Batch 04243: setting learning rate to 0.00019948210437336928.\n",
      " 683/1780 [==========>...................] - ETA: 5:13 - loss: 0.2102 - accuracy: 0.9449\n",
      "Batch 04244: setting learning rate to 0.00019948137940321687.\n",
      " 684/1780 [==========>...................] - ETA: 5:13 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04245: setting learning rate to 0.00019948065392731804.\n",
      " 685/1780 [==========>...................] - ETA: 5:13 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04246: setting learning rate to 0.00019947992794567643.\n",
      " 686/1780 [==========>...................] - ETA: 5:12 - loss: 0.2109 - accuracy: 0.9448\n",
      "Batch 04247: setting learning rate to 0.00019947920145829576.\n",
      " 687/1780 [==========>...................] - ETA: 5:12 - loss: 0.2107 - accuracy: 0.9448\n",
      "Batch 04248: setting learning rate to 0.0001994784744651797.\n",
      " 688/1780 [==========>...................] - ETA: 5:12 - loss: 0.2105 - accuracy: 0.9449\n",
      "Batch 04249: setting learning rate to 0.00019947774696633193.\n",
      " 689/1780 [==========>...................] - ETA: 5:11 - loss: 0.2103 - accuracy: 0.9449\n",
      "Batch 04250: setting learning rate to 0.0001994770189617562.\n",
      " 690/1780 [==========>...................] - ETA: 5:11 - loss: 0.2101 - accuracy: 0.9450\n",
      "Batch 04251: setting learning rate to 0.00019947629045145618.\n",
      " 691/1780 [==========>...................] - ETA: 5:11 - loss: 0.2103 - accuracy: 0.9449\n",
      "Batch 04252: setting learning rate to 0.0001994755614354356.\n",
      " 692/1780 [==========>...................] - ETA: 5:10 - loss: 0.2100 - accuracy: 0.9450\n",
      "Batch 04253: setting learning rate to 0.00019947483191369808.\n",
      " 693/1780 [==========>...................] - ETA: 5:10 - loss: 0.2098 - accuracy: 0.9451\n",
      "Batch 04254: setting learning rate to 0.00019947410188624745.\n",
      " 694/1780 [==========>...................] - ETA: 5:10 - loss: 0.2096 - accuracy: 0.9452\n",
      "Batch 04255: setting learning rate to 0.00019947337135308737.\n",
      " 695/1780 [==========>...................] - ETA: 5:09 - loss: 0.2099 - accuracy: 0.9450\n",
      "Batch 04256: setting learning rate to 0.00019947264031422153.\n",
      " 696/1780 [==========>...................] - ETA: 5:09 - loss: 0.2101 - accuracy: 0.9450\n",
      "Batch 04257: setting learning rate to 0.00019947190876965366.\n",
      " 697/1780 [==========>...................] - ETA: 5:10 - loss: 0.2099 - accuracy: 0.9451\n",
      "Batch 04258: setting learning rate to 0.00019947117671938752.\n",
      " 698/1780 [==========>...................] - ETA: 5:09 - loss: 0.2098 - accuracy: 0.9451\n",
      "Batch 04259: setting learning rate to 0.00019947044416342678.\n",
      " 699/1780 [==========>...................] - ETA: 5:09 - loss: 0.2098 - accuracy: 0.9451\n",
      "Batch 04260: setting learning rate to 0.00019946971110177517.\n",
      " 700/1780 [==========>...................] - ETA: 5:09 - loss: 0.2097 - accuracy: 0.9451\n",
      "Batch 04261: setting learning rate to 0.00019946897753443644.\n",
      " 701/1780 [==========>...................] - ETA: 5:08 - loss: 0.2095 - accuracy: 0.9452\n",
      "Batch 04262: setting learning rate to 0.0001994682434614143.\n",
      " 702/1780 [==========>...................] - ETA: 5:08 - loss: 0.2097 - accuracy: 0.9452\n",
      "Batch 04263: setting learning rate to 0.0001994675088827125.\n",
      " 703/1780 [==========>...................] - ETA: 5:07 - loss: 0.2096 - accuracy: 0.9452\n",
      "Batch 04264: setting learning rate to 0.00019946677379833474.\n",
      " 704/1780 [==========>...................] - ETA: 5:07 - loss: 0.2095 - accuracy: 0.9452\n",
      "Batch 04265: setting learning rate to 0.00019946603820828477.\n",
      " 705/1780 [==========>...................] - ETA: 5:07 - loss: 0.2094 - accuracy: 0.9452\n",
      "Batch 04266: setting learning rate to 0.0001994653021125664.\n",
      " 706/1780 [==========>...................] - ETA: 5:07 - loss: 0.2092 - accuracy: 0.9453\n",
      "Batch 04267: setting learning rate to 0.00019946456551118326.\n",
      " 707/1780 [==========>...................] - ETA: 5:06 - loss: 0.2094 - accuracy: 0.9453\n",
      "Batch 04268: setting learning rate to 0.00019946382840413916.\n",
      " 708/1780 [==========>...................] - ETA: 5:06 - loss: 0.2093 - accuracy: 0.9453\n",
      "Batch 04269: setting learning rate to 0.00019946309079143783.\n",
      " 709/1780 [==========>...................] - ETA: 5:05 - loss: 0.2091 - accuracy: 0.9454\n",
      "Batch 04270: setting learning rate to 0.00019946235267308303.\n",
      " 710/1780 [==========>...................] - ETA: 5:05 - loss: 0.2091 - accuracy: 0.9453\n",
      "Batch 04271: setting learning rate to 0.00019946161404907852.\n",
      " 711/1780 [==========>...................] - ETA: 5:05 - loss: 0.2090 - accuracy: 0.9454\n",
      "Batch 04272: setting learning rate to 0.00019946087491942805.\n",
      " 712/1780 [===========>..................] - ETA: 5:05 - loss: 0.2090 - accuracy: 0.9454\n",
      "Batch 04273: setting learning rate to 0.00019946013528413534.\n",
      " 713/1780 [===========>..................] - ETA: 5:05 - loss: 0.2088 - accuracy: 0.9454\n",
      "Batch 04274: setting learning rate to 0.0001994593951432042.\n",
      " 714/1780 [===========>..................] - ETA: 5:04 - loss: 0.2089 - accuracy: 0.9454\n",
      "Batch 04275: setting learning rate to 0.00019945865449663838.\n",
      " 715/1780 [===========>..................] - ETA: 5:04 - loss: 0.2088 - accuracy: 0.9454\n",
      "Batch 04276: setting learning rate to 0.0001994579133444416.\n",
      " 716/1780 [===========>..................] - ETA: 5:03 - loss: 0.2087 - accuracy: 0.9454\n",
      "Batch 04277: setting learning rate to 0.0001994571716866177.\n",
      " 717/1780 [===========>..................] - ETA: 5:03 - loss: 0.2085 - accuracy: 0.9454\n",
      "Batch 04278: setting learning rate to 0.00019945642952317038.\n",
      " 718/1780 [===========>..................] - ETA: 5:03 - loss: 0.2085 - accuracy: 0.9454\n",
      "Batch 04279: setting learning rate to 0.00019945568685410348.\n",
      " 719/1780 [===========>..................] - ETA: 5:03 - loss: 0.2084 - accuracy: 0.9455\n",
      "Batch 04280: setting learning rate to 0.00019945494367942072.\n",
      " 720/1780 [===========>..................] - ETA: 5:02 - loss: 0.2082 - accuracy: 0.9456\n",
      "Batch 04281: setting learning rate to 0.00019945419999912594.\n",
      " 721/1780 [===========>..................] - ETA: 5:02 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04282: setting learning rate to 0.00019945345581322285.\n",
      " 722/1780 [===========>..................] - ETA: 5:02 - loss: 0.2084 - accuracy: 0.9456\n",
      "Batch 04283: setting learning rate to 0.00019945271112171528.\n",
      " 723/1780 [===========>..................] - ETA: 5:02 - loss: 0.2083 - accuracy: 0.9456\n",
      "Batch 04284: setting learning rate to 0.00019945196592460702.\n",
      " 724/1780 [===========>..................] - ETA: 5:02 - loss: 0.2083 - accuracy: 0.9455\n",
      "Batch 04285: setting learning rate to 0.00019945122022190182.\n",
      " 725/1780 [===========>..................] - ETA: 5:01 - loss: 0.2081 - accuracy: 0.9456\n",
      "Batch 04286: setting learning rate to 0.00019945047401360348.\n",
      " 726/1780 [===========>..................] - ETA: 5:00 - loss: 0.2080 - accuracy: 0.9456\n",
      "Batch 04287: setting learning rate to 0.00019944972729971582.\n",
      " 727/1780 [===========>..................] - ETA: 5:00 - loss: 0.2081 - accuracy: 0.9456\n",
      "Batch 04288: setting learning rate to 0.00019944898008024265.\n",
      " 728/1780 [===========>..................] - ETA: 5:00 - loss: 0.2082 - accuracy: 0.9456\n",
      "Batch 04289: setting learning rate to 0.0001994482323551877.\n",
      " 729/1780 [===========>..................] - ETA: 5:00 - loss: 0.2082 - accuracy: 0.9455\n",
      "Batch 04290: setting learning rate to 0.00019944748412455482.\n",
      " 730/1780 [===========>..................] - ETA: 5:00 - loss: 0.2080 - accuracy: 0.9456\n",
      "Batch 04291: setting learning rate to 0.00019944673538834783.\n",
      " 731/1780 [===========>..................] - ETA: 4:59 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04292: setting learning rate to 0.00019944598614657052.\n",
      " 732/1780 [===========>..................] - ETA: 4:59 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04293: setting learning rate to 0.00019944523639922666.\n",
      " 733/1780 [===========>..................] - ETA: 4:58 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04294: setting learning rate to 0.0001994444861463201.\n",
      " 734/1780 [===========>..................] - ETA: 4:58 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04295: setting learning rate to 0.0001994437353878546.\n",
      " 735/1780 [===========>..................] - ETA: 4:58 - loss: 0.2077 - accuracy: 0.9456\n",
      "Batch 04296: setting learning rate to 0.0001994429841238341.\n",
      " 736/1780 [===========>..................] - ETA: 4:57 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04297: setting learning rate to 0.00019944223235426233.\n",
      " 737/1780 [===========>..................] - ETA: 4:57 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04298: setting learning rate to 0.0001994414800791431.\n",
      " 738/1780 [===========>..................] - ETA: 4:58 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04299: setting learning rate to 0.00019944072729848026.\n",
      " 739/1780 [===========>..................] - ETA: 4:57 - loss: 0.2077 - accuracy: 0.9456\n",
      "Batch 04300: setting learning rate to 0.00019943997401227766.\n",
      " 740/1780 [===========>..................] - ETA: 4:57 - loss: 0.2076 - accuracy: 0.9457\n",
      "Batch 04301: setting learning rate to 0.00019943922022053908.\n",
      " 741/1780 [===========>..................] - ETA: 4:57 - loss: 0.2077 - accuracy: 0.9456\n",
      "Batch 04302: setting learning rate to 0.00019943846592326842.\n",
      " 742/1780 [===========>..................] - ETA: 4:56 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04303: setting learning rate to 0.0001994377111204694.\n",
      " 743/1780 [===========>..................] - ETA: 4:56 - loss: 0.2080 - accuracy: 0.9456\n",
      "Batch 04304: setting learning rate to 0.000199436955812146.\n",
      " 744/1780 [===========>..................] - ETA: 4:55 - loss: 0.2079 - accuracy: 0.9455\n",
      "Batch 04305: setting learning rate to 0.00019943619999830196.\n",
      " 745/1780 [===========>..................] - ETA: 4:55 - loss: 0.2079 - accuracy: 0.9455\n",
      "Batch 04306: setting learning rate to 0.00019943544367894117.\n",
      " 746/1780 [===========>..................] - ETA: 4:55 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04307: setting learning rate to 0.00019943468685406743.\n",
      " 747/1780 [===========>..................] - ETA: 4:55 - loss: 0.2079 - accuracy: 0.9455\n",
      "Batch 04308: setting learning rate to 0.00019943392952368464.\n",
      " 748/1780 [===========>..................] - ETA: 4:54 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04309: setting learning rate to 0.0001994331716877966.\n",
      " 749/1780 [===========>..................] - ETA: 4:54 - loss: 0.2084 - accuracy: 0.9455\n",
      "Batch 04310: setting learning rate to 0.00019943241334640722.\n",
      " 750/1780 [===========>..................] - ETA: 4:53 - loss: 0.2085 - accuracy: 0.9455\n",
      "Batch 04311: setting learning rate to 0.00019943165449952028.\n",
      " 751/1780 [===========>..................] - ETA: 4:53 - loss: 0.2085 - accuracy: 0.9455\n",
      "Batch 04312: setting learning rate to 0.00019943089514713974.\n",
      " 752/1780 [===========>..................] - ETA: 4:53 - loss: 0.2084 - accuracy: 0.9455\n",
      "Batch 04313: setting learning rate to 0.00019943013528926935.\n",
      " 753/1780 [===========>..................] - ETA: 4:53 - loss: 0.2084 - accuracy: 0.9456\n",
      "Batch 04314: setting learning rate to 0.00019942937492591306.\n",
      " 754/1780 [===========>..................] - ETA: 4:52 - loss: 0.2083 - accuracy: 0.9456\n",
      "Batch 04315: setting learning rate to 0.00019942861405707467.\n",
      " 755/1780 [===========>..................] - ETA: 4:53 - loss: 0.2082 - accuracy: 0.9456\n",
      "Batch 04316: setting learning rate to 0.0001994278526827581.\n",
      " 756/1780 [===========>..................] - ETA: 4:52 - loss: 0.2083 - accuracy: 0.9455\n",
      "Batch 04317: setting learning rate to 0.00019942709080296718.\n",
      " 757/1780 [===========>..................] - ETA: 4:51 - loss: 0.2082 - accuracy: 0.9455\n",
      "Batch 04318: setting learning rate to 0.00019942632841770582.\n",
      " 758/1780 [===========>..................] - ETA: 4:52 - loss: 0.2081 - accuracy: 0.9455\n",
      "Batch 04319: setting learning rate to 0.00019942556552697788.\n",
      " 759/1780 [===========>..................] - ETA: 4:51 - loss: 0.2081 - accuracy: 0.9455\n",
      "Batch 04320: setting learning rate to 0.00019942480213078721.\n",
      " 760/1780 [===========>..................] - ETA: 4:51 - loss: 0.2081 - accuracy: 0.9456\n",
      "Batch 04321: setting learning rate to 0.00019942403822913773.\n",
      " 761/1780 [===========>..................] - ETA: 4:50 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04322: setting learning rate to 0.00019942327382203333.\n",
      " 762/1780 [===========>..................] - ETA: 4:50 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04323: setting learning rate to 0.00019942250890947786.\n",
      " 763/1780 [===========>..................] - ETA: 4:49 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04324: setting learning rate to 0.00019942174349147523.\n",
      " 764/1780 [===========>..................] - ETA: 4:50 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04325: setting learning rate to 0.00019942097756802933.\n",
      " 765/1780 [===========>..................] - ETA: 4:50 - loss: 0.2081 - accuracy: 0.9457\n",
      "Batch 04326: setting learning rate to 0.00019942021113914405.\n",
      " 766/1780 [===========>..................] - ETA: 4:49 - loss: 0.2079 - accuracy: 0.9458\n",
      "Batch 04327: setting learning rate to 0.00019941944420482328.\n",
      " 767/1780 [===========>..................] - ETA: 4:48 - loss: 0.2079 - accuracy: 0.9458\n",
      "Batch 04328: setting learning rate to 0.00019941867676507094.\n",
      " 768/1780 [===========>..................] - ETA: 4:48 - loss: 0.2078 - accuracy: 0.9458\n",
      "Batch 04329: setting learning rate to 0.00019941790881989092.\n",
      " 769/1780 [===========>..................] - ETA: 4:48 - loss: 0.2080 - accuracy: 0.9457\n",
      "Batch 04330: setting learning rate to 0.0001994171403692871.\n",
      " 770/1780 [===========>..................] - ETA: 4:48 - loss: 0.2079 - accuracy: 0.9457\n",
      "Batch 04331: setting learning rate to 0.0001994163714132634.\n",
      " 771/1780 [===========>..................] - ETA: 4:48 - loss: 0.2078 - accuracy: 0.9457\n",
      "Batch 04332: setting learning rate to 0.00019941560195182377.\n",
      " 772/1780 [============>.................] - ETA: 4:48 - loss: 0.2079 - accuracy: 0.9457\n",
      "Batch 04333: setting learning rate to 0.00019941483198497207.\n",
      " 773/1780 [============>.................] - ETA: 4:47 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04334: setting learning rate to 0.00019941406151271224.\n",
      " 774/1780 [============>.................] - ETA: 4:47 - loss: 0.2079 - accuracy: 0.9456\n",
      "Batch 04335: setting learning rate to 0.00019941329053504818.\n",
      " 775/1780 [============>.................] - ETA: 4:46 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04336: setting learning rate to 0.00019941251905198384.\n",
      " 776/1780 [============>.................] - ETA: 4:46 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04337: setting learning rate to 0.0001994117470635231.\n",
      " 777/1780 [============>.................] - ETA: 4:45 - loss: 0.2076 - accuracy: 0.9457\n",
      "Batch 04338: setting learning rate to 0.00019941097456966994.\n",
      " 778/1780 [============>.................] - ETA: 4:46 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04339: setting learning rate to 0.00019941020157042823.\n",
      " 779/1780 [============>.................] - ETA: 4:45 - loss: 0.2078 - accuracy: 0.9457\n",
      "Batch 04340: setting learning rate to 0.0001994094280658019.\n",
      " 780/1780 [============>.................] - ETA: 4:45 - loss: 0.2076 - accuracy: 0.9457\n",
      "Batch 04341: setting learning rate to 0.00019940865405579497.\n",
      " 781/1780 [============>.................] - ETA: 4:45 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04342: setting learning rate to 0.00019940787954041126.\n",
      " 782/1780 [============>.................] - ETA: 4:44 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04343: setting learning rate to 0.00019940710451965478.\n",
      " 783/1780 [============>.................] - ETA: 4:44 - loss: 0.2077 - accuracy: 0.9457\n",
      "Batch 04344: setting learning rate to 0.00019940632899352948.\n",
      " 784/1780 [============>.................] - ETA: 4:44 - loss: 0.2075 - accuracy: 0.9457\n",
      "Batch 04345: setting learning rate to 0.0001994055529620392.\n",
      " 785/1780 [============>.................] - ETA: 4:44 - loss: 0.2074 - accuracy: 0.9457\n",
      "Batch 04346: setting learning rate to 0.000199404776425188.\n",
      " 786/1780 [============>.................] - ETA: 4:44 - loss: 0.2076 - accuracy: 0.9456\n",
      "Batch 04347: setting learning rate to 0.0001994039993829798.\n",
      " 787/1780 [============>.................] - ETA: 4:43 - loss: 0.2077 - accuracy: 0.9456\n",
      "Batch 04348: setting learning rate to 0.00019940322183541849.\n",
      " 788/1780 [============>.................] - ETA: 4:43 - loss: 0.2077 - accuracy: 0.9456\n",
      "Batch 04349: setting learning rate to 0.0001994024437825081.\n",
      " 789/1780 [============>.................] - ETA: 4:43 - loss: 0.2076 - accuracy: 0.9456\n",
      "Batch 04350: setting learning rate to 0.00019940166522425254.\n",
      " 790/1780 [============>.................] - ETA: 4:42 - loss: 0.2077 - accuracy: 0.9456\n",
      "Batch 04351: setting learning rate to 0.0001994008861606558.\n",
      " 791/1780 [============>.................] - ETA: 4:42 - loss: 0.2078 - accuracy: 0.9456\n",
      "Batch 04352: setting learning rate to 0.0001994001065917218.\n",
      " 792/1780 [============>.................] - ETA: 4:41 - loss: 0.2076 - accuracy: 0.9456\n",
      "Batch 04353: setting learning rate to 0.00019939932651745452.\n",
      " 793/1780 [============>.................] - ETA: 4:42 - loss: 0.2078 - accuracy: 0.9455\n",
      "Batch 04354: setting learning rate to 0.00019939854593785795.\n",
      " 794/1780 [============>.................] - ETA: 4:41 - loss: 0.2081 - accuracy: 0.9456\n",
      "Batch 04355: setting learning rate to 0.00019939776485293606.\n",
      " 795/1780 [============>.................] - ETA: 4:41 - loss: 0.2083 - accuracy: 0.9456\n",
      "Batch 04356: setting learning rate to 0.00019939698326269277.\n",
      " 796/1780 [============>.................] - ETA: 4:41 - loss: 0.2082 - accuracy: 0.9456\n",
      "Batch 04357: setting learning rate to 0.00019939620116713207.\n",
      " 797/1780 [============>.................] - ETA: 4:41 - loss: 0.2086 - accuracy: 0.9456\n",
      "Batch 04358: setting learning rate to 0.00019939541856625799.\n",
      " 798/1780 [============>.................] - ETA: 4:40 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04359: setting learning rate to 0.00019939463546007444.\n",
      " 799/1780 [============>.................] - ETA: 4:40 - loss: 0.2084 - accuracy: 0.9456\n",
      "Batch 04360: setting learning rate to 0.00019939385184858542.\n",
      " 800/1780 [============>.................] - ETA: 4:39 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04361: setting learning rate to 0.00019939306773179497.\n",
      " 801/1780 [============>.................] - ETA: 4:39 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04362: setting learning rate to 0.000199392283109707.\n",
      " 802/1780 [============>.................] - ETA: 4:39 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04363: setting learning rate to 0.00019939149798232553.\n",
      " 803/1780 [============>.................] - ETA: 4:39 - loss: 0.2086 - accuracy: 0.9456\n",
      "Batch 04364: setting learning rate to 0.00019939071234965455.\n",
      " 804/1780 [============>.................] - ETA: 4:38 - loss: 0.2087 - accuracy: 0.9455\n",
      "Batch 04365: setting learning rate to 0.00019938992621169806.\n",
      " 805/1780 [============>.................] - ETA: 4:38 - loss: 0.2089 - accuracy: 0.9455\n",
      "Batch 04366: setting learning rate to 0.00019938913956846005.\n",
      " 806/1780 [============>.................] - ETA: 4:37 - loss: 0.2090 - accuracy: 0.9455\n",
      "Batch 04367: setting learning rate to 0.0001993883524199445.\n",
      " 807/1780 [============>.................] - ETA: 4:37 - loss: 0.2088 - accuracy: 0.9455\n",
      "Batch 04368: setting learning rate to 0.00019938756476615543.\n",
      " 808/1780 [============>.................] - ETA: 4:37 - loss: 0.2088 - accuracy: 0.9455\n",
      "Batch 04369: setting learning rate to 0.00019938677660709685.\n",
      " 809/1780 [============>.................] - ETA: 4:36 - loss: 0.2089 - accuracy: 0.9454\n",
      "Batch 04370: setting learning rate to 0.00019938598794277279.\n",
      " 810/1780 [============>.................] - ETA: 4:36 - loss: 0.2090 - accuracy: 0.9455\n",
      "Batch 04371: setting learning rate to 0.00019938519877318723.\n",
      " 811/1780 [============>.................] - ETA: 4:36 - loss: 0.2088 - accuracy: 0.9455\n",
      "Batch 04372: setting learning rate to 0.00019938440909834414.\n",
      " 812/1780 [============>.................] - ETA: 4:36 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04373: setting learning rate to 0.00019938361891824765.\n",
      " 813/1780 [============>.................] - ETA: 4:36 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04374: setting learning rate to 0.00019938282823290165.\n",
      " 814/1780 [============>.................] - ETA: 4:36 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04375: setting learning rate to 0.00019938203704231023.\n",
      " 815/1780 [============>.................] - ETA: 4:35 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04376: setting learning rate to 0.0001993812453464774.\n",
      " 816/1780 [============>.................] - ETA: 4:35 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04377: setting learning rate to 0.00019938045314540716.\n",
      " 817/1780 [============>.................] - ETA: 4:35 - loss: 0.2086 - accuracy: 0.9456\n",
      "Batch 04378: setting learning rate to 0.0001993796604391036.\n",
      " 818/1780 [============>.................] - ETA: 4:34 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04379: setting learning rate to 0.00019937886722757065.\n",
      " 819/1780 [============>.................] - ETA: 4:34 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04380: setting learning rate to 0.00019937807351081248.\n",
      " 820/1780 [============>.................] - ETA: 4:34 - loss: 0.2086 - accuracy: 0.9456\n",
      "Batch 04381: setting learning rate to 0.000199377279288833.\n",
      " 821/1780 [============>.................] - ETA: 4:33 - loss: 0.2086 - accuracy: 0.9456\n",
      "Batch 04382: setting learning rate to 0.0001993764845616363.\n",
      " 822/1780 [============>.................] - ETA: 4:33 - loss: 0.2086 - accuracy: 0.9456\n",
      "Batch 04383: setting learning rate to 0.00019937568932922644.\n",
      " 823/1780 [============>.................] - ETA: 4:32 - loss: 0.2085 - accuracy: 0.9456\n",
      "Batch 04384: setting learning rate to 0.0001993748935916074.\n",
      " 824/1780 [============>.................] - ETA: 4:33 - loss: 0.2084 - accuracy: 0.9456\n",
      "Batch 04385: setting learning rate to 0.0001993740973487833.\n",
      " 825/1780 [============>.................] - ETA: 4:32 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04386: setting learning rate to 0.00019937330060075813.\n",
      " 826/1780 [============>.................] - ETA: 4:32 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04387: setting learning rate to 0.00019937250334753598.\n",
      " 827/1780 [============>.................] - ETA: 4:31 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04388: setting learning rate to 0.00019937170558912088.\n",
      " 828/1780 [============>.................] - ETA: 4:32 - loss: 0.2093 - accuracy: 0.9455\n",
      "Batch 04389: setting learning rate to 0.0001993709073255169.\n",
      " 829/1780 [============>.................] - ETA: 4:31 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04390: setting learning rate to 0.00019937010855672807.\n",
      " 830/1780 [============>.................] - ETA: 4:31 - loss: 0.2094 - accuracy: 0.9455\n",
      "Batch 04391: setting learning rate to 0.00019936930928275847.\n",
      " 831/1780 [=============>................] - ETA: 4:30 - loss: 0.2093 - accuracy: 0.9455\n",
      "Batch 04392: setting learning rate to 0.00019936850950361216.\n",
      " 832/1780 [=============>................] - ETA: 4:31 - loss: 0.2093 - accuracy: 0.9455\n",
      "Batch 04393: setting learning rate to 0.00019936770921929323.\n",
      " 833/1780 [=============>................] - ETA: 4:30 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04394: setting learning rate to 0.00019936690842980572.\n",
      " 834/1780 [=============>................] - ETA: 4:30 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04395: setting learning rate to 0.00019936610713515372.\n",
      " 835/1780 [=============>................] - ETA: 4:29 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04396: setting learning rate to 0.00019936530533534125.\n",
      " 836/1780 [=============>................] - ETA: 4:29 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04397: setting learning rate to 0.00019936450303037248.\n",
      " 837/1780 [=============>................] - ETA: 4:29 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04398: setting learning rate to 0.00019936370022025142.\n",
      " 838/1780 [=============>................] - ETA: 4:28 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04399: setting learning rate to 0.00019936289690498216.\n",
      " 839/1780 [=============>................] - ETA: 4:28 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04400: setting learning rate to 0.0001993620930845688.\n",
      " 840/1780 [=============>................] - ETA: 4:27 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04401: setting learning rate to 0.0001993612887590154.\n",
      " 841/1780 [=============>................] - ETA: 4:28 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04402: setting learning rate to 0.0001993604839283261.\n",
      " 842/1780 [=============>................] - ETA: 4:27 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04403: setting learning rate to 0.00019935967859250494.\n",
      " 843/1780 [=============>................] - ETA: 4:27 - loss: 0.2096 - accuracy: 0.9454\n",
      "Batch 04404: setting learning rate to 0.00019935887275155604.\n",
      " 844/1780 [=============>................] - ETA: 4:27 - loss: 0.2095 - accuracy: 0.9454\n",
      "Batch 04405: setting learning rate to 0.0001993580664054835.\n",
      " 845/1780 [=============>................] - ETA: 4:26 - loss: 0.2095 - accuracy: 0.9454\n",
      "Batch 04406: setting learning rate to 0.0001993572595542914.\n",
      " 846/1780 [=============>................] - ETA: 4:26 - loss: 0.2095 - accuracy: 0.9453\n",
      "Batch 04407: setting learning rate to 0.00019935645219798387.\n",
      " 847/1780 [=============>................] - ETA: 4:26 - loss: 0.2095 - accuracy: 0.9454\n",
      "Batch 04408: setting learning rate to 0.00019935564433656497.\n",
      " 848/1780 [=============>................] - ETA: 4:26 - loss: 0.2096 - accuracy: 0.9453\n",
      "Batch 04409: setting learning rate to 0.00019935483597003885.\n",
      " 849/1780 [=============>................] - ETA: 4:26 - loss: 0.2095 - accuracy: 0.9454\n",
      "Batch 04410: setting learning rate to 0.0001993540270984096.\n",
      " 850/1780 [=============>................] - ETA: 4:25 - loss: 0.2094 - accuracy: 0.9454\n",
      "Batch 04411: setting learning rate to 0.00019935321772168133.\n",
      " 851/1780 [=============>................] - ETA: 4:25 - loss: 0.2093 - accuracy: 0.9455\n",
      "Batch 04412: setting learning rate to 0.00019935240783985818.\n",
      " 852/1780 [=============>................] - ETA: 4:24 - loss: 0.2091 - accuracy: 0.9455\n",
      "Batch 04413: setting learning rate to 0.00019935159745294422.\n",
      " 853/1780 [=============>................] - ETA: 4:24 - loss: 0.2091 - accuracy: 0.9455\n",
      "Batch 04414: setting learning rate to 0.00019935078656094365.\n",
      " 854/1780 [=============>................] - ETA: 4:24 - loss: 0.2092 - accuracy: 0.9455\n",
      "Batch 04415: setting learning rate to 0.00019934997516386047.\n",
      " 855/1780 [=============>................] - ETA: 4:23 - loss: 0.2091 - accuracy: 0.9455\n",
      "Batch 04416: setting learning rate to 0.00019934916326169892.\n",
      " 856/1780 [=============>................] - ETA: 4:23 - loss: 0.2089 - accuracy: 0.9455\n",
      "Batch 04417: setting learning rate to 0.00019934835085446307.\n",
      " 857/1780 [=============>................] - ETA: 4:23 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04418: setting learning rate to 0.00019934753794215705.\n",
      " 858/1780 [=============>................] - ETA: 4:23 - loss: 0.2089 - accuracy: 0.9455\n",
      "Batch 04419: setting learning rate to 0.00019934672452478501.\n",
      " 859/1780 [=============>................] - ETA: 4:22 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04420: setting learning rate to 0.00019934591060235111.\n",
      " 860/1780 [=============>................] - ETA: 4:22 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04421: setting learning rate to 0.00019934509617485944.\n",
      " 861/1780 [=============>................] - ETA: 4:22 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04422: setting learning rate to 0.00019934428124231414.\n",
      " 862/1780 [=============>................] - ETA: 4:22 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04423: setting learning rate to 0.0001993434658047194.\n",
      " 863/1780 [=============>................] - ETA: 4:21 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04424: setting learning rate to 0.0001993426498620793.\n",
      " 864/1780 [=============>................] - ETA: 4:21 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04425: setting learning rate to 0.00019934183341439808.\n",
      " 865/1780 [=============>................] - ETA: 4:21 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04426: setting learning rate to 0.0001993410164616798.\n",
      " 866/1780 [=============>................] - ETA: 4:20 - loss: 0.2085 - accuracy: 0.9457\n",
      "Batch 04427: setting learning rate to 0.00019934019900392865.\n",
      " 867/1780 [=============>................] - ETA: 4:20 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04428: setting learning rate to 0.00019933938104114879.\n",
      " 868/1780 [=============>................] - ETA: 4:20 - loss: 0.2085 - accuracy: 0.9457\n",
      "Batch 04429: setting learning rate to 0.00019933856257334438.\n",
      " 869/1780 [=============>................] - ETA: 4:19 - loss: 0.2084 - accuracy: 0.9458\n",
      "Batch 04430: setting learning rate to 0.00019933774360051957.\n",
      " 870/1780 [=============>................] - ETA: 4:19 - loss: 0.2085 - accuracy: 0.9457\n",
      "Batch 04431: setting learning rate to 0.00019933692412267852.\n",
      " 871/1780 [=============>................] - ETA: 4:18 - loss: 0.2085 - accuracy: 0.9457\n",
      "Batch 04432: setting learning rate to 0.00019933610413982544.\n",
      " 872/1780 [=============>................] - ETA: 4:18 - loss: 0.2084 - accuracy: 0.9457\n",
      "Batch 04433: setting learning rate to 0.0001993352836519644.\n",
      " 873/1780 [=============>................] - ETA: 4:19 - loss: 0.2084 - accuracy: 0.9457\n",
      "Batch 04434: setting learning rate to 0.00019933446265909968.\n",
      " 874/1780 [=============>................] - ETA: 4:18 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04435: setting learning rate to 0.0001993336411612354.\n",
      " 875/1780 [=============>................] - ETA: 4:18 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04436: setting learning rate to 0.00019933281915837574.\n",
      " 876/1780 [=============>................] - ETA: 4:17 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04437: setting learning rate to 0.00019933199665052488.\n",
      " 877/1780 [=============>................] - ETA: 4:17 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04438: setting learning rate to 0.00019933117363768704.\n",
      " 878/1780 [=============>................] - ETA: 4:17 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04439: setting learning rate to 0.00019933035011986632.\n",
      " 879/1780 [=============>................] - ETA: 4:17 - loss: 0.2093 - accuracy: 0.9457\n",
      "Batch 04440: setting learning rate to 0.000199329526097067.\n",
      " 880/1780 [=============>................] - ETA: 4:17 - loss: 0.2092 - accuracy: 0.9457\n",
      "Batch 04441: setting learning rate to 0.0001993287015692932.\n",
      " 881/1780 [=============>................] - ETA: 4:16 - loss: 0.2090 - accuracy: 0.9458\n",
      "Batch 04442: setting learning rate to 0.0001993278765365491.\n",
      " 882/1780 [=============>................] - ETA: 4:16 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04443: setting learning rate to 0.00019932705099883898.\n",
      " 883/1780 [=============>................] - ETA: 4:15 - loss: 0.2092 - accuracy: 0.9457\n",
      "Batch 04444: setting learning rate to 0.00019932622495616697.\n",
      " 884/1780 [=============>................] - ETA: 4:15 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04445: setting learning rate to 0.0001993253984085373.\n",
      " 885/1780 [=============>................] - ETA: 4:15 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04446: setting learning rate to 0.00019932457135595416.\n",
      " 886/1780 [=============>................] - ETA: 4:14 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04447: setting learning rate to 0.00019932374379842173.\n",
      " 887/1780 [=============>................] - ETA: 4:14 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04448: setting learning rate to 0.00019932291573594425.\n",
      " 888/1780 [=============>................] - ETA: 4:13 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04449: setting learning rate to 0.00019932208716852594.\n",
      " 889/1780 [=============>................] - ETA: 4:14 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04450: setting learning rate to 0.00019932125809617098.\n",
      " 890/1780 [==============>...............] - ETA: 4:14 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04451: setting learning rate to 0.0001993204285188836.\n",
      " 891/1780 [==============>...............] - ETA: 4:13 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04452: setting learning rate to 0.000199319598436668.\n",
      " 892/1780 [==============>...............] - ETA: 4:13 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04453: setting learning rate to 0.00019931876784952843.\n",
      " 893/1780 [==============>...............] - ETA: 4:12 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04454: setting learning rate to 0.00019931793675746908.\n",
      " 894/1780 [==============>...............] - ETA: 4:12 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04455: setting learning rate to 0.00019931710516049423.\n",
      " 895/1780 [==============>...............] - ETA: 4:12 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04456: setting learning rate to 0.00019931627305860806.\n",
      " 896/1780 [==============>...............] - ETA: 4:12 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04457: setting learning rate to 0.0001993154404518148.\n",
      " 897/1780 [==============>...............] - ETA: 4:12 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04458: setting learning rate to 0.00019931460734011868.\n",
      " 898/1780 [==============>...............] - ETA: 4:11 - loss: 0.2088 - accuracy: 0.9458\n",
      "Batch 04459: setting learning rate to 0.00019931377372352395.\n",
      " 899/1780 [==============>...............] - ETA: 4:11 - loss: 0.2088 - accuracy: 0.9458\n",
      "Batch 04460: setting learning rate to 0.00019931293960203487.\n",
      " 900/1780 [==============>...............] - ETA: 4:10 - loss: 0.2087 - accuracy: 0.9458\n",
      "Batch 04461: setting learning rate to 0.00019931210497565563.\n",
      " 901/1780 [==============>...............] - ETA: 4:10 - loss: 0.2088 - accuracy: 0.9458\n",
      "Batch 04462: setting learning rate to 0.00019931126984439052.\n",
      " 902/1780 [==============>...............] - ETA: 4:10 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04463: setting learning rate to 0.00019931043420824378.\n",
      " 903/1780 [==============>...............] - ETA: 4:09 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04464: setting learning rate to 0.00019930959806721963.\n",
      " 904/1780 [==============>...............] - ETA: 4:09 - loss: 0.2088 - accuracy: 0.9458\n",
      "Batch 04465: setting learning rate to 0.0001993087614213223.\n",
      " 905/1780 [==============>...............] - ETA: 4:09 - loss: 0.2086 - accuracy: 0.9458\n",
      "Batch 04466: setting learning rate to 0.00019930792427055614.\n",
      " 906/1780 [==============>...............] - ETA: 4:09 - loss: 0.2087 - accuracy: 0.9458\n",
      "Batch 04467: setting learning rate to 0.0001993070866149253.\n",
      " 907/1780 [==============>...............] - ETA: 4:08 - loss: 0.2086 - accuracy: 0.9458\n",
      "Batch 04468: setting learning rate to 0.0001993062484544341.\n",
      " 908/1780 [==============>...............] - ETA: 4:08 - loss: 0.2085 - accuracy: 0.9458\n",
      "Batch 04469: setting learning rate to 0.0001993054097890868.\n",
      " 909/1780 [==============>...............] - ETA: 4:08 - loss: 0.2087 - accuracy: 0.9458\n",
      "Batch 04470: setting learning rate to 0.00019930457061888762.\n",
      " 910/1780 [==============>...............] - ETA: 4:08 - loss: 0.2087 - accuracy: 0.9458\n",
      "Batch 04471: setting learning rate to 0.00019930373094384084.\n",
      " 911/1780 [==============>...............] - ETA: 4:07 - loss: 0.2086 - accuracy: 0.9458\n",
      "Batch 04472: setting learning rate to 0.0001993028907639508.\n",
      " 912/1780 [==============>...............] - ETA: 4:07 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04473: setting learning rate to 0.00019930205007922166.\n",
      " 913/1780 [==============>...............] - ETA: 4:07 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04474: setting learning rate to 0.00019930120888965778.\n",
      " 914/1780 [==============>...............] - ETA: 4:07 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04475: setting learning rate to 0.0001993003671952634.\n",
      " 915/1780 [==============>...............] - ETA: 4:06 - loss: 0.2091 - accuracy: 0.9456\n",
      "Batch 04476: setting learning rate to 0.0001992995249960428.\n",
      " 916/1780 [==============>...............] - ETA: 4:06 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04477: setting learning rate to 0.00019929868229200026.\n",
      " 917/1780 [==============>...............] - ETA: 4:05 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04478: setting learning rate to 0.00019929783908314005.\n",
      " 918/1780 [==============>...............] - ETA: 4:05 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04479: setting learning rate to 0.00019929699536946652.\n",
      " 919/1780 [==============>...............] - ETA: 4:05 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04480: setting learning rate to 0.00019929615115098388.\n",
      " 920/1780 [==============>...............] - ETA: 4:04 - loss: 0.2086 - accuracy: 0.9458\n",
      "Batch 04481: setting learning rate to 0.00019929530642769647.\n",
      " 921/1780 [==============>...............] - ETA: 4:04 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04482: setting learning rate to 0.0001992944611996086.\n",
      " 922/1780 [==============>...............] - ETA: 4:04 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04483: setting learning rate to 0.0001992936154667245.\n",
      " 923/1780 [==============>...............] - ETA: 4:04 - loss: 0.2088 - accuracy: 0.9458\n",
      "Batch 04484: setting learning rate to 0.00019929276922904852.\n",
      " 924/1780 [==============>...............] - ETA: 4:03 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04485: setting learning rate to 0.00019929192248658494.\n",
      " 925/1780 [==============>...............] - ETA: 4:04 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04486: setting learning rate to 0.00019929107523933808.\n",
      " 926/1780 [==============>...............] - ETA: 4:03 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04487: setting learning rate to 0.00019929022748731224.\n",
      " 927/1780 [==============>...............] - ETA: 4:03 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04488: setting learning rate to 0.00019928937923051178.\n",
      " 928/1780 [==============>...............] - ETA: 4:02 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04489: setting learning rate to 0.0001992885304689409.\n",
      " 929/1780 [==============>...............] - ETA: 4:02 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04490: setting learning rate to 0.000199287681202604.\n",
      " 930/1780 [==============>...............] - ETA: 4:02 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04491: setting learning rate to 0.00019928683143150537.\n",
      " 931/1780 [==============>...............] - ETA: 4:02 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04492: setting learning rate to 0.00019928598115564935.\n",
      " 932/1780 [==============>...............] - ETA: 4:01 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04493: setting learning rate to 0.00019928513037504027.\n",
      " 933/1780 [==============>...............] - ETA: 4:01 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04494: setting learning rate to 0.0001992842790896824.\n",
      " 934/1780 [==============>...............] - ETA: 4:00 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04495: setting learning rate to 0.0001992834272995801.\n",
      " 935/1780 [==============>...............] - ETA: 4:00 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04496: setting learning rate to 0.00019928257500473773.\n",
      " 936/1780 [==============>...............] - ETA: 4:00 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04497: setting learning rate to 0.0001992817222051596.\n",
      " 937/1780 [==============>...............] - ETA: 4:00 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04498: setting learning rate to 0.00019928086890085.\n",
      " 938/1780 [==============>...............] - ETA: 4:00 - loss: 0.2090 - accuracy: 0.9456\n",
      "Batch 04499: setting learning rate to 0.00019928001509181333.\n",
      " 939/1780 [==============>...............] - ETA: 3:59 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04500: setting learning rate to 0.00019927916077805394.\n",
      " 940/1780 [==============>...............] - ETA: 3:59 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04501: setting learning rate to 0.00019927830595957611.\n",
      " 941/1780 [==============>...............] - ETA: 3:59 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04502: setting learning rate to 0.0001992774506363842.\n",
      " 942/1780 [==============>...............] - ETA: 3:58 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04503: setting learning rate to 0.00019927659480848263.\n",
      " 943/1780 [==============>...............] - ETA: 3:58 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04504: setting learning rate to 0.00019927573847587567.\n",
      " 944/1780 [==============>...............] - ETA: 3:58 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04505: setting learning rate to 0.00019927488163856772.\n",
      " 945/1780 [==============>...............] - ETA: 3:57 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04506: setting learning rate to 0.00019927402429656307.\n",
      " 946/1780 [==============>...............] - ETA: 3:57 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04507: setting learning rate to 0.00019927316644986615.\n",
      " 947/1780 [==============>...............] - ETA: 3:57 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04508: setting learning rate to 0.0001992723080984813.\n",
      " 948/1780 [==============>...............] - ETA: 3:57 - loss: 0.2090 - accuracy: 0.9456\n",
      "Batch 04509: setting learning rate to 0.0001992714492424129.\n",
      " 949/1780 [==============>...............] - ETA: 3:56 - loss: 0.2091 - accuracy: 0.9456\n",
      "Batch 04510: setting learning rate to 0.0001992705898816653.\n",
      " 950/1780 [===============>..............] - ETA: 3:56 - loss: 0.2090 - accuracy: 0.9456\n",
      "Batch 04511: setting learning rate to 0.00019926973001624284.\n",
      " 951/1780 [===============>..............] - ETA: 3:55 - loss: 0.2090 - accuracy: 0.9455\n",
      "Batch 04512: setting learning rate to 0.00019926886964614994.\n",
      " 952/1780 [===============>..............] - ETA: 3:56 - loss: 0.2089 - accuracy: 0.9455\n",
      "Batch 04513: setting learning rate to 0.00019926800877139093.\n",
      " 953/1780 [===============>..............] - ETA: 3:55 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04514: setting learning rate to 0.00019926714739197023.\n",
      " 954/1780 [===============>..............] - ETA: 3:55 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04515: setting learning rate to 0.00019926628550789218.\n",
      " 955/1780 [===============>..............] - ETA: 3:55 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04516: setting learning rate to 0.0001992654231191612.\n",
      " 956/1780 [===============>..............] - ETA: 3:55 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04517: setting learning rate to 0.0001992645602257816.\n",
      " 957/1780 [===============>..............] - ETA: 3:54 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04518: setting learning rate to 0.00019926369682775788.\n",
      " 958/1780 [===============>..............] - ETA: 3:54 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04519: setting learning rate to 0.00019926283292509437.\n",
      " 959/1780 [===============>..............] - ETA: 3:53 - loss: 0.2094 - accuracy: 0.9455\n",
      "Batch 04520: setting learning rate to 0.00019926196851779542.\n",
      " 960/1780 [===============>..............] - ETA: 3:53 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04521: setting learning rate to 0.0001992611036058655.\n",
      " 961/1780 [===============>..............] - ETA: 3:53 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04522: setting learning rate to 0.00019926023818930895.\n",
      " 962/1780 [===============>..............] - ETA: 3:52 - loss: 0.2096 - accuracy: 0.9454\n",
      "Batch 04523: setting learning rate to 0.0001992593722681302.\n",
      " 963/1780 [===============>..............] - ETA: 3:52 - loss: 0.2096 - accuracy: 0.9454\n",
      "Batch 04524: setting learning rate to 0.00019925850584233366.\n",
      " 964/1780 [===============>..............] - ETA: 3:52 - loss: 0.2098 - accuracy: 0.9455\n",
      "Batch 04525: setting learning rate to 0.00019925763891192369.\n",
      " 965/1780 [===============>..............] - ETA: 3:52 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04526: setting learning rate to 0.00019925677147690474.\n",
      " 966/1780 [===============>..............] - ETA: 3:52 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04527: setting learning rate to 0.0001992559035372812.\n",
      " 967/1780 [===============>..............] - ETA: 3:51 - loss: 0.2096 - accuracy: 0.9454\n",
      "Batch 04528: setting learning rate to 0.0001992550350930575.\n",
      " 968/1780 [===============>..............] - ETA: 3:51 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04529: setting learning rate to 0.00019925416614423804.\n",
      " 969/1780 [===============>..............] - ETA: 3:50 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04530: setting learning rate to 0.00019925329669082725.\n",
      " 970/1780 [===============>..............] - ETA: 3:50 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04531: setting learning rate to 0.00019925242673282954.\n",
      " 971/1780 [===============>..............] - ETA: 3:50 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04532: setting learning rate to 0.00019925155627024933.\n",
      " 972/1780 [===============>..............] - ETA: 3:50 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04533: setting learning rate to 0.00019925068530309107.\n",
      " 973/1780 [===============>..............] - ETA: 3:50 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04534: setting learning rate to 0.00019924981383135913.\n",
      " 974/1780 [===============>..............] - ETA: 3:49 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04535: setting learning rate to 0.000199248941855058.\n",
      " 975/1780 [===============>..............] - ETA: 3:49 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04536: setting learning rate to 0.0001992480693741921.\n",
      " 976/1780 [===============>..............] - ETA: 3:49 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04537: setting learning rate to 0.00019924719638876584.\n",
      " 977/1780 [===============>..............] - ETA: 3:48 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04538: setting learning rate to 0.00019924632289878368.\n",
      " 978/1780 [===============>..............] - ETA: 3:48 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04539: setting learning rate to 0.00019924544890425003.\n",
      " 979/1780 [===============>..............] - ETA: 3:48 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04540: setting learning rate to 0.0001992445744051694.\n",
      " 980/1780 [===============>..............] - ETA: 3:47 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04541: setting learning rate to 0.00019924369940154616.\n",
      " 981/1780 [===============>..............] - ETA: 3:47 - loss: 0.2096 - accuracy: 0.9454\n",
      "Batch 04542: setting learning rate to 0.00019924282389338483.\n",
      " 982/1780 [===============>..............] - ETA: 3:47 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04543: setting learning rate to 0.0001992419478806898.\n",
      " 983/1780 [===============>..............] - ETA: 3:47 - loss: 0.2094 - accuracy: 0.9455\n",
      "Batch 04544: setting learning rate to 0.00019924107136346555.\n",
      " 984/1780 [===============>..............] - ETA: 3:46 - loss: 0.2093 - accuracy: 0.9455\n",
      "Batch 04545: setting learning rate to 0.00019924019434171654.\n",
      " 985/1780 [===============>..............] - ETA: 3:46 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04546: setting learning rate to 0.0001992393168154472.\n",
      " 986/1780 [===============>..............] - ETA: 3:46 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04547: setting learning rate to 0.00019923843878466205.\n",
      " 987/1780 [===============>..............] - ETA: 3:46 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04548: setting learning rate to 0.0001992375602493655.\n",
      " 988/1780 [===============>..............] - ETA: 3:45 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04549: setting learning rate to 0.00019923668120956203.\n",
      " 989/1780 [===============>..............] - ETA: 3:45 - loss: 0.2096 - accuracy: 0.9456\n",
      "Batch 04550: setting learning rate to 0.0001992358016652561.\n",
      " 990/1780 [===============>..............] - ETA: 3:45 - loss: 0.2095 - accuracy: 0.9456\n",
      "Batch 04551: setting learning rate to 0.00019923492161645222.\n",
      " 991/1780 [===============>..............] - ETA: 3:44 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04552: setting learning rate to 0.00019923404106315484.\n",
      " 992/1780 [===============>..............] - ETA: 3:45 - loss: 0.2095 - accuracy: 0.9456\n",
      "Batch 04553: setting learning rate to 0.00019923316000536843.\n",
      " 993/1780 [===============>..............] - ETA: 3:44 - loss: 0.2093 - accuracy: 0.9456\n",
      "Batch 04554: setting learning rate to 0.00019923227844309744.\n",
      " 994/1780 [===============>..............] - ETA: 3:44 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04555: setting learning rate to 0.0001992313963763464.\n",
      " 995/1780 [===============>..............] - ETA: 3:44 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04556: setting learning rate to 0.00019923051380511982.\n",
      " 996/1780 [===============>..............] - ETA: 3:43 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04557: setting learning rate to 0.0001992296307294221.\n",
      " 997/1780 [===============>..............] - ETA: 3:43 - loss: 0.2093 - accuracy: 0.9456\n",
      "Batch 04558: setting learning rate to 0.0001992287471492578.\n",
      " 998/1780 [===============>..............] - ETA: 3:42 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04559: setting learning rate to 0.00019922786306463138.\n",
      " 999/1780 [===============>..............] - ETA: 3:42 - loss: 0.2092 - accuracy: 0.9457\n",
      "Batch 04560: setting learning rate to 0.00019922697847554732.\n",
      "1000/1780 [===============>..............] - ETA: 3:42 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04561: setting learning rate to 0.00019922609338201018.\n",
      "1001/1780 [===============>..............] - ETA: 3:41 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04562: setting learning rate to 0.0001992252077840244.\n",
      "1002/1780 [===============>..............] - ETA: 3:41 - loss: 0.2090 - accuracy: 0.9458\n",
      "Batch 04563: setting learning rate to 0.0001992243216815945.\n",
      "1003/1780 [===============>..............] - ETA: 3:41 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04564: setting learning rate to 0.00019922343507472497.\n",
      "1004/1780 [===============>..............] - ETA: 3:41 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04565: setting learning rate to 0.00019922254796342034.\n",
      "1005/1780 [===============>..............] - ETA: 3:40 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04566: setting learning rate to 0.0001992216603476851.\n",
      "1006/1780 [===============>..............] - ETA: 3:40 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04567: setting learning rate to 0.00019922077222752382.\n",
      "1007/1780 [===============>..............] - ETA: 3:40 - loss: 0.2091 - accuracy: 0.9457\n",
      "Batch 04568: setting learning rate to 0.00019921988360294094.\n",
      "1008/1780 [===============>..............] - ETA: 3:39 - loss: 0.2090 - accuracy: 0.9457\n",
      "Batch 04569: setting learning rate to 0.00019921899447394102.\n",
      "1009/1780 [================>.............] - ETA: 3:39 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04570: setting learning rate to 0.0001992181048405286.\n",
      "1010/1780 [================>.............] - ETA: 3:39 - loss: 0.2089 - accuracy: 0.9458\n",
      "Batch 04571: setting learning rate to 0.00019921721470270813.\n",
      "1011/1780 [================>.............] - ETA: 3:39 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04572: setting learning rate to 0.00019921632406048417.\n",
      "1012/1780 [================>.............] - ETA: 3:39 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04573: setting learning rate to 0.00019921543291386126.\n",
      "1013/1780 [================>.............] - ETA: 3:38 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04574: setting learning rate to 0.00019921454126284395.\n",
      "1014/1780 [================>.............] - ETA: 3:38 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04575: setting learning rate to 0.00019921364910743675.\n",
      "1015/1780 [================>.............] - ETA: 3:38 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04576: setting learning rate to 0.00019921275644764418.\n",
      "1016/1780 [================>.............] - ETA: 3:37 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04577: setting learning rate to 0.00019921186328347075.\n",
      "1017/1780 [================>.............] - ETA: 3:37 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04578: setting learning rate to 0.0001992109696149211.\n",
      "1018/1780 [================>.............] - ETA: 3:36 - loss: 0.2086 - accuracy: 0.9458\n",
      "Batch 04579: setting learning rate to 0.0001992100754419997.\n",
      "1019/1780 [================>.............] - ETA: 3:36 - loss: 0.2087 - accuracy: 0.9458\n",
      "Batch 04580: setting learning rate to 0.00019920918076471108.\n",
      "1020/1780 [================>.............] - ETA: 3:36 - loss: 0.2086 - accuracy: 0.9458\n",
      "Batch 04581: setting learning rate to 0.00019920828558305987.\n",
      "1021/1780 [================>.............] - ETA: 3:36 - loss: 0.2085 - accuracy: 0.9458\n",
      "Batch 04582: setting learning rate to 0.0001992073898970505.\n",
      "1022/1780 [================>.............] - ETA: 3:36 - loss: 0.2084 - accuracy: 0.9458\n",
      "Batch 04583: setting learning rate to 0.00019920649370668768.\n",
      "1023/1780 [================>.............] - ETA: 3:35 - loss: 0.2084 - accuracy: 0.9458\n",
      "Batch 04584: setting learning rate to 0.00019920559701197584.\n",
      "1024/1780 [================>.............] - ETA: 3:35 - loss: 0.2086 - accuracy: 0.9457\n",
      "Batch 04585: setting learning rate to 0.00019920469981291958.\n",
      "1025/1780 [================>.............] - ETA: 3:35 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04586: setting learning rate to 0.00019920380210952345.\n",
      "1026/1780 [================>.............] - ETA: 3:34 - loss: 0.2087 - accuracy: 0.9456\n",
      "Batch 04587: setting learning rate to 0.00019920290390179206.\n",
      "1027/1780 [================>.............] - ETA: 3:35 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04588: setting learning rate to 0.0001992020051897299.\n",
      "1028/1780 [================>.............] - ETA: 3:34 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04589: setting learning rate to 0.00019920110597334157.\n",
      "1029/1780 [================>.............] - ETA: 3:34 - loss: 0.2088 - accuracy: 0.9456\n",
      "Batch 04590: setting learning rate to 0.0001992002062526317.\n",
      "1030/1780 [================>.............] - ETA: 3:33 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04591: setting learning rate to 0.0001991993060276048.\n",
      "1031/1780 [================>.............] - ETA: 3:33 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04592: setting learning rate to 0.00019919840529826542.\n",
      "1032/1780 [================>.............] - ETA: 3:33 - loss: 0.2089 - accuracy: 0.9457\n",
      "Batch 04593: setting learning rate to 0.00019919750406461822.\n",
      "1033/1780 [================>.............] - ETA: 3:32 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04594: setting learning rate to 0.00019919660232666774.\n",
      "1034/1780 [================>.............] - ETA: 3:32 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04595: setting learning rate to 0.00019919570008441857.\n",
      "1035/1780 [================>.............] - ETA: 3:32 - loss: 0.2088 - accuracy: 0.9457\n",
      "Batch 04596: setting learning rate to 0.00019919479733787528.\n",
      "1036/1780 [================>.............] - ETA: 3:31 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04597: setting learning rate to 0.0001991938940870425.\n",
      "1037/1780 [================>.............] - ETA: 3:32 - loss: 0.2087 - accuracy: 0.9457\n",
      "Batch 04598: setting learning rate to 0.00019919299033192476.\n",
      "1038/1780 [================>.............] - ETA: 3:31 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04599: setting learning rate to 0.0001991920860725267.\n",
      "1039/1780 [================>.............] - ETA: 3:31 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04600: setting learning rate to 0.0001991911813088529.\n",
      "1040/1780 [================>.............] - ETA: 3:31 - loss: 0.2091 - accuracy: 0.9456\n",
      "Batch 04601: setting learning rate to 0.00019919027604090798.\n",
      "1041/1780 [================>.............] - ETA: 3:30 - loss: 0.2092 - accuracy: 0.9455\n",
      "Batch 04602: setting learning rate to 0.00019918937026869654.\n",
      "1042/1780 [================>.............] - ETA: 3:30 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04603: setting learning rate to 0.00019918846399222316.\n",
      "1043/1780 [================>.............] - ETA: 3:30 - loss: 0.2091 - accuracy: 0.9456\n",
      "Batch 04604: setting learning rate to 0.00019918755721149242.\n",
      "1044/1780 [================>.............] - ETA: 3:30 - loss: 0.2089 - accuracy: 0.9456\n",
      "Batch 04605: setting learning rate to 0.00019918664992650903.\n",
      "1045/1780 [================>.............] - ETA: 3:29 - loss: 0.2091 - accuracy: 0.9456\n",
      "Batch 04606: setting learning rate to 0.0001991857421372775.\n",
      "1046/1780 [================>.............] - ETA: 3:29 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04607: setting learning rate to 0.0001991848338438025.\n",
      "1047/1780 [================>.............] - ETA: 3:29 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04608: setting learning rate to 0.00019918392504608866.\n",
      "1048/1780 [================>.............] - ETA: 3:28 - loss: 0.2093 - accuracy: 0.9456\n",
      "Batch 04609: setting learning rate to 0.00019918301574414055.\n",
      "1049/1780 [================>.............] - ETA: 3:28 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04610: setting learning rate to 0.00019918210593796283.\n",
      "1050/1780 [================>.............] - ETA: 3:27 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04611: setting learning rate to 0.0001991811956275601.\n",
      "1051/1780 [================>.............] - ETA: 3:27 - loss: 0.2093 - accuracy: 0.9456\n",
      "Batch 04612: setting learning rate to 0.00019918028481293702.\n",
      "1052/1780 [================>.............] - ETA: 3:27 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04613: setting learning rate to 0.00019917937349409818.\n",
      "1053/1780 [================>.............] - ETA: 3:27 - loss: 0.2092 - accuracy: 0.9456\n",
      "Batch 04614: setting learning rate to 0.00019917846167104826.\n",
      "1054/1780 [================>.............] - ETA: 3:27 - loss: 0.2091 - accuracy: 0.9456\n",
      "Batch 04615: setting learning rate to 0.00019917754934379187.\n",
      "1055/1780 [================>.............] - ETA: 3:26 - loss: 0.2094 - accuracy: 0.9456\n",
      "Batch 04616: setting learning rate to 0.00019917663651233364.\n",
      "1056/1780 [================>.............] - ETA: 3:26 - loss: 0.2093 - accuracy: 0.9456\n",
      "Batch 04617: setting learning rate to 0.0001991757231766782.\n",
      "1057/1780 [================>.............] - ETA: 3:25 - loss: 0.2093 - accuracy: 0.9456\n",
      "Batch 04618: setting learning rate to 0.00019917480933683023.\n",
      "1058/1780 [================>.............] - ETA: 3:25 - loss: 0.2095 - accuracy: 0.9456\n",
      "Batch 04619: setting learning rate to 0.00019917389499279437.\n",
      "1059/1780 [================>.............] - ETA: 3:25 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04620: setting learning rate to 0.00019917298014457524.\n",
      "1060/1780 [================>.............] - ETA: 3:25 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04621: setting learning rate to 0.0001991720647921775.\n",
      "1061/1780 [================>.............] - ETA: 3:25 - loss: 0.2095 - accuracy: 0.9455\n",
      "Batch 04622: setting learning rate to 0.00019917114893560584.\n",
      "1062/1780 [================>.............] - ETA: 3:24 - loss: 0.2098 - accuracy: 0.9455\n",
      "Batch 04623: setting learning rate to 0.00019917023257486486.\n",
      "1063/1780 [================>.............] - ETA: 3:24 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04624: setting learning rate to 0.00019916931570995928.\n",
      "1064/1780 [================>.............] - ETA: 3:23 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04625: setting learning rate to 0.00019916839834089366.\n",
      "1065/1780 [================>.............] - ETA: 3:24 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04626: setting learning rate to 0.0001991674804676728.\n",
      "1066/1780 [================>.............] - ETA: 3:23 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04627: setting learning rate to 0.00019916656209030128.\n",
      "1067/1780 [================>.............] - ETA: 3:23 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04628: setting learning rate to 0.00019916564320878374.\n",
      "1068/1780 [=================>............] - ETA: 3:22 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04629: setting learning rate to 0.00019916472382312494.\n",
      "1069/1780 [=================>............] - ETA: 3:22 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04630: setting learning rate to 0.00019916380393332952.\n",
      "1070/1780 [=================>............] - ETA: 3:22 - loss: 0.2097 - accuracy: 0.9454\n",
      "Batch 04631: setting learning rate to 0.00019916288353940214.\n",
      "1071/1780 [=================>............] - ETA: 3:22 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04632: setting learning rate to 0.00019916196264134747.\n",
      "1072/1780 [=================>............] - ETA: 3:21 - loss: 0.2096 - accuracy: 0.9455\n",
      "Batch 04633: setting learning rate to 0.00019916104123917023.\n",
      "1073/1780 [=================>............] - ETA: 3:21 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04634: setting learning rate to 0.00019916011933287505.\n",
      "1074/1780 [=================>............] - ETA: 3:21 - loss: 0.2098 - accuracy: 0.9455\n",
      "Batch 04635: setting learning rate to 0.0001991591969224667.\n",
      "1075/1780 [=================>............] - ETA: 3:21 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04636: setting learning rate to 0.0001991582740079498.\n",
      "1076/1780 [=================>............] - ETA: 3:20 - loss: 0.2097 - accuracy: 0.9456\n",
      "Batch 04637: setting learning rate to 0.00019915735058932902.\n",
      "1077/1780 [=================>............] - ETA: 3:20 - loss: 0.2097 - accuracy: 0.9456\n",
      "Batch 04638: setting learning rate to 0.00019915642666660914.\n",
      "1078/1780 [=================>............] - ETA: 3:20 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04639: setting learning rate to 0.0001991555022397948.\n",
      "1079/1780 [=================>............] - ETA: 3:19 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04640: setting learning rate to 0.0001991545773088907.\n",
      "1080/1780 [=================>............] - ETA: 3:19 - loss: 0.2097 - accuracy: 0.9455\n",
      "Batch 04641: setting learning rate to 0.00019915365187390157.\n",
      "1081/1780 [=================>............] - ETA: 3:19 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04642: setting learning rate to 0.00019915272593483212.\n",
      "1082/1780 [=================>............] - ETA: 3:19 - loss: 0.2099 - accuracy: 0.9454\n",
      "Batch 04643: setting learning rate to 0.000199151799491687.\n",
      "1083/1780 [=================>............] - ETA: 3:18 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04644: setting learning rate to 0.000199150872544471.\n",
      "1084/1780 [=================>............] - ETA: 3:18 - loss: 0.2098 - accuracy: 0.9453\n",
      "Batch 04645: setting learning rate to 0.00019914994509318874.\n",
      "1085/1780 [=================>............] - ETA: 3:18 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04646: setting learning rate to 0.000199149017137845.\n",
      "1086/1780 [=================>............] - ETA: 3:17 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04647: setting learning rate to 0.00019914808867844448.\n",
      "1087/1780 [=================>............] - ETA: 3:17 - loss: 0.2099 - accuracy: 0.9454\n",
      "Batch 04648: setting learning rate to 0.0001991471597149919.\n",
      "1088/1780 [=================>............] - ETA: 3:17 - loss: 0.2099 - accuracy: 0.9454\n",
      "Batch 04649: setting learning rate to 0.000199146230247492.\n",
      "1089/1780 [=================>............] - ETA: 3:16 - loss: 0.2104 - accuracy: 0.9453\n",
      "Batch 04650: setting learning rate to 0.0001991453002759495.\n",
      "1090/1780 [=================>............] - ETA: 3:16 - loss: 0.2103 - accuracy: 0.9454\n",
      "Batch 04651: setting learning rate to 0.00019914436980036907.\n",
      "1091/1780 [=================>............] - ETA: 3:16 - loss: 0.2103 - accuracy: 0.9454\n",
      "Batch 04652: setting learning rate to 0.00019914343882075554.\n",
      "1092/1780 [=================>............] - ETA: 3:16 - loss: 0.2103 - accuracy: 0.9453\n",
      "Batch 04653: setting learning rate to 0.00019914250733711356.\n",
      "1093/1780 [=================>............] - ETA: 3:15 - loss: 0.2103 - accuracy: 0.9453\n",
      "Batch 04654: setting learning rate to 0.00019914157534944792.\n",
      "1094/1780 [=================>............] - ETA: 3:15 - loss: 0.2102 - accuracy: 0.9453\n",
      "Batch 04655: setting learning rate to 0.0001991406428577633.\n",
      "1095/1780 [=================>............] - ETA: 3:15 - loss: 0.2102 - accuracy: 0.9453\n",
      "Batch 04656: setting learning rate to 0.0001991397098620645.\n",
      "1096/1780 [=================>............] - ETA: 3:14 - loss: 0.2102 - accuracy: 0.9453\n",
      "Batch 04657: setting learning rate to 0.00019913877636235623.\n",
      "1097/1780 [=================>............] - ETA: 3:14 - loss: 0.2101 - accuracy: 0.9453\n",
      "Batch 04658: setting learning rate to 0.00019913784235864323.\n",
      "1098/1780 [=================>............] - ETA: 3:14 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04659: setting learning rate to 0.0001991369078509303.\n",
      "1099/1780 [=================>............] - ETA: 3:14 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04660: setting learning rate to 0.00019913597283922213.\n",
      "1100/1780 [=================>............] - ETA: 3:14 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04661: setting learning rate to 0.0001991350373235235.\n",
      "1101/1780 [=================>............] - ETA: 3:13 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04662: setting learning rate to 0.00019913410130383916.\n",
      "1102/1780 [=================>............] - ETA: 3:13 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04663: setting learning rate to 0.00019913316478017385.\n",
      "1103/1780 [=================>............] - ETA: 3:12 - loss: 0.2100 - accuracy: 0.9453\n",
      "Batch 04664: setting learning rate to 0.00019913222775253239.\n",
      "1104/1780 [=================>............] - ETA: 3:12 - loss: 0.2100 - accuracy: 0.9453\n",
      "Batch 04665: setting learning rate to 0.00019913129022091947.\n",
      "1105/1780 [=================>............] - ETA: 3:12 - loss: 0.2100 - accuracy: 0.9453\n",
      "Batch 04666: setting learning rate to 0.00019913035218533992.\n",
      "1106/1780 [=================>............] - ETA: 3:12 - loss: 0.2100 - accuracy: 0.9453\n",
      "Batch 04667: setting learning rate to 0.00019912941364579848.\n",
      "1107/1780 [=================>............] - ETA: 3:11 - loss: 0.2099 - accuracy: 0.9454\n",
      "Batch 04668: setting learning rate to 0.0001991284746022999.\n",
      "1108/1780 [=================>............] - ETA: 3:11 - loss: 0.2099 - accuracy: 0.9454\n",
      "Batch 04669: setting learning rate to 0.00019912753505484898.\n",
      "1109/1780 [=================>............] - ETA: 3:11 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04670: setting learning rate to 0.00019912659500345053.\n",
      "1110/1780 [=================>............] - ETA: 3:10 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04671: setting learning rate to 0.00019912565444810923.\n",
      "1111/1780 [=================>............] - ETA: 3:10 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04672: setting learning rate to 0.00019912471338882996.\n",
      "1112/1780 [=================>............] - ETA: 3:10 - loss: 0.2098 - accuracy: 0.9454\n",
      "Batch 04673: setting learning rate to 0.0001991237718256175.\n",
      "1113/1780 [=================>............] - ETA: 3:10 - loss: 0.2099 - accuracy: 0.9453\n",
      "Batch 04674: setting learning rate to 0.00019912282975847656.\n",
      "1114/1780 [=================>............] - ETA: 3:09 - loss: 0.2100 - accuracy: 0.9453\n",
      "Batch 04675: setting learning rate to 0.000199121887187412.\n",
      "1115/1780 [=================>............] - ETA: 3:09 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04676: setting learning rate to 0.00019912094411242854.\n",
      "1116/1780 [=================>............] - ETA: 3:09 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04677: setting learning rate to 0.00019912000053353107.\n",
      "1117/1780 [=================>............] - ETA: 3:08 - loss: 0.2100 - accuracy: 0.9452\n",
      "Batch 04678: setting learning rate to 0.00019911905645072432.\n",
      "1118/1780 [=================>............] - ETA: 3:08 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04679: setting learning rate to 0.0001991181118640131.\n",
      "1119/1780 [=================>............] - ETA: 3:08 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04680: setting learning rate to 0.0001991171667734022.\n",
      "1120/1780 [=================>............] - ETA: 3:08 - loss: 0.2101 - accuracy: 0.9453\n",
      "Batch 04681: setting learning rate to 0.00019911622117889648.\n",
      "1121/1780 [=================>............] - ETA: 3:07 - loss: 0.2102 - accuracy: 0.9453\n",
      "Batch 04682: setting learning rate to 0.0001991152750805007.\n",
      "1122/1780 [=================>............] - ETA: 3:07 - loss: 0.2101 - accuracy: 0.9453\n",
      "Batch 04683: setting learning rate to 0.00019911432847821966.\n",
      "1123/1780 [=================>............] - ETA: 3:07 - loss: 0.2101 - accuracy: 0.9453\n",
      "Batch 04684: setting learning rate to 0.0001991133813720582.\n",
      "1124/1780 [=================>............] - ETA: 3:06 - loss: 0.2104 - accuracy: 0.9453\n",
      "Batch 04685: setting learning rate to 0.00019911243376202116.\n",
      "1125/1780 [=================>............] - ETA: 3:06 - loss: 0.2103 - accuracy: 0.9453\n",
      "Batch 04686: setting learning rate to 0.00019911148564811328.\n",
      "1126/1780 [=================>............] - ETA: 3:06 - loss: 0.2102 - accuracy: 0.9453\n",
      "Batch 04687: setting learning rate to 0.00019911053703033945.\n",
      "1127/1780 [=================>............] - ETA: 3:06 - loss: 0.2103 - accuracy: 0.9452\n",
      "Batch 04688: setting learning rate to 0.00019910958790870446.\n",
      "1128/1780 [==================>...........] - ETA: 3:05 - loss: 0.2102 - accuracy: 0.9453\n",
      "Batch 04689: setting learning rate to 0.00019910863828321315.\n",
      "1129/1780 [==================>...........] - ETA: 3:05 - loss: 0.2101 - accuracy: 0.9452\n",
      "Batch 04690: setting learning rate to 0.00019910768815387036.\n",
      "1130/1780 [==================>...........] - ETA: 3:05 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04691: setting learning rate to 0.00019910673752068087.\n",
      "1131/1780 [==================>...........] - ETA: 3:05 - loss: 0.2102 - accuracy: 0.9451\n",
      "Batch 04692: setting learning rate to 0.00019910578638364956.\n",
      "1132/1780 [==================>...........] - ETA: 3:05 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04693: setting learning rate to 0.00019910483474278124.\n",
      "1133/1780 [==================>...........] - ETA: 3:04 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04694: setting learning rate to 0.00019910388259808076.\n",
      "1134/1780 [==================>...........] - ETA: 3:04 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04695: setting learning rate to 0.00019910292994955296.\n",
      "1135/1780 [==================>...........] - ETA: 3:03 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04696: setting learning rate to 0.00019910197679720267.\n",
      "1136/1780 [==================>...........] - ETA: 3:03 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04697: setting learning rate to 0.00019910102314103475.\n",
      "1137/1780 [==================>...........] - ETA: 3:03 - loss: 0.2103 - accuracy: 0.9451\n",
      "Batch 04698: setting learning rate to 0.00019910006898105407.\n",
      "1138/1780 [==================>...........] - ETA: 3:02 - loss: 0.2102 - accuracy: 0.9451\n",
      "Batch 04699: setting learning rate to 0.00019909911431726545.\n",
      "1139/1780 [==================>...........] - ETA: 3:02 - loss: 0.2102 - accuracy: 0.9452\n",
      "Batch 04700: setting learning rate to 0.0001990981591496737.\n",
      "1140/1780 [==================>...........] - ETA: 3:02 - loss: 0.2105 - accuracy: 0.9451\n",
      "Batch 04701: setting learning rate to 0.00019909720347828378.\n",
      "1141/1780 [==================>...........] - ETA: 3:02 - loss: 0.2105 - accuracy: 0.9451\n",
      "Batch 04702: setting learning rate to 0.00019909624730310048.\n",
      "1142/1780 [==================>...........] - ETA: 3:01 - loss: 0.2104 - accuracy: 0.9452\n",
      "Batch 04703: setting learning rate to 0.00019909529062412865.\n",
      "1143/1780 [==================>...........] - ETA: 3:01 - loss: 0.2104 - accuracy: 0.9451\n",
      "Batch 04704: setting learning rate to 0.0001990943334413732.\n",
      "1144/1780 [==================>...........] - ETA: 3:01 - loss: 0.2105 - accuracy: 0.9451\n",
      "Batch 04705: setting learning rate to 0.00019909337575483899.\n",
      "1145/1780 [==================>...........] - ETA: 3:01 - loss: 0.2105 - accuracy: 0.9450\n",
      "Batch 04706: setting learning rate to 0.00019909241756453083.\n",
      "1146/1780 [==================>...........] - ETA: 3:00 - loss: 0.2106 - accuracy: 0.9450\n",
      "Batch 04707: setting learning rate to 0.0001990914588704537.\n",
      "1147/1780 [==================>...........] - ETA: 3:00 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04708: setting learning rate to 0.00019909049967261236.\n",
      "1148/1780 [==================>...........] - ETA: 3:00 - loss: 0.2107 - accuracy: 0.9449\n",
      "Batch 04709: setting learning rate to 0.00019908953997101173.\n",
      "1149/1780 [==================>...........] - ETA: 3:00 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04710: setting learning rate to 0.0001990885797656567.\n",
      "1150/1780 [==================>...........] - ETA: 2:59 - loss: 0.2106 - accuracy: 0.9449\n",
      "Batch 04711: setting learning rate to 0.00019908761905655214.\n",
      "1151/1780 [==================>...........] - ETA: 2:59 - loss: 0.2105 - accuracy: 0.9449\n",
      "Batch 04712: setting learning rate to 0.00019908665784370294.\n",
      "1152/1780 [==================>...........] - ETA: 2:58 - loss: 0.2104 - accuracy: 0.9449\n",
      "Batch 04713: setting learning rate to 0.000199085696127114.\n",
      "1153/1780 [==================>...........] - ETA: 2:58 - loss: 0.2104 - accuracy: 0.9450\n",
      "Batch 04714: setting learning rate to 0.0001990847339067902.\n",
      "1154/1780 [==================>...........] - ETA: 2:58 - loss: 0.2106 - accuracy: 0.9450\n",
      "Batch 04715: setting learning rate to 0.0001990837711827364.\n",
      "1155/1780 [==================>...........] - ETA: 2:58 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04716: setting learning rate to 0.00019908280795495755.\n",
      "1156/1780 [==================>...........] - ETA: 2:58 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04717: setting learning rate to 0.00019908184422345852.\n",
      "1157/1780 [==================>...........] - ETA: 2:57 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04718: setting learning rate to 0.00019908087998824416.\n",
      "1158/1780 [==================>...........] - ETA: 2:57 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04719: setting learning rate to 0.00019907991524931946.\n",
      "1159/1780 [==================>...........] - ETA: 2:57 - loss: 0.2111 - accuracy: 0.9449\n",
      "Batch 04720: setting learning rate to 0.00019907895000668928.\n",
      "1160/1780 [==================>...........] - ETA: 2:57 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04721: setting learning rate to 0.0001990779842603585.\n",
      "1161/1780 [==================>...........] - ETA: 2:56 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04722: setting learning rate to 0.00019907701801033208.\n",
      "1162/1780 [==================>...........] - ETA: 2:56 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04723: setting learning rate to 0.00019907605125661494.\n",
      "1163/1780 [==================>...........] - ETA: 2:56 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04724: setting learning rate to 0.00019907508399921194.\n",
      "1164/1780 [==================>...........] - ETA: 2:55 - loss: 0.2113 - accuracy: 0.9448\n",
      "Batch 04725: setting learning rate to 0.00019907411623812805.\n",
      "1165/1780 [==================>...........] - ETA: 2:55 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04726: setting learning rate to 0.00019907314797336812.\n",
      "1166/1780 [==================>...........] - ETA: 2:55 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04727: setting learning rate to 0.00019907217920493714.\n",
      "1167/1780 [==================>...........] - ETA: 2:54 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04728: setting learning rate to 0.00019907120993284.\n",
      "1168/1780 [==================>...........] - ETA: 2:54 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04729: setting learning rate to 0.0001990702401570817.\n",
      "1169/1780 [==================>...........] - ETA: 2:54 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04730: setting learning rate to 0.00019906926987766705.\n",
      "1170/1780 [==================>...........] - ETA: 2:53 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04731: setting learning rate to 0.00019906829909460105.\n",
      "1171/1780 [==================>...........] - ETA: 2:53 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04732: setting learning rate to 0.00019906732780788865.\n",
      "1172/1780 [==================>...........] - ETA: 2:53 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04733: setting learning rate to 0.00019906635601753476.\n",
      "1173/1780 [==================>...........] - ETA: 2:53 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04734: setting learning rate to 0.00019906538372354432.\n",
      "1174/1780 [==================>...........] - ETA: 2:52 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04735: setting learning rate to 0.00019906441092592227.\n",
      "1175/1780 [==================>...........] - ETA: 2:52 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04736: setting learning rate to 0.00019906343762467356.\n",
      "1176/1780 [==================>...........] - ETA: 2:52 - loss: 0.2113 - accuracy: 0.9448\n",
      "Batch 04737: setting learning rate to 0.0001990624638198032.\n",
      "1177/1780 [==================>...........] - ETA: 2:52 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04738: setting learning rate to 0.000199061489511316.\n",
      "1178/1780 [==================>...........] - ETA: 2:52 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04739: setting learning rate to 0.00019906051469921704.\n",
      "1179/1780 [==================>...........] - ETA: 2:51 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04740: setting learning rate to 0.00019905953938351126.\n",
      "1180/1780 [==================>...........] - ETA: 2:51 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04741: setting learning rate to 0.00019905856356420353.\n",
      "1181/1780 [==================>...........] - ETA: 2:50 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04742: setting learning rate to 0.00019905758724129893.\n",
      "1182/1780 [==================>...........] - ETA: 2:50 - loss: 0.2111 - accuracy: 0.9448\n",
      "Batch 04743: setting learning rate to 0.00019905661041480234.\n",
      "1183/1780 [==================>...........] - ETA: 2:50 - loss: 0.2110 - accuracy: 0.9449\n",
      "Batch 04744: setting learning rate to 0.00019905563308471875.\n",
      "1184/1780 [==================>...........] - ETA: 2:49 - loss: 0.2111 - accuracy: 0.9449\n",
      "Batch 04745: setting learning rate to 0.00019905465525105315.\n",
      "1185/1780 [==================>...........] - ETA: 2:49 - loss: 0.2110 - accuracy: 0.9449\n",
      "Batch 04746: setting learning rate to 0.00019905367691381044.\n",
      "1186/1780 [==================>...........] - ETA: 2:49 - loss: 0.2111 - accuracy: 0.9449\n",
      "Batch 04747: setting learning rate to 0.0001990526980729957.\n",
      "1187/1780 [===================>..........] - ETA: 2:49 - loss: 0.2111 - accuracy: 0.9449\n",
      "Batch 04748: setting learning rate to 0.00019905171872861384.\n",
      "1188/1780 [===================>..........] - ETA: 2:49 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04749: setting learning rate to 0.00019905073888066982.\n",
      "1189/1780 [===================>..........] - ETA: 2:48 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04750: setting learning rate to 0.00019904975852916867.\n",
      "1190/1780 [===================>..........] - ETA: 2:48 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04751: setting learning rate to 0.00019904877767411535.\n",
      "1191/1780 [===================>..........] - ETA: 2:48 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04752: setting learning rate to 0.00019904779631551488.\n",
      "1192/1780 [===================>..........] - ETA: 2:47 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04753: setting learning rate to 0.00019904681445337218.\n",
      "1193/1780 [===================>..........] - ETA: 2:47 - loss: 0.2113 - accuracy: 0.9448\n",
      "Batch 04754: setting learning rate to 0.00019904583208769233.\n",
      "1194/1780 [===================>..........] - ETA: 2:47 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04755: setting learning rate to 0.00019904484921848026.\n",
      "1195/1780 [===================>..........] - ETA: 2:47 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04756: setting learning rate to 0.00019904386584574096.\n",
      "1196/1780 [===================>..........] - ETA: 2:46 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04757: setting learning rate to 0.00019904288196947946.\n",
      "1197/1780 [===================>..........] - ETA: 2:46 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04758: setting learning rate to 0.0001990418975897008.\n",
      "1198/1780 [===================>..........] - ETA: 2:46 - loss: 0.2112 - accuracy: 0.9448\n",
      "Batch 04759: setting learning rate to 0.00019904091270640985.\n",
      "1199/1780 [===================>..........] - ETA: 2:45 - loss: 0.2113 - accuracy: 0.9448\n",
      "Batch 04760: setting learning rate to 0.00019903992731961178.\n",
      "1200/1780 [===================>..........] - ETA: 2:45 - loss: 0.2114 - accuracy: 0.9448\n",
      "Batch 04761: setting learning rate to 0.0001990389414293115.\n",
      "1201/1780 [===================>..........] - ETA: 2:45 - loss: 0.2113 - accuracy: 0.9448\n",
      "Batch 04762: setting learning rate to 0.00019903795503551405.\n",
      "1202/1780 [===================>..........] - ETA: 2:44 - loss: 0.2112 - accuracy: 0.9449\n",
      "Batch 04763: setting learning rate to 0.00019903696813822444.\n",
      "1203/1780 [===================>..........] - ETA: 2:44 - loss: 0.2111 - accuracy: 0.9449\n",
      "Batch 04764: setting learning rate to 0.00019903598073744768.\n",
      "1204/1780 [===================>..........] - ETA: 2:44 - loss: 0.2111 - accuracy: 0.9449\n",
      "Batch 04765: setting learning rate to 0.00019903499283318882.\n",
      "1205/1780 [===================>..........] - ETA: 2:44 - loss: 0.2110 - accuracy: 0.9449\n",
      "Batch 04766: setting learning rate to 0.00019903400442545285.\n",
      "1206/1780 [===================>..........] - ETA: 2:43 - loss: 0.2109 - accuracy: 0.9449\n",
      "Batch 04767: setting learning rate to 0.0001990330155142448.\n",
      "1207/1780 [===================>..........] - ETA: 2:43 - loss: 0.2108 - accuracy: 0.9449\n",
      "Batch 04768: setting learning rate to 0.00019903202609956972.\n",
      "1208/1780 [===================>..........] - ETA: 2:43 - loss: 0.2109 - accuracy: 0.9450\n",
      "Batch 04769: setting learning rate to 0.00019903103618143262.\n",
      "1209/1780 [===================>..........] - ETA: 2:42 - loss: 0.2109 - accuracy: 0.9450\n",
      "Batch 04770: setting learning rate to 0.00019903004575983854.\n",
      "1210/1780 [===================>..........] - ETA: 2:43 - loss: 0.2108 - accuracy: 0.9450\n",
      "Batch 04771: setting learning rate to 0.0001990290548347925.\n",
      "1211/1780 [===================>..........] - ETA: 2:42 - loss: 0.2109 - accuracy: 0.9450\n",
      "Batch 04772: setting learning rate to 0.00019902806340629957.\n",
      "1212/1780 [===================>..........] - ETA: 2:42 - loss: 0.2108 - accuracy: 0.9450\n",
      "Batch 04773: setting learning rate to 0.00019902707147436476.\n",
      "1213/1780 [===================>..........] - ETA: 2:42 - loss: 0.2108 - accuracy: 0.9449\n",
      "Batch 04774: setting learning rate to 0.0001990260790389931.\n",
      "1214/1780 [===================>..........] - ETA: 2:41 - loss: 0.2108 - accuracy: 0.9449\n",
      "Batch 04775: setting learning rate to 0.0001990250861001897.\n",
      "1215/1780 [===================>..........] - ETA: 2:41 - loss: 0.2108 - accuracy: 0.9449\n",
      "Batch 04776: setting learning rate to 0.00019902409265795955.\n",
      "1216/1780 [===================>..........] - ETA: 2:41 - loss: 0.2110 - accuracy: 0.9448\n",
      "Batch 04777: setting learning rate to 0.0001990230987123077.\n",
      "1217/1780 [===================>..........] - ETA: 2:40 - loss: 0.2113 - accuracy: 0.9448\n",
      "Batch 04778: setting learning rate to 0.00019902210426323922.\n",
      "1218/1780 [===================>..........] - ETA: 2:40 - loss: 0.2114 - accuracy: 0.9448\n",
      "Batch 04779: setting learning rate to 0.00019902110931075917.\n",
      "1219/1780 [===================>..........] - ETA: 2:39 - loss: 0.2114 - accuracy: 0.9448\n",
      "Batch 04780: setting learning rate to 0.00019902011385487263.\n",
      "1220/1780 [===================>..........] - ETA: 2:39 - loss: 0.2114 - accuracy: 0.9448\n",
      "Batch 04781: setting learning rate to 0.0001990191178955846.\n",
      "1221/1780 [===================>..........] - ETA: 2:39 - loss: 0.2114 - accuracy: 0.9448\n",
      "Batch 04782: setting learning rate to 0.00019901812143290016.\n",
      "1222/1780 [===================>..........] - ETA: 2:39 - loss: 0.2114 - accuracy: 0.9449\n",
      "Batch 04783: setting learning rate to 0.00019901712446682444.\n",
      "1223/1780 [===================>..........] - ETA: 2:39 - loss: 0.2113 - accuracy: 0.9449\n",
      "Batch 04784: setting learning rate to 0.00019901612699736246.\n",
      "1224/1780 [===================>..........] - ETA: 2:39 - loss: 0.2114 - accuracy: 0.9448\n",
      "Batch 04785: setting learning rate to 0.00019901512902451924.\n",
      "1225/1780 [===================>..........] - ETA: 2:38 - loss: 0.2115 - accuracy: 0.9448\n",
      "Batch 04786: setting learning rate to 0.00019901413054829997.\n",
      "1226/1780 [===================>..........] - ETA: 2:38 - loss: 0.2116 - accuracy: 0.9448\n",
      "Batch 04787: setting learning rate to 0.00019901313156870962.\n",
      "1227/1780 [===================>..........] - ETA: 2:38 - loss: 0.2117 - accuracy: 0.9447\n",
      "Batch 04788: setting learning rate to 0.00019901213208575334.\n",
      "1228/1780 [===================>..........] - ETA: 2:37 - loss: 0.2117 - accuracy: 0.9446\n",
      "Batch 04789: setting learning rate to 0.00019901113209943615.\n",
      "1229/1780 [===================>..........] - ETA: 2:37 - loss: 0.2118 - accuracy: 0.9446\n",
      "Batch 04790: setting learning rate to 0.0001990101316097632.\n",
      "1230/1780 [===================>..........] - ETA: 2:37 - loss: 0.2117 - accuracy: 0.9446\n",
      "Batch 04791: setting learning rate to 0.0001990091306167395.\n",
      "1231/1780 [===================>..........] - ETA: 2:36 - loss: 0.2119 - accuracy: 0.9446\n",
      "Batch 04792: setting learning rate to 0.00019900812912037021.\n",
      "1232/1780 [===================>..........] - ETA: 2:36 - loss: 0.2121 - accuracy: 0.9446\n",
      "Batch 04793: setting learning rate to 0.0001990071271206604.\n",
      "1233/1780 [===================>..........] - ETA: 2:36 - loss: 0.2124 - accuracy: 0.9446\n",
      "Batch 04794: setting learning rate to 0.00019900612461761513.\n",
      "1234/1780 [===================>..........] - ETA: 2:35 - loss: 0.2125 - accuracy: 0.9446\n",
      "Batch 04795: setting learning rate to 0.00019900512161123953.\n",
      "1235/1780 [===================>..........] - ETA: 2:35 - loss: 0.2125 - accuracy: 0.9446\n",
      "Batch 04796: setting learning rate to 0.0001990041181015387.\n",
      "1236/1780 [===================>..........] - ETA: 2:35 - loss: 0.2125 - accuracy: 0.9446\n",
      "Batch 04797: setting learning rate to 0.00019900311408851772.\n",
      "1237/1780 [===================>..........] - ETA: 2:34 - loss: 0.2126 - accuracy: 0.9446\n",
      "Batch 04798: setting learning rate to 0.00019900210957218172.\n",
      "1238/1780 [===================>..........] - ETA: 2:34 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04799: setting learning rate to 0.00019900110455253582.\n",
      "1239/1780 [===================>..........] - ETA: 2:34 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04800: setting learning rate to 0.00019900009902958505.\n",
      "1240/1780 [===================>..........] - ETA: 2:34 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04801: setting learning rate to 0.0001989990930033346.\n",
      "1241/1780 [===================>..........] - ETA: 2:34 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04802: setting learning rate to 0.00019899808647378956.\n",
      "1242/1780 [===================>..........] - ETA: 2:34 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04803: setting learning rate to 0.00019899707944095508.\n",
      "1243/1780 [===================>..........] - ETA: 2:33 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04804: setting learning rate to 0.00019899607190483623.\n",
      "1244/1780 [===================>..........] - ETA: 2:33 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04805: setting learning rate to 0.00019899506386543813.\n",
      "1245/1780 [===================>..........] - ETA: 2:33 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04806: setting learning rate to 0.00019899405532276593.\n",
      "1246/1780 [====================>.........] - ETA: 2:32 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04807: setting learning rate to 0.00019899304627682475.\n",
      "1247/1780 [====================>.........] - ETA: 2:32 - loss: 0.2127 - accuracy: 0.9445\n",
      "Batch 04808: setting learning rate to 0.00019899203672761972.\n",
      "1248/1780 [====================>.........] - ETA: 2:32 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04809: setting learning rate to 0.00019899102667515596.\n",
      "1249/1780 [====================>.........] - ETA: 2:31 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04810: setting learning rate to 0.00019899001611943865.\n",
      "1250/1780 [====================>.........] - ETA: 2:31 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04811: setting learning rate to 0.00019898900506047288.\n",
      "1251/1780 [====================>.........] - ETA: 2:30 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04812: setting learning rate to 0.0001989879934982638.\n",
      "1252/1780 [====================>.........] - ETA: 2:30 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04813: setting learning rate to 0.0001989869814328166.\n",
      "1253/1780 [====================>.........] - ETA: 2:30 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04814: setting learning rate to 0.00019898596886413632.\n",
      "1254/1780 [====================>.........] - ETA: 2:30 - loss: 0.2127 - accuracy: 0.9445\n",
      "Batch 04815: setting learning rate to 0.0001989849557922282.\n",
      "1255/1780 [====================>.........] - ETA: 2:30 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04816: setting learning rate to 0.00019898394221709738.\n",
      "1256/1780 [====================>.........] - ETA: 2:29 - loss: 0.2128 - accuracy: 0.9445\n",
      "Batch 04817: setting learning rate to 0.00019898292813874896.\n",
      "1257/1780 [====================>.........] - ETA: 2:29 - loss: 0.2128 - accuracy: 0.9445\n",
      "Batch 04818: setting learning rate to 0.00019898191355718816.\n",
      "1258/1780 [====================>.........] - ETA: 2:29 - loss: 0.2128 - accuracy: 0.9444\n",
      "Batch 04819: setting learning rate to 0.00019898089847242008.\n",
      "1259/1780 [====================>.........] - ETA: 2:29 - loss: 0.2128 - accuracy: 0.9444\n",
      "Batch 04820: setting learning rate to 0.00019897988288444994.\n",
      "1260/1780 [====================>.........] - ETA: 2:28 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04821: setting learning rate to 0.00019897886679328284.\n",
      "1261/1780 [====================>.........] - ETA: 2:28 - loss: 0.2129 - accuracy: 0.9443\n",
      "Batch 04822: setting learning rate to 0.000198977850198924.\n",
      "1262/1780 [====================>.........] - ETA: 2:28 - loss: 0.2129 - accuracy: 0.9443\n",
      "Batch 04823: setting learning rate to 0.00019897683310137853.\n",
      "1263/1780 [====================>.........] - ETA: 2:27 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04824: setting learning rate to 0.00019897581550065168.\n",
      "1264/1780 [====================>.........] - ETA: 2:27 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04825: setting learning rate to 0.00019897479739674855.\n",
      "1265/1780 [====================>.........] - ETA: 2:27 - loss: 0.2128 - accuracy: 0.9444\n",
      "Batch 04826: setting learning rate to 0.00019897377878967437.\n",
      "1266/1780 [====================>.........] - ETA: 2:26 - loss: 0.2127 - accuracy: 0.9444\n",
      "Batch 04827: setting learning rate to 0.00019897275967943427.\n",
      "1267/1780 [====================>.........] - ETA: 2:26 - loss: 0.2126 - accuracy: 0.9444\n",
      "Batch 04828: setting learning rate to 0.00019897174006603346.\n",
      "1268/1780 [====================>.........] - ETA: 2:26 - loss: 0.2125 - accuracy: 0.9444\n",
      "Batch 04829: setting learning rate to 0.00019897071994947712.\n",
      "1269/1780 [====================>.........] - ETA: 2:25 - loss: 0.2126 - accuracy: 0.9445\n",
      "Batch 04830: setting learning rate to 0.00019896969932977043.\n",
      "1270/1780 [====================>.........] - ETA: 2:25 - loss: 0.2127 - accuracy: 0.9445\n",
      "Batch 04831: setting learning rate to 0.00019896867820691857.\n",
      "1271/1780 [====================>.........] - ETA: 2:25 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04832: setting learning rate to 0.0001989676565809268.\n",
      "1272/1780 [====================>.........] - ETA: 2:25 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04833: setting learning rate to 0.0001989666344518002.\n",
      "1273/1780 [====================>.........] - ETA: 2:25 - loss: 0.2125 - accuracy: 0.9445\n",
      "Batch 04834: setting learning rate to 0.00019896561181954406.\n",
      "1274/1780 [====================>.........] - ETA: 2:24 - loss: 0.2126 - accuracy: 0.9444\n",
      "Batch 04835: setting learning rate to 0.00019896458868416352.\n",
      "1275/1780 [====================>.........] - ETA: 2:24 - loss: 0.2127 - accuracy: 0.9444\n",
      "Batch 04836: setting learning rate to 0.00019896356504566382.\n",
      "1276/1780 [====================>.........] - ETA: 2:24 - loss: 0.2127 - accuracy: 0.9444\n",
      "Batch 04837: setting learning rate to 0.00019896254090405013.\n",
      "1277/1780 [====================>.........] - ETA: 2:24 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04838: setting learning rate to 0.00019896151625932768.\n",
      "1278/1780 [====================>.........] - ETA: 2:23 - loss: 0.2129 - accuracy: 0.9442\n",
      "Batch 04839: setting learning rate to 0.00019896049111150168.\n",
      "1279/1780 [====================>.........] - ETA: 2:23 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04840: setting learning rate to 0.00019895946546057735.\n",
      "1280/1780 [====================>.........] - ETA: 2:23 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04841: setting learning rate to 0.00019895843930655985.\n",
      "1281/1780 [====================>.........] - ETA: 2:22 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04842: setting learning rate to 0.0001989574126494545.\n",
      "1282/1780 [====================>.........] - ETA: 2:22 - loss: 0.2129 - accuracy: 0.9442\n",
      "Batch 04843: setting learning rate to 0.0001989563854892664.\n",
      "1283/1780 [====================>.........] - ETA: 2:22 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04844: setting learning rate to 0.00019895535782600085.\n",
      "1284/1780 [====================>.........] - ETA: 2:21 - loss: 0.2130 - accuracy: 0.9443\n",
      "Batch 04845: setting learning rate to 0.00019895432965966306.\n",
      "1285/1780 [====================>.........] - ETA: 2:21 - loss: 0.2129 - accuracy: 0.9443\n",
      "Batch 04846: setting learning rate to 0.00019895330099025825.\n",
      "1286/1780 [====================>.........] - ETA: 2:20 - loss: 0.2131 - accuracy: 0.9443\n",
      "Batch 04847: setting learning rate to 0.00019895227181779162.\n",
      "1287/1780 [====================>.........] - ETA: 2:20 - loss: 0.2131 - accuracy: 0.9443\n",
      "Batch 04848: setting learning rate to 0.00019895124214226845.\n",
      "1288/1780 [====================>.........] - ETA: 2:20 - loss: 0.2131 - accuracy: 0.9443\n",
      "Batch 04849: setting learning rate to 0.00019895021196369398.\n",
      "1289/1780 [====================>.........] - ETA: 2:20 - loss: 0.2131 - accuracy: 0.9443\n",
      "Batch 04850: setting learning rate to 0.0001989491812820734.\n",
      "1290/1780 [====================>.........] - ETA: 2:20 - loss: 0.2132 - accuracy: 0.9442\n",
      "Batch 04851: setting learning rate to 0.00019894815009741196.\n",
      "1291/1780 [====================>.........] - ETA: 2:19 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04852: setting learning rate to 0.0001989471184097149.\n",
      "1292/1780 [====================>.........] - ETA: 2:19 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04853: setting learning rate to 0.00019894608621898755.\n",
      "1293/1780 [====================>.........] - ETA: 2:19 - loss: 0.2134 - accuracy: 0.9441\n",
      "Batch 04854: setting learning rate to 0.000198945053525235.\n",
      "1294/1780 [====================>.........] - ETA: 2:19 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04855: setting learning rate to 0.00019894402032846265.\n",
      "1295/1780 [====================>.........] - ETA: 2:18 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04856: setting learning rate to 0.0001989429866286757.\n",
      "1296/1780 [====================>.........] - ETA: 2:18 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04857: setting learning rate to 0.00019894195242587935.\n",
      "1297/1780 [====================>.........] - ETA: 2:18 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04858: setting learning rate to 0.00019894091772007896.\n",
      "1298/1780 [====================>.........] - ETA: 2:17 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04859: setting learning rate to 0.00019893988251127968.\n",
      "1299/1780 [====================>.........] - ETA: 2:17 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04860: setting learning rate to 0.00019893884679948683.\n",
      "1300/1780 [====================>.........] - ETA: 2:17 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04861: setting learning rate to 0.0001989378105847057.\n",
      "1301/1780 [====================>.........] - ETA: 2:16 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04862: setting learning rate to 0.0001989367738669415.\n",
      "1302/1780 [====================>.........] - ETA: 2:16 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04863: setting learning rate to 0.00019893573664619954.\n",
      "1303/1780 [====================>.........] - ETA: 2:16 - loss: 0.2135 - accuracy: 0.9442\n",
      "Batch 04864: setting learning rate to 0.00019893469892248508.\n",
      "1304/1780 [====================>.........] - ETA: 2:15 - loss: 0.2134 - accuracy: 0.9441\n",
      "Batch 04865: setting learning rate to 0.00019893366069580345.\n",
      "1305/1780 [====================>.........] - ETA: 2:16 - loss: 0.2134 - accuracy: 0.9442\n",
      "Batch 04866: setting learning rate to 0.0001989326219661598.\n",
      "1306/1780 [=====================>........] - ETA: 2:15 - loss: 0.2133 - accuracy: 0.9442\n",
      "Batch 04867: setting learning rate to 0.00019893158273355955.\n",
      "1307/1780 [=====================>........] - ETA: 2:15 - loss: 0.2133 - accuracy: 0.9442\n",
      "Batch 04868: setting learning rate to 0.00019893054299800788.\n",
      "1308/1780 [=====================>........] - ETA: 2:15 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04869: setting learning rate to 0.0001989295027595101.\n",
      "1309/1780 [=====================>........] - ETA: 2:14 - loss: 0.2134 - accuracy: 0.9441\n",
      "Batch 04870: setting learning rate to 0.00019892846201807158.\n",
      "1310/1780 [=====================>........] - ETA: 2:14 - loss: 0.2136 - accuracy: 0.9440\n",
      "Batch 04871: setting learning rate to 0.00019892742077369752.\n",
      "1311/1780 [=====================>........] - ETA: 2:14 - loss: 0.2136 - accuracy: 0.9440\n",
      "Batch 04872: setting learning rate to 0.00019892637902639324.\n",
      "1312/1780 [=====================>........] - ETA: 2:13 - loss: 0.2135 - accuracy: 0.9440\n",
      "Batch 04873: setting learning rate to 0.000198925336776164.\n",
      "1313/1780 [=====================>........] - ETA: 2:13 - loss: 0.2135 - accuracy: 0.9440\n",
      "Batch 04874: setting learning rate to 0.00019892429402301517.\n",
      "1314/1780 [=====================>........] - ETA: 2:13 - loss: 0.2135 - accuracy: 0.9440\n",
      "Batch 04875: setting learning rate to 0.000198923250766952.\n",
      "1315/1780 [=====================>........] - ETA: 2:12 - loss: 0.2134 - accuracy: 0.9440\n",
      "Batch 04876: setting learning rate to 0.00019892220700797983.\n",
      "1316/1780 [=====================>........] - ETA: 2:12 - loss: 0.2133 - accuracy: 0.9440\n",
      "Batch 04877: setting learning rate to 0.00019892116274610396.\n",
      "1317/1780 [=====================>........] - ETA: 2:12 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04878: setting learning rate to 0.00019892011798132965.\n",
      "1318/1780 [=====================>........] - ETA: 2:11 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04879: setting learning rate to 0.00019891907271366222.\n",
      "1319/1780 [=====================>........] - ETA: 2:11 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04880: setting learning rate to 0.00019891802694310706.\n",
      "1320/1780 [=====================>........] - ETA: 2:11 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04881: setting learning rate to 0.00019891698066966943.\n",
      "1321/1780 [=====================>........] - ETA: 2:11 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04882: setting learning rate to 0.00019891593389335466.\n",
      "1322/1780 [=====================>........] - ETA: 2:11 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04883: setting learning rate to 0.00019891488661416803.\n",
      "1323/1780 [=====================>........] - ETA: 2:10 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04884: setting learning rate to 0.00019891383883211493.\n",
      "1324/1780 [=====================>........] - ETA: 2:10 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04885: setting learning rate to 0.00019891279054720065.\n",
      "1325/1780 [=====================>........] - ETA: 2:10 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04886: setting learning rate to 0.00019891174175943054.\n",
      "1326/1780 [=====================>........] - ETA: 2:09 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04887: setting learning rate to 0.00019891069246880992.\n",
      "1327/1780 [=====================>........] - ETA: 2:09 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04888: setting learning rate to 0.0001989096426753441.\n",
      "1328/1780 [=====================>........] - ETA: 2:09 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04889: setting learning rate to 0.00019890859237903848.\n",
      "1329/1780 [=====================>........] - ETA: 2:08 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04890: setting learning rate to 0.00019890754157989832.\n",
      "1330/1780 [=====================>........] - ETA: 2:08 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04891: setting learning rate to 0.00019890649027792904.\n",
      "1331/1780 [=====================>........] - ETA: 2:08 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04892: setting learning rate to 0.00019890543847313593.\n",
      "1332/1780 [=====================>........] - ETA: 2:08 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04893: setting learning rate to 0.00019890438616552433.\n",
      "1333/1780 [=====================>........] - ETA: 2:07 - loss: 0.2131 - accuracy: 0.9441\n",
      "Batch 04894: setting learning rate to 0.00019890333335509963.\n",
      "1334/1780 [=====================>........] - ETA: 2:07 - loss: 0.2131 - accuracy: 0.9441\n",
      "Batch 04895: setting learning rate to 0.00019890228004186718.\n",
      "1335/1780 [=====================>........] - ETA: 2:07 - loss: 0.2131 - accuracy: 0.9441\n",
      "Batch 04896: setting learning rate to 0.00019890122622583228.\n",
      "1336/1780 [=====================>........] - ETA: 2:06 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04897: setting learning rate to 0.00019890017190700036.\n",
      "1337/1780 [=====================>........] - ETA: 2:06 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04898: setting learning rate to 0.00019889911708537676.\n",
      "1338/1780 [=====================>........] - ETA: 2:06 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04899: setting learning rate to 0.00019889806176096682.\n",
      "1339/1780 [=====================>........] - ETA: 2:06 - loss: 0.2129 - accuracy: 0.9442\n",
      "Batch 04900: setting learning rate to 0.00019889700593377592.\n",
      "1340/1780 [=====================>........] - ETA: 2:05 - loss: 0.2129 - accuracy: 0.9442\n",
      "Batch 04901: setting learning rate to 0.00019889594960380942.\n",
      "1341/1780 [=====================>........] - ETA: 2:05 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04902: setting learning rate to 0.00019889489277107267.\n",
      "1342/1780 [=====================>........] - ETA: 2:05 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04903: setting learning rate to 0.00019889383543557113.\n",
      "1343/1780 [=====================>........] - ETA: 2:04 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04904: setting learning rate to 0.00019889277759731005.\n",
      "1344/1780 [=====================>........] - ETA: 2:04 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04905: setting learning rate to 0.00019889171925629488.\n",
      "1345/1780 [=====================>........] - ETA: 2:04 - loss: 0.2127 - accuracy: 0.9443\n",
      "Batch 04906: setting learning rate to 0.000198890660412531.\n",
      "1346/1780 [=====================>........] - ETA: 2:04 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04907: setting learning rate to 0.0001988896010660238.\n",
      "1347/1780 [=====================>........] - ETA: 2:03 - loss: 0.2125 - accuracy: 0.9443\n",
      "Batch 04908: setting learning rate to 0.00019888854121677863.\n",
      "1348/1780 [=====================>........] - ETA: 2:03 - loss: 0.2124 - accuracy: 0.9443\n",
      "Batch 04909: setting learning rate to 0.00019888748086480088.\n",
      "1349/1780 [=====================>........] - ETA: 2:03 - loss: 0.2124 - accuracy: 0.9443\n",
      "Batch 04910: setting learning rate to 0.000198886420010096.\n",
      "1350/1780 [=====================>........] - ETA: 2:02 - loss: 0.2124 - accuracy: 0.9443\n",
      "Batch 04911: setting learning rate to 0.0001988853586526693.\n",
      "1351/1780 [=====================>........] - ETA: 2:02 - loss: 0.2127 - accuracy: 0.9443\n",
      "Batch 04912: setting learning rate to 0.0001988842967925262.\n",
      "1352/1780 [=====================>........] - ETA: 2:02 - loss: 0.2127 - accuracy: 0.9443\n",
      "Batch 04913: setting learning rate to 0.0001988832344296722.\n",
      "1353/1780 [=====================>........] - ETA: 2:02 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04914: setting learning rate to 0.00019888217156411255.\n",
      "1354/1780 [=====================>........] - ETA: 2:01 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04915: setting learning rate to 0.00019888110819585273.\n",
      "1355/1780 [=====================>........] - ETA: 2:01 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04916: setting learning rate to 0.00019888004432489816.\n",
      "1356/1780 [=====================>........] - ETA: 2:01 - loss: 0.2127 - accuracy: 0.9443\n",
      "Batch 04917: setting learning rate to 0.00019887897995125417.\n",
      "1357/1780 [=====================>........] - ETA: 2:01 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04918: setting learning rate to 0.0001988779150749263.\n",
      "1358/1780 [=====================>........] - ETA: 2:00 - loss: 0.2125 - accuracy: 0.9444\n",
      "Batch 04919: setting learning rate to 0.0001988768496959198.\n",
      "1359/1780 [=====================>........] - ETA: 2:00 - loss: 0.2124 - accuracy: 0.9444\n",
      "Batch 04920: setting learning rate to 0.00019887578381424023.\n",
      "1360/1780 [=====================>........] - ETA: 2:00 - loss: 0.2124 - accuracy: 0.9444\n",
      "Batch 04921: setting learning rate to 0.00019887471742989295.\n",
      "1361/1780 [=====================>........] - ETA: 1:59 - loss: 0.2124 - accuracy: 0.9444\n",
      "Batch 04922: setting learning rate to 0.0001988736505428834.\n",
      "1362/1780 [=====================>........] - ETA: 1:59 - loss: 0.2124 - accuracy: 0.9444\n",
      "Batch 04923: setting learning rate to 0.00019887258315321696.\n",
      "1363/1780 [=====================>........] - ETA: 1:59 - loss: 0.2126 - accuracy: 0.9444\n",
      "Batch 04924: setting learning rate to 0.0001988715152608991.\n",
      "1364/1780 [=====================>........] - ETA: 1:58 - loss: 0.2125 - accuracy: 0.9444\n",
      "Batch 04925: setting learning rate to 0.0001988704468659352.\n",
      "1365/1780 [======================>.......] - ETA: 1:58 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04926: setting learning rate to 0.00019886937796833077.\n",
      "1366/1780 [======================>.......] - ETA: 1:58 - loss: 0.2127 - accuracy: 0.9443\n",
      "Batch 04927: setting learning rate to 0.00019886830856809118.\n",
      "1367/1780 [======================>.......] - ETA: 1:58 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04928: setting learning rate to 0.00019886723866522187.\n",
      "1368/1780 [======================>.......] - ETA: 1:57 - loss: 0.2126 - accuracy: 0.9443\n",
      "Batch 04929: setting learning rate to 0.00019886616825972832.\n",
      "1369/1780 [======================>.......] - ETA: 1:57 - loss: 0.2125 - accuracy: 0.9443\n",
      "Batch 04930: setting learning rate to 0.00019886509735161595.\n",
      "1370/1780 [======================>.......] - ETA: 1:57 - loss: 0.2124 - accuracy: 0.9443\n",
      "Batch 04931: setting learning rate to 0.00019886402594089018.\n",
      "1371/1780 [======================>.......] - ETA: 1:57 - loss: 0.2125 - accuracy: 0.9443\n",
      "Batch 04932: setting learning rate to 0.0001988629540275565.\n",
      "1372/1780 [======================>.......] - ETA: 1:56 - loss: 0.2125 - accuracy: 0.9443\n",
      "Batch 04933: setting learning rate to 0.00019886188161162034.\n",
      "1373/1780 [======================>.......] - ETA: 1:56 - loss: 0.2125 - accuracy: 0.9443\n",
      "Batch 04934: setting learning rate to 0.0001988608086930871.\n",
      "1374/1780 [======================>.......] - ETA: 1:56 - loss: 0.2129 - accuracy: 0.9443\n",
      "Batch 04935: setting learning rate to 0.00019885973527196234.\n",
      "1375/1780 [======================>.......] - ETA: 1:55 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04936: setting learning rate to 0.00019885866134825144.\n",
      "1376/1780 [======================>.......] - ETA: 1:55 - loss: 0.2128 - accuracy: 0.9443\n",
      "Batch 04937: setting learning rate to 0.0001988575869219599.\n",
      "1377/1780 [======================>.......] - ETA: 1:55 - loss: 0.2129 - accuracy: 0.9443\n",
      "Batch 04938: setting learning rate to 0.00019885651199309315.\n",
      "1378/1780 [======================>.......] - ETA: 1:54 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04939: setting learning rate to 0.00019885543656165665.\n",
      "1379/1780 [======================>.......] - ETA: 1:54 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04940: setting learning rate to 0.00019885436062765593.\n",
      "1380/1780 [======================>.......] - ETA: 1:54 - loss: 0.2131 - accuracy: 0.9442\n",
      "Batch 04941: setting learning rate to 0.00019885328419109636.\n",
      "1381/1780 [======================>.......] - ETA: 1:53 - loss: 0.2130 - accuracy: 0.9442\n",
      "Batch 04942: setting learning rate to 0.0001988522072519835.\n",
      "1382/1780 [======================>.......] - ETA: 1:53 - loss: 0.2130 - accuracy: 0.9441\n",
      "Batch 04943: setting learning rate to 0.00019885112981032278.\n",
      "1383/1780 [======================>.......] - ETA: 1:53 - loss: 0.2131 - accuracy: 0.9441\n",
      "Batch 04944: setting learning rate to 0.00019885005186611972.\n",
      "1384/1780 [======================>.......] - ETA: 1:53 - loss: 0.2131 - accuracy: 0.9441\n",
      "Batch 04945: setting learning rate to 0.00019884897341937975.\n",
      "1385/1780 [======================>.......] - ETA: 1:53 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04946: setting learning rate to 0.00019884789447010835.\n",
      "1386/1780 [======================>.......] - ETA: 1:52 - loss: 0.2131 - accuracy: 0.9441\n",
      "Batch 04947: setting learning rate to 0.00019884681501831106.\n",
      "1387/1780 [======================>.......] - ETA: 1:52 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04948: setting learning rate to 0.0001988457350639933.\n",
      "1388/1780 [======================>.......] - ETA: 1:52 - loss: 0.2134 - accuracy: 0.9441\n",
      "Batch 04949: setting learning rate to 0.00019884465460716063.\n",
      "1389/1780 [======================>.......] - ETA: 1:51 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04950: setting learning rate to 0.00019884357364781852.\n",
      "1390/1780 [======================>.......] - ETA: 1:51 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04951: setting learning rate to 0.00019884249218597244.\n",
      "1391/1780 [======================>.......] - ETA: 1:51 - loss: 0.2132 - accuracy: 0.9441\n",
      "Batch 04952: setting learning rate to 0.00019884141022162788.\n",
      "1392/1780 [======================>.......] - ETA: 1:50 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04953: setting learning rate to 0.0001988403277547904.\n",
      "1393/1780 [======================>.......] - ETA: 1:50 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04954: setting learning rate to 0.00019883924478546544.\n",
      "1394/1780 [======================>.......] - ETA: 1:50 - loss: 0.2133 - accuracy: 0.9441\n",
      "Batch 04955: setting learning rate to 0.0001988381613136585.\n",
      "1395/1780 [======================>.......] - ETA: 1:50 - loss: 0.2134 - accuracy: 0.9441\n",
      "Batch 04956: setting learning rate to 0.0001988370773393752.\n",
      "1396/1780 [======================>.......] - ETA: 1:49 - loss: 0.2134 - accuracy: 0.9440\n",
      "Batch 04957: setting learning rate to 0.00019883599286262088.\n",
      "1397/1780 [======================>.......] - ETA: 1:49 - loss: 0.2133 - accuracy: 0.9440\n",
      "Batch 04958: setting learning rate to 0.00019883490788340122.\n",
      "1398/1780 [======================>.......] - ETA: 1:49 - loss: 0.2133 - accuracy: 0.9440\n",
      "Batch 04959: setting learning rate to 0.0001988338224017216.\n",
      "1399/1780 [======================>.......] - ETA: 1:49 - loss: 0.2134 - accuracy: 0.9440\n",
      "Batch 04960: setting learning rate to 0.00019883273641758766.\n",
      "1400/1780 [======================>.......] - ETA: 1:48 - loss: 0.2134 - accuracy: 0.9440\n",
      "Batch 04961: setting learning rate to 0.00019883164993100483.\n",
      "1401/1780 [======================>.......] - ETA: 1:48 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04962: setting learning rate to 0.00019883056294197867.\n",
      "1402/1780 [======================>.......] - ETA: 1:48 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04963: setting learning rate to 0.00019882947545051466.\n",
      "1403/1780 [======================>.......] - ETA: 1:48 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04964: setting learning rate to 0.0001988283874566184.\n",
      "1404/1780 [======================>.......] - ETA: 1:47 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04965: setting learning rate to 0.0001988272989602954.\n",
      "1405/1780 [======================>.......] - ETA: 1:47 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04966: setting learning rate to 0.00019882620996155117.\n",
      "1406/1780 [======================>.......] - ETA: 1:47 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04967: setting learning rate to 0.00019882512046039125.\n",
      "1407/1780 [======================>.......] - ETA: 1:46 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04968: setting learning rate to 0.00019882403045682118.\n",
      "1408/1780 [======================>.......] - ETA: 1:46 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04969: setting learning rate to 0.00019882293995084654.\n",
      "1409/1780 [======================>.......] - ETA: 1:46 - loss: 0.2137 - accuracy: 0.9439\n",
      "Batch 04970: setting learning rate to 0.0001988218489424728.\n",
      "1410/1780 [======================>.......] - ETA: 1:45 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04971: setting learning rate to 0.0001988207574317056.\n",
      "1411/1780 [======================>.......] - ETA: 1:45 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04972: setting learning rate to 0.0001988196654185504.\n",
      "1412/1780 [======================>.......] - ETA: 1:45 - loss: 0.2137 - accuracy: 0.9439\n",
      "Batch 04973: setting learning rate to 0.0001988185729030128.\n",
      "1413/1780 [======================>.......] - ETA: 1:44 - loss: 0.2137 - accuracy: 0.9439\n",
      "Batch 04974: setting learning rate to 0.00019881747988509835.\n",
      "1414/1780 [======================>.......] - ETA: 1:44 - loss: 0.2137 - accuracy: 0.9439\n",
      "Batch 04975: setting learning rate to 0.00019881638636481258.\n",
      "1415/1780 [======================>.......] - ETA: 1:44 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04976: setting learning rate to 0.00019881529234216109.\n",
      "1416/1780 [======================>.......] - ETA: 1:44 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04977: setting learning rate to 0.00019881419781714942.\n",
      "1417/1780 [======================>.......] - ETA: 1:43 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 04978: setting learning rate to 0.00019881310278978314.\n",
      "1418/1780 [======================>.......] - ETA: 1:43 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04979: setting learning rate to 0.0001988120072600678.\n",
      "1419/1780 [======================>.......] - ETA: 1:43 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04980: setting learning rate to 0.00019881091122800899.\n",
      "1420/1780 [======================>.......] - ETA: 1:43 - loss: 0.2135 - accuracy: 0.9438\n",
      "Batch 04981: setting learning rate to 0.00019880981469361228.\n",
      "1421/1780 [======================>.......] - ETA: 1:42 - loss: 0.2137 - accuracy: 0.9438\n",
      "Batch 04982: setting learning rate to 0.00019880871765688322.\n",
      "1422/1780 [======================>.......] - ETA: 1:42 - loss: 0.2137 - accuracy: 0.9438\n",
      "Batch 04983: setting learning rate to 0.0001988076201178274.\n",
      "1423/1780 [======================>.......] - ETA: 1:42 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04984: setting learning rate to 0.00019880652207645045.\n",
      "1424/1780 [=======================>......] - ETA: 1:41 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04985: setting learning rate to 0.0001988054235327579.\n",
      "1425/1780 [=======================>......] - ETA: 1:41 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04986: setting learning rate to 0.00019880432448675533.\n",
      "1426/1780 [=======================>......] - ETA: 1:41 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04987: setting learning rate to 0.00019880322493844832.\n",
      "1427/1780 [=======================>......] - ETA: 1:41 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04988: setting learning rate to 0.00019880212488784248.\n",
      "1428/1780 [=======================>......] - ETA: 1:40 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04989: setting learning rate to 0.00019880102433494343.\n",
      "1429/1780 [=======================>......] - ETA: 1:40 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04990: setting learning rate to 0.00019879992327975673.\n",
      "1430/1780 [=======================>......] - ETA: 1:40 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04991: setting learning rate to 0.00019879882172228798.\n",
      "1431/1780 [=======================>......] - ETA: 1:39 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04992: setting learning rate to 0.0001987977196625428.\n",
      "1432/1780 [=======================>......] - ETA: 1:39 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04993: setting learning rate to 0.00019879661710052674.\n",
      "1433/1780 [=======================>......] - ETA: 1:39 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04994: setting learning rate to 0.0001987955140362455.\n",
      "1434/1780 [=======================>......] - ETA: 1:39 - loss: 0.2134 - accuracy: 0.9439\n",
      "Batch 04995: setting learning rate to 0.0001987944104697046.\n",
      "1435/1780 [=======================>......] - ETA: 1:38 - loss: 0.2135 - accuracy: 0.9439\n",
      "Batch 04996: setting learning rate to 0.00019879330640090967.\n",
      "1436/1780 [=======================>......] - ETA: 1:38 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04997: setting learning rate to 0.00019879220182986637.\n",
      "1437/1780 [=======================>......] - ETA: 1:38 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04998: setting learning rate to 0.00019879109675658026.\n",
      "1438/1780 [=======================>......] - ETA: 1:38 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 04999: setting learning rate to 0.00019878999118105696.\n",
      "1439/1780 [=======================>......] - ETA: 1:37 - loss: 0.2136 - accuracy: 0.9439\n",
      "Batch 05000: setting learning rate to 0.00019878888510330213.\n",
      "1440/1780 [=======================>......] - ETA: 1:37 - loss: 0.2137 - accuracy: 0.9438\n",
      "Batch 05001: setting learning rate to 0.00019878777852332138.\n",
      "1441/1780 [=======================>......] - ETA: 1:37 - loss: 0.2138 - accuracy: 0.9437\n",
      "Batch 05002: setting learning rate to 0.00019878667144112032.\n",
      "1442/1780 [=======================>......] - ETA: 1:36 - loss: 0.2137 - accuracy: 0.9437\n",
      "Batch 05003: setting learning rate to 0.00019878556385670456.\n",
      "1443/1780 [=======================>......] - ETA: 1:36 - loss: 0.2137 - accuracy: 0.9437\n",
      "Batch 05004: setting learning rate to 0.00019878445577007977.\n",
      "1444/1780 [=======================>......] - ETA: 1:36 - loss: 0.2138 - accuracy: 0.9437\n",
      "Batch 05005: setting learning rate to 0.00019878334718125159.\n",
      "1445/1780 [=======================>......] - ETA: 1:35 - loss: 0.2137 - accuracy: 0.9437\n",
      "Batch 05006: setting learning rate to 0.0001987822380902256.\n",
      "1446/1780 [=======================>......] - ETA: 1:35 - loss: 0.2137 - accuracy: 0.9438\n",
      "Batch 05007: setting learning rate to 0.0001987811284970075.\n",
      "1447/1780 [=======================>......] - ETA: 1:35 - loss: 0.2136 - accuracy: 0.9438\n",
      "Batch 05008: setting learning rate to 0.00019878001840160293.\n"
     ]
    }
   ],
   "source": [
    "run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet', batch = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "564DXmWLDMxa"
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "1qVdSNFxjg5-",
    "outputId": "c571704d-0a0e-4a65-a7f8-15fcb6b82127"
   },
   "outputs": [],
   "source": [
    "# run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-6Yocnrq2u4F"
   ],
   "name": "25_May.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
