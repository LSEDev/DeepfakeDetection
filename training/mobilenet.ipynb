{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udRhv-d-2i8l"
   },
   "source": [
    "# Train preliminary models - Xception, ResNet, EfficentNet etc.\n",
    "       -- built for FF+ dataset with file structure as required by Keras' flow_from_directory method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "t7E1CjC9fqhq",
    "outputId": "abdef6eb-823c-4fb1-ad28-7521345d04fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 14 13:34:42 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P0    29W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# See available GPU RAM \n",
    "!nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !watch -n0.1 nvidia-smi # better way to see GPU utilisation\n",
    "# !htop # cpu threads and if they're all working\n",
    "# !pip3 install --no-cache-dir -I tensorflow==2.2 #Â use if no gpu is attached so code will run \n",
    "# !sudo kill -9 PID # clear GPU memory where 9 is PID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation installations\n",
    "# !pip install CMake\n",
    "# !pip install dlib\n",
    "# !pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Llx-HRnYiWQU",
    "outputId": "6e6a3556-fbb1-4972-b046-8586183f768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.1-dlenv_tfe\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "3DRA3QPDgLLR",
    "outputId": "ed171b89-378d-469d-a254-c291be71af4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/efficientnet\n",
      "  Cloning https://github.com/qubvel/efficientnet to /tmp/pip-req-build-aswq7ze_\n",
      "  Running command git clone -q https://github.com/qubvel/efficientnet /tmp/pip-req-build-aswq7ze_\n",
      "Requirement already satisfied (use --upgrade to upgrade): efficientnet==1.1.0 from git+https://github.com/qubvel/efficientnet in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (0.17.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.19.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.5.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (3.2.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.4)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (7.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2020.7.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (0.10.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.0) (4.4.2)\n",
      "Building wheels for collected packages: efficientnet\n",
      "  Building wheel for efficientnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet: filename=efficientnet-1.1.0-py3-none-any.whl size=18397 sha256=4e9c4bdc96f8d831173a2559937a1c65a1a3c732ea6a5454a0ea14c40fe2f95e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u36sg8kn/wheels/11/69/85/814d64d694c96db0eef17b718042d644a1e54f113920481920\n",
      "Successfully built efficientnet\n"
     ]
    }
   ],
   "source": [
    "# Required for EfficientNet\n",
    "!pip install git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtO5vELz8i3-"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ7mWThq32JA"
   },
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture):\n",
    "    '''Builds a specified network with the selected dropout after the last dense layer.\n",
    "\n",
    "    Architectures that can be selected are:\n",
    "    vgg, xception, resnet50, mobilenet, efficientnet, densenet\n",
    "    \n",
    "    Optimiser is Adam, with a provided learning rate (lr_rate) and fixed\n",
    "    decay 1e-6, loss is traditionally categorical_crossentropy.'''\n",
    "\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "    if architecture=='xception':\n",
    "        from tensorflow.keras.applications.xception import Xception\n",
    "        conv_base = Xception(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture=='vgg':\n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='resnet50':\n",
    "        from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "        conv_base = ResNet50(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='mobilenet':\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture== 'efficientnet':\n",
    "        # EfficientNetB7 has the highest top-1 accuracy on imagenet\n",
    "        # among EfficientNextB{0:7}\n",
    "        from efficientnet.tfkeras import EfficientNetB0\n",
    "        conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "        \n",
    "    elif architecture== 'densenet':\n",
    "        from tensorflow.keras.applications.densenet import DenseNet201\n",
    "        conv_base = DenseNet201(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture not in ['vgg', 'xception', 'resnet50',\n",
    "                              'mobilenet', 'efficientnet', 'densenet']:\n",
    "        return \"An unknown network is specified\"\n",
    "    \n",
    "\n",
    "    outputconv_base = conv_base.output\n",
    "    t_flat = Flatten()(outputconv_base)\n",
    "#     t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
    "#     t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "#     t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
    "#     t_do = Dropout(dropout)(t_dense3)\n",
    "    t_dense1 = Dense(256, activation='relu')(t_flat)\n",
    "    t_dense2 = Dense(128, activation='relu')(t_dense1)\n",
    "    t_do = Dropout(dropout)(t_dense2)\n",
    "    predictions = Dense(2, activation= 'softmax')(t_do)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=predictions, name = 'model')\n",
    "\n",
    "    conv_base.trainable = False # freeze the convolutional base\n",
    "    \n",
    "    # # Code below trains all layers without using any pretrained weights\n",
    "    #for layer in conv_base.layers:\n",
    "    #  layer.trainable = True\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate= lr_rate, decay=1e-6)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_rate)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01),\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen_train = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "#             width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "#             height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            brightness_range=[0.6, 1.4],\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "    \n",
    "    datagen_test = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen_train.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    val_data = datagen_test.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary train time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights\n",
    "\n",
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed. '''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/mob_dense_test\"\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # if there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # pick out accuracies out of file names\n",
    "            acc = [el[len(el)-10 : len(el)-5] for el in all_weights]\n",
    "            # get index of the first maximum accuracy\n",
    "            optimal_index = acc.index(max(acc))\n",
    "            # get the name of the file with optimal weights, load corresponding weights\n",
    "            optimal_weights = all_weights[optimal_index]\n",
    "            print(\"Loading\", path_to_weights + '/' + optimal_weights)\n",
    "            model.load_weights(path_to_weights + '/' + optimal_weights)\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture):\n",
    "    '''Takes the weights with the highest val accuracy and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/mob_dense_test_model.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/mob_dense_test'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/mob_dense_test')\n",
    "\n",
    "    # Save weights - below saves every epoch where there is improvement\n",
    "    # filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/mob_dense_test/highest_val_acc.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/mob_dense_test.csv',\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    # Set learning rate config \n",
    "    sample_count = 60000 # number of training samples\n",
    "    epochs = 50 # total epochs - affects total steps (and hence speed of decay)\n",
    "    warmup_epoch = 3 # number of warmup epochs\n",
    "    batch_size = train_data.batch_size\n",
    "    learning_rate_base = 0.0002\n",
    "    total_steps = int(epochs * sample_count / batch_size)\n",
    "    warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "    \n",
    "    warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warmup_steps,\n",
    "                                        hold_base_rate_steps=2,\n",
    "                                        verbose=0)\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses, checkpoint, csv_logger, es, warm_up_lr],\n",
    "              initial_epoch=0,                    # start training epoch - useful if continuing training\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')\n",
    "        \n",
    "    model.save_weights('../all_faces_bucket/trained_models/weights/mob_dense_testlastepoch.hdf5') \n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/mob_dense_testlastepoch.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIUZirJoxsdx"
   },
   "source": [
    "## Unifying Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYFsNbZMqYTv"
   },
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture)\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in8HHH594qtA"
   },
   "source": [
    "## Train Various Model Architectures\n",
    "Note: Make sure CPUs have enough memory for each batch eg. 1 core with 3.75GB RAM cant take batches larger than 32.  \n",
    "8 CPUs with 30GB RAM typically works well for batches of 256.\n",
    "\n",
    "In this model: 4 cores/8GB and T4 used - took approx 8 hours to train top dense layers and fine tune. \n",
    "\n",
    "Also note that while multiprocessing speeds up training, it interacts badly with Tensorflow and leads to deadlocks. To be on the safe side, set use_multiprocessing to False when training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz0ilB_F8F4g"
   },
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "colab_type": "code",
    "id": "_GZwsiNC7rkK",
    "outputId": "7b0348f4-e292-4cb7-af8f-49af0eda2638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n",
      "Found 113928 images belonging to 2 classes.\n",
      "Found 21291 images belonging to 2 classes.\n",
      "Ensure class weights function corresponds to these class indices: {'authentic': 0, 'fake': 1}\n",
      "There are no weights stored. Training model from scratch:\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 3560 steps, validate for 665 steps\n",
      "Epoch 1/50\n",
      "\n",
      "Batch 00001: setting learning rate to 0.0.\n",
      "   1/3560 [..............................] - ETA: 10:27:42 - loss: 1.5081 - accuracy: 0.4375\n",
      "Batch 00002: setting learning rate to 3.5555555555555554e-08.\n",
      "   2/3560 [..............................] - ETA: 5:15:22 - loss: 1.5243 - accuracy: 0.4688 \n",
      "Batch 00003: setting learning rate to 7.111111111111111e-08.\n",
      "   3/3560 [..............................] - ETA: 3:31:12 - loss: 1.4674 - accuracy: 0.4792\n",
      "Batch 00004: setting learning rate to 1.0666666666666667e-07.\n",
      "   4/3560 [..............................] - ETA: 2:39:09 - loss: 1.5419 - accuracy: 0.5000\n",
      "Batch 00005: setting learning rate to 1.4222222222222222e-07.\n",
      "   5/3560 [..............................] - ETA: 2:07:59 - loss: 1.7915 - accuracy: 0.4938\n",
      "Batch 00006: setting learning rate to 1.7777777777777776e-07.\n",
      "   6/3560 [..............................] - ETA: 1:47:15 - loss: 1.7230 - accuracy: 0.4896\n",
      "Batch 00007: setting learning rate to 2.1333333333333334e-07.\n",
      "   7/3560 [..............................] - ETA: 1:32:22 - loss: 1.7475 - accuracy: 0.4821\n",
      "Batch 00008: setting learning rate to 2.4888888888888886e-07.\n",
      "   8/3560 [..............................] - ETA: 1:21:10 - loss: 1.7314 - accuracy: 0.4961\n",
      "Batch 00009: setting learning rate to 2.8444444444444443e-07.\n",
      "   9/3560 [..............................] - ETA: 1:12:29 - loss: 1.6580 - accuracy: 0.5104\n",
      "Batch 00010: setting learning rate to 3.2e-07.\n",
      "  10/3560 [..............................] - ETA: 1:05:32 - loss: 1.6580 - accuracy: 0.5031\n",
      "Batch 00011: setting learning rate to 3.5555555555555553e-07.\n",
      "  11/3560 [..............................] - ETA: 59:51 - loss: 1.6440 - accuracy: 0.5028  \n",
      "Batch 00012: setting learning rate to 3.911111111111111e-07.\n",
      "  12/3560 [..............................] - ETA: 55:06 - loss: 1.5635 - accuracy: 0.5182\n",
      "Batch 00013: setting learning rate to 4.266666666666667e-07.\n",
      "  13/3560 [..............................] - ETA: 51:05 - loss: 1.5428 - accuracy: 0.5264\n",
      "Batch 00014: setting learning rate to 4.622222222222222e-07.\n",
      "  14/3560 [..............................] - ETA: 47:40 - loss: 1.4985 - accuracy: 0.5402\n",
      "Batch 00015: setting learning rate to 4.977777777777777e-07.\n",
      "  15/3560 [..............................] - ETA: 44:41 - loss: 1.4929 - accuracy: 0.5479\n",
      "Batch 00016: setting learning rate to 5.333333333333333e-07.\n",
      "  16/3560 [..............................] - ETA: 42:06 - loss: 1.4688 - accuracy: 0.5508\n",
      "Batch 00017: setting learning rate to 5.688888888888889e-07.\n",
      "  17/3560 [..............................] - ETA: 39:47 - loss: 1.5096 - accuracy: 0.5441\n",
      "Batch 00018: setting learning rate to 6.044444444444444e-07.\n",
      "  18/3560 [..............................] - ETA: 37:45 - loss: 1.4817 - accuracy: 0.5469\n",
      "Batch 00019: setting learning rate to 6.4e-07.\n",
      "  19/3560 [..............................] - ETA: 35:56 - loss: 1.5127 - accuracy: 0.5526\n",
      "Batch 00020: setting learning rate to 6.755555555555555e-07.\n",
      "  20/3560 [..............................] - ETA: 34:16 - loss: 1.4863 - accuracy: 0.5547\n",
      "Batch 00021: setting learning rate to 7.111111111111111e-07.\n",
      "  21/3560 [..............................] - ETA: 32:47 - loss: 1.4959 - accuracy: 0.5536\n",
      "Batch 00022: setting learning rate to 7.466666666666667e-07.\n",
      "  22/3560 [..............................] - ETA: 31:26 - loss: 1.5144 - accuracy: 0.5554\n",
      "Batch 00023: setting learning rate to 7.822222222222222e-07.\n",
      "  23/3560 [..............................] - ETA: 30:12 - loss: 1.5293 - accuracy: 0.5530\n",
      "Batch 00024: setting learning rate to 8.177777777777777e-07.\n",
      "\n",
      "Batch 00025: setting learning rate to 8.533333333333334e-07.\n",
      "  25/3560 [..............................] - ETA: 28:00 - loss: 1.4913 - accuracy: 0.5612\n",
      "Batch 00026: setting learning rate to 8.888888888888889e-07.\n",
      "  26/3560 [..............................] - ETA: 27:02 - loss: 1.4967 - accuracy: 0.5613\n",
      "Batch 00027: setting learning rate to 9.244444444444444e-07.\n",
      "  27/3560 [..............................] - ETA: 26:09 - loss: 1.4786 - accuracy: 0.5660\n",
      "Batch 00028: setting learning rate to 9.6e-07.\n",
      "  28/3560 [..............................] - ETA: 25:19 - loss: 1.4601 - accuracy: 0.5692\n",
      "Batch 00029: setting learning rate to 9.955555555555554e-07.\n",
      "  29/3560 [..............................] - ETA: 24:33 - loss: 1.4535 - accuracy: 0.5733\n",
      "Batch 00030: setting learning rate to 1.0311111111111112e-06.\n",
      "  30/3560 [..............................] - ETA: 23:50 - loss: 1.4328 - accuracy: 0.5729\n",
      "Batch 00031: setting learning rate to 1.0666666666666667e-06.\n",
      "  31/3560 [..............................] - ETA: 23:10 - loss: 1.4260 - accuracy: 0.5736\n",
      "Batch 00032: setting learning rate to 1.1022222222222222e-06.\n",
      "  32/3560 [..............................] - ETA: 22:32 - loss: 1.4245 - accuracy: 0.5703\n",
      "Batch 00033: setting learning rate to 1.1377777777777777e-06.\n",
      "\n",
      "Batch 00034: setting learning rate to 1.1733333333333333e-06.\n",
      "  34/3560 [..............................] - ETA: 21:22 - loss: 1.4255 - accuracy: 0.5542\n",
      "Batch 00035: setting learning rate to 1.2088888888888888e-06.\n",
      "  35/3560 [..............................] - ETA: 20:50 - loss: 1.4082 - accuracy: 0.5571\n",
      "Batch 00036: setting learning rate to 1.2444444444444443e-06.\n",
      "  36/3560 [..............................] - ETA: 20:21 - loss: 1.4151 - accuracy: 0.5547\n",
      "Batch 00037: setting learning rate to 1.28e-06.\n",
      "  37/3560 [..............................] - ETA: 19:53 - loss: 1.4177 - accuracy: 0.5515\n",
      "Batch 00038: setting learning rate to 1.3155555555555555e-06.\n",
      "  38/3560 [..............................] - ETA: 19:26 - loss: 1.4098 - accuracy: 0.5535\n",
      "Batch 00039: setting learning rate to 1.351111111111111e-06.\n",
      "  39/3560 [..............................] - ETA: 19:01 - loss: 1.4038 - accuracy: 0.5529\n",
      "Batch 00040: setting learning rate to 1.3866666666666666e-06.\n",
      "  40/3560 [..............................] - ETA: 18:37 - loss: 1.4054 - accuracy: 0.5555\n",
      "Batch 00041: setting learning rate to 1.4222222222222221e-06.\n",
      "  41/3560 [..............................] - ETA: 18:14 - loss: 1.3959 - accuracy: 0.5587\n",
      "Batch 00042: setting learning rate to 1.4577777777777776e-06.\n",
      "  42/3560 [..............................] - ETA: 17:52 - loss: 1.3968 - accuracy: 0.5580\n",
      "Batch 00043: setting learning rate to 1.4933333333333334e-06.\n",
      "  43/3560 [..............................] - ETA: 17:31 - loss: 1.3945 - accuracy: 0.5596\n",
      "Batch 00044: setting learning rate to 1.5288888888888889e-06.\n",
      "  44/3560 [..............................] - ETA: 17:12 - loss: 1.3960 - accuracy: 0.5582\n",
      "Batch 00045: setting learning rate to 1.5644444444444444e-06.\n",
      "  45/3560 [..............................] - ETA: 16:53 - loss: 1.3983 - accuracy: 0.5583\n",
      "Batch 00046: setting learning rate to 1.6e-06.\n",
      "  46/3560 [..............................] - ETA: 16:35 - loss: 1.3933 - accuracy: 0.5584\n",
      "Batch 00047: setting learning rate to 1.6355555555555554e-06.\n",
      "  47/3560 [..............................] - ETA: 16:17 - loss: 1.3856 - accuracy: 0.5559\n",
      "Batch 00048: setting learning rate to 1.671111111111111e-06.\n",
      "  48/3560 [..............................] - ETA: 16:00 - loss: 1.3833 - accuracy: 0.5560\n",
      "Batch 00049: setting learning rate to 1.7066666666666667e-06.\n",
      "  49/3560 [..............................] - ETA: 15:45 - loss: 1.3824 - accuracy: 0.5555\n",
      "Batch 00050: setting learning rate to 1.7422222222222222e-06.\n",
      "  50/3560 [..............................] - ETA: 15:29 - loss: 1.3799 - accuracy: 0.5562\n",
      "Batch 00051: setting learning rate to 1.7777777777777777e-06.\n",
      "  51/3560 [..............................] - ETA: 15:14 - loss: 1.3794 - accuracy: 0.5582\n",
      "Batch 00052: setting learning rate to 1.8133333333333333e-06.\n",
      "\n",
      "Batch 00053: setting learning rate to 1.8488888888888888e-06.\n",
      "  53/3560 [..............................] - ETA: 14:46 - loss: 1.3718 - accuracy: 0.5619\n",
      "Batch 00054: setting learning rate to 1.8844444444444443e-06.\n",
      "  54/3560 [..............................] - ETA: 14:33 - loss: 1.3645 - accuracy: 0.5625\n",
      "Batch 00055: setting learning rate to 1.92e-06.\n",
      "  55/3560 [..............................] - ETA: 14:20 - loss: 1.3676 - accuracy: 0.5602\n",
      "Batch 00056: setting learning rate to 1.9555555555555556e-06.\n",
      "  56/3560 [..............................] - ETA: 14:08 - loss: 1.3757 - accuracy: 0.5597\n",
      "Batch 00057: setting learning rate to 1.991111111111111e-06.\n",
      "  57/3560 [..............................] - ETA: 13:56 - loss: 1.3743 - accuracy: 0.5609\n",
      "Batch 00058: setting learning rate to 2.0266666666666666e-06.\n",
      "  58/3560 [..............................] - ETA: 13:44 - loss: 1.3717 - accuracy: 0.5614\n",
      "Batch 00059: setting learning rate to 2.0622222222222223e-06.\n",
      "  59/3560 [..............................] - ETA: 13:33 - loss: 1.3606 - accuracy: 0.5625\n",
      "Batch 00060: setting learning rate to 2.0977777777777776e-06.\n",
      "  60/3560 [..............................] - ETA: 13:23 - loss: 1.3622 - accuracy: 0.5615\n",
      "Batch 00061: setting learning rate to 2.1333333333333334e-06.\n",
      "  61/3560 [..............................] - ETA: 13:12 - loss: 1.3657 - accuracy: 0.5599\n",
      "Batch 00062: setting learning rate to 2.1688888888888887e-06.\n",
      "  62/3560 [..............................] - ETA: 13:03 - loss: 1.3602 - accuracy: 0.5590\n",
      "Batch 00063: setting learning rate to 2.2044444444444444e-06.\n",
      "  63/3560 [..............................] - ETA: 12:53 - loss: 1.3582 - accuracy: 0.5585\n",
      "Batch 00064: setting learning rate to 2.2399999999999997e-06.\n",
      "  64/3560 [..............................] - ETA: 12:44 - loss: 1.3595 - accuracy: 0.5562\n",
      "Batch 00065: setting learning rate to 2.2755555555555555e-06.\n",
      "  65/3560 [..............................] - ETA: 12:35 - loss: 1.3611 - accuracy: 0.5534\n",
      "Batch 00066: setting learning rate to 2.311111111111111e-06.\n",
      "\n",
      "Batch 00067: setting learning rate to 2.3466666666666665e-06.\n",
      "  67/3560 [..............................] - ETA: 12:31 - loss: 1.3547 - accuracy: 0.5541\n",
      "Batch 00068: setting learning rate to 2.3822222222222222e-06.\n",
      "  68/3560 [..............................] - ETA: 12:23 - loss: 1.3514 - accuracy: 0.5524\n",
      "Batch 00069: setting learning rate to 2.4177777777777775e-06.\n",
      "  69/3560 [..............................] - ETA: 12:15 - loss: 1.3542 - accuracy: 0.5525\n",
      "Batch 00070: setting learning rate to 2.4533333333333333e-06.\n",
      "  70/3560 [..............................] - ETA: 12:07 - loss: 1.3515 - accuracy: 0.5527\n",
      "Batch 00071: setting learning rate to 2.4888888888888886e-06.\n",
      "  71/3560 [..............................] - ETA: 11:59 - loss: 1.3484 - accuracy: 0.5502\n",
      "Batch 00072: setting learning rate to 2.5244444444444443e-06.\n",
      "\n",
      "Batch 00073: setting learning rate to 2.56e-06.\n",
      "  73/3560 [..............................] - ETA: 11:44 - loss: 1.3473 - accuracy: 0.5475\n",
      "Batch 00074: setting learning rate to 2.5955555555555554e-06.\n",
      "  74/3560 [..............................] - ETA: 11:37 - loss: 1.3451 - accuracy: 0.5473\n",
      "Batch 00075: setting learning rate to 2.631111111111111e-06.\n",
      "  75/3560 [..............................] - ETA: 11:30 - loss: 1.3434 - accuracy: 0.5458\n",
      "Batch 00076: setting learning rate to 2.6666666666666664e-06.\n",
      "  76/3560 [..............................] - ETA: 11:23 - loss: 1.3392 - accuracy: 0.5477\n",
      "Batch 00077: setting learning rate to 2.702222222222222e-06.\n",
      "  77/3560 [..............................] - ETA: 11:18 - loss: 1.3373 - accuracy: 0.5475\n",
      "Batch 00078: setting learning rate to 2.737777777777778e-06.\n",
      "  78/3560 [..............................] - ETA: 11:18 - loss: 1.3386 - accuracy: 0.5477\n",
      "Batch 00079: setting learning rate to 2.773333333333333e-06.\n",
      "  79/3560 [..............................] - ETA: 11:15 - loss: 1.3395 - accuracy: 0.5475\n",
      "Batch 00080: setting learning rate to 2.808888888888889e-06.\n",
      "  80/3560 [..............................] - ETA: 11:08 - loss: 1.3377 - accuracy: 0.5469\n",
      "Batch 00081: setting learning rate to 2.8444444444444442e-06.\n",
      "  81/3560 [..............................] - ETA: 11:04 - loss: 1.3349 - accuracy: 0.5475\n",
      "Batch 00082: setting learning rate to 2.88e-06.\n",
      "  82/3560 [..............................] - ETA: 11:19 - loss: 1.3292 - accuracy: 0.5465\n",
      "Batch 00083: setting learning rate to 2.9155555555555553e-06.\n",
      "  83/3560 [..............................] - ETA: 11:13 - loss: 1.3327 - accuracy: 0.5437\n",
      "Batch 00084: setting learning rate to 2.951111111111111e-06.\n",
      "  84/3560 [..............................] - ETA: 11:07 - loss: 1.3300 - accuracy: 0.5443\n",
      "Batch 00085: setting learning rate to 2.9866666666666667e-06.\n",
      "  85/3560 [..............................] - ETA: 11:01 - loss: 1.3297 - accuracy: 0.5426\n",
      "Batch 00086: setting learning rate to 3.022222222222222e-06.\n",
      "  86/3560 [..............................] - ETA: 10:55 - loss: 1.3306 - accuracy: 0.5429\n",
      "Batch 00087: setting learning rate to 3.0577777777777778e-06.\n",
      "  87/3560 [..............................] - ETA: 10:56 - loss: 1.3241 - accuracy: 0.5445\n",
      "Batch 00088: setting learning rate to 3.093333333333333e-06.\n",
      "  88/3560 [..............................] - ETA: 10:50 - loss: 1.3229 - accuracy: 0.5455\n",
      "Batch 00089: setting learning rate to 3.128888888888889e-06.\n",
      "  89/3560 [..............................] - ETA: 10:45 - loss: 1.3242 - accuracy: 0.5456\n",
      "Batch 00090: setting learning rate to 3.164444444444444e-06.\n",
      "  90/3560 [..............................] - ETA: 10:39 - loss: 1.3183 - accuracy: 0.5483\n",
      "Batch 00091: setting learning rate to 3.2e-06.\n",
      "  91/3560 [..............................] - ETA: 10:34 - loss: 1.3171 - accuracy: 0.5484\n",
      "Batch 00092: setting learning rate to 3.2355555555555556e-06.\n",
      "\n",
      "Batch 00093: setting learning rate to 3.271111111111111e-06.\n",
      "  93/3560 [..............................] - ETA: 10:31 - loss: 1.3109 - accuracy: 0.5504\n",
      "Batch 00094: setting learning rate to 3.3066666666666666e-06.\n",
      "  94/3560 [..............................] - ETA: 10:26 - loss: 1.3159 - accuracy: 0.5505\n",
      "Batch 00095: setting learning rate to 3.342222222222222e-06.\n",
      "  95/3560 [..............................] - ETA: 10:32 - loss: 1.3142 - accuracy: 0.5520\n",
      "Batch 00096: setting learning rate to 3.3777777777777777e-06.\n",
      "  96/3560 [..............................] - ETA: 10:27 - loss: 1.3130 - accuracy: 0.5514\n",
      "Batch 00097: setting learning rate to 3.4133333333333334e-06.\n",
      "  97/3560 [..............................] - ETA: 10:22 - loss: 1.3104 - accuracy: 0.5515\n",
      "Batch 00098: setting learning rate to 3.4488888888888887e-06.\n",
      "  98/3560 [..............................] - ETA: 10:27 - loss: 1.3079 - accuracy: 0.5520\n",
      "Batch 00099: setting learning rate to 3.4844444444444444e-06.\n",
      "  99/3560 [..............................] - ETA: 10:23 - loss: 1.3032 - accuracy: 0.5521\n",
      "Batch 00100: setting learning rate to 3.5199999999999998e-06.\n",
      "\n",
      "Batch 00101: setting learning rate to 3.5555555555555555e-06.\n",
      " 101/3560 [..............................] - ETA: 10:16 - loss: 1.3000 - accuracy: 0.5511\n",
      "Batch 00102: setting learning rate to 3.591111111111111e-06.\n",
      "\n",
      "Batch 00103: setting learning rate to 3.6266666666666665e-06.\n",
      " 103/3560 [..............................] - ETA: 10:12 - loss: 1.2935 - accuracy: 0.5534\n",
      "Batch 00104: setting learning rate to 3.6622222222222223e-06.\n",
      " 104/3560 [..............................] - ETA: 10:08 - loss: 1.2929 - accuracy: 0.5541\n",
      "Batch 00105: setting learning rate to 3.6977777777777776e-06.\n",
      "\n",
      "Batch 00106: setting learning rate to 3.7333333333333333e-06.\n",
      " 106/3560 [..............................] - ETA: 9:59 - loss: 1.2931 - accuracy: 0.5537 \n",
      "Batch 00107: setting learning rate to 3.7688888888888886e-06.\n",
      " 107/3560 [..............................] - ETA: 9:56 - loss: 1.2921 - accuracy: 0.5526\n",
      "Batch 00108: setting learning rate to 3.8044444444444443e-06.\n",
      " 108/3560 [..............................] - ETA: 10:00 - loss: 1.2913 - accuracy: 0.5524\n",
      "Batch 00109: setting learning rate to 3.84e-06.\n",
      " 109/3560 [..............................] - ETA: 9:56 - loss: 1.2916 - accuracy: 0.5513 \n",
      "Batch 00110: setting learning rate to 3.875555555555556e-06.\n",
      "\n",
      "Batch 00111: setting learning rate to 3.911111111111111e-06.\n",
      " 111/3560 [..............................] - ETA: 9:47 - loss: 1.2896 - accuracy: 0.5490\n",
      "Batch 00112: setting learning rate to 3.9466666666666664e-06.\n",
      " 112/3560 [..............................] - ETA: 9:49 - loss: 1.2881 - accuracy: 0.5472\n",
      "Batch 00113: setting learning rate to 3.982222222222222e-06.\n",
      " 113/3560 [..............................] - ETA: 9:46 - loss: 1.2866 - accuracy: 0.5467\n",
      "Batch 00114: setting learning rate to 4.017777777777778e-06.\n",
      " 114/3560 [..............................] - ETA: 9:52 - loss: 1.2863 - accuracy: 0.5466\n",
      "Batch 00115: setting learning rate to 4.053333333333333e-06.\n",
      " 115/3560 [..............................] - ETA: 9:48 - loss: 1.2860 - accuracy: 0.5459\n",
      "Batch 00116: setting learning rate to 4.0888888888888885e-06.\n",
      " 116/3560 [..............................] - ETA: 9:45 - loss: 1.2839 - accuracy: 0.5458\n",
      "Batch 00117: setting learning rate to 4.124444444444445e-06.\n",
      "\n",
      "Batch 00118: setting learning rate to 4.16e-06.\n",
      " 118/3560 [..............................] - ETA: 9:44 - loss: 1.2772 - accuracy: 0.5469\n",
      "Batch 00119: setting learning rate to 4.195555555555555e-06.\n",
      " 119/3560 [>.............................] - ETA: 9:40 - loss: 1.2765 - accuracy: 0.5465\n",
      "Batch 00120: setting learning rate to 4.231111111111111e-06.\n",
      " 120/3560 [>.............................] - ETA: 9:37 - loss: 1.2751 - accuracy: 0.5466\n",
      "Batch 00121: setting learning rate to 4.266666666666667e-06.\n",
      " 121/3560 [>.............................] - ETA: 9:33 - loss: 1.2715 - accuracy: 0.5467\n",
      "Batch 00122: setting learning rate to 4.302222222222222e-06.\n",
      " 122/3560 [>.............................] - ETA: 9:30 - loss: 1.2723 - accuracy: 0.5459\n",
      "Batch 00123: setting learning rate to 4.337777777777777e-06.\n",
      "\n",
      "Batch 00124: setting learning rate to 4.3733333333333335e-06.\n",
      " 124/3560 [>.............................] - ETA: 9:30 - loss: 1.2659 - accuracy: 0.5474\n",
      "Batch 00125: setting learning rate to 4.408888888888889e-06.\n",
      " 125/3560 [>.............................] - ETA: 9:29 - loss: 1.2641 - accuracy: 0.5483\n",
      "Batch 00126: setting learning rate to 4.444444444444444e-06.\n",
      " 126/3560 [>.............................] - ETA: 9:26 - loss: 1.2609 - accuracy: 0.5499\n",
      "Batch 00127: setting learning rate to 4.4799999999999995e-06.\n",
      " 127/3560 [>.............................] - ETA: 9:23 - loss: 1.2614 - accuracy: 0.5504\n",
      "Batch 00128: setting learning rate to 4.515555555555556e-06.\n",
      "\n",
      "Batch 00129: setting learning rate to 4.551111111111111e-06.\n",
      " 129/3560 [>.............................] - ETA: 9:27 - loss: 1.2584 - accuracy: 0.5509\n",
      "Batch 00130: setting learning rate to 4.586666666666666e-06.\n",
      " 130/3560 [>.............................] - ETA: 9:27 - loss: 1.2557 - accuracy: 0.5519\n",
      "Batch 00131: setting learning rate to 4.622222222222222e-06.\n",
      " 131/3560 [>.............................] - ETA: 9:24 - loss: 1.2544 - accuracy: 0.5520\n",
      "Batch 00132: setting learning rate to 4.657777777777778e-06.\n",
      " 132/3560 [>.............................] - ETA: 9:21 - loss: 1.2537 - accuracy: 0.5523\n",
      "Batch 00133: setting learning rate to 4.693333333333333e-06.\n",
      " 133/3560 [>.............................] - ETA: 9:18 - loss: 1.2555 - accuracy: 0.5522\n",
      "Batch 00134: setting learning rate to 4.728888888888888e-06.\n",
      " 134/3560 [>.............................] - ETA: 9:20 - loss: 1.2539 - accuracy: 0.5536\n",
      "Batch 00135: setting learning rate to 4.7644444444444445e-06.\n",
      " 135/3560 [>.............................] - ETA: 9:17 - loss: 1.2534 - accuracy: 0.5544\n",
      "Batch 00136: setting learning rate to 4.8e-06.\n",
      "\n",
      "Batch 00137: setting learning rate to 4.835555555555555e-06.\n",
      " 137/3560 [>.............................] - ETA: 9:11 - loss: 1.2489 - accuracy: 0.5557\n",
      "Batch 00138: setting learning rate to 4.871111111111111e-06.\n",
      "\n",
      "Batch 00139: setting learning rate to 4.9066666666666666e-06.\n",
      " 139/3560 [>.............................] - ETA: 9:05 - loss: 1.2473 - accuracy: 0.5542\n",
      "Batch 00140: setting learning rate to 4.942222222222222e-06.\n",
      "\n",
      "Batch 00141: setting learning rate to 4.977777777777777e-06.\n",
      " 141/3560 [>.............................] - ETA: 9:04 - loss: 1.2450 - accuracy: 0.5545\n",
      "Batch 00142: setting learning rate to 5.013333333333333e-06.\n",
      " 142/3560 [>.............................] - ETA: 9:02 - loss: 1.2422 - accuracy: 0.5544\n",
      "Batch 00143: setting learning rate to 5.048888888888889e-06.\n",
      " 143/3560 [>.............................] - ETA: 9:05 - loss: 1.2417 - accuracy: 0.5551\n",
      "Batch 00144: setting learning rate to 5.084444444444444e-06.\n",
      "\n",
      "Batch 00145: setting learning rate to 5.12e-06.\n",
      " 145/3560 [>.............................] - ETA: 9:02 - loss: 1.2394 - accuracy: 0.5534\n",
      "Batch 00146: setting learning rate to 5.155555555555555e-06.\n",
      "\n",
      "Batch 00147: setting learning rate to 5.191111111111111e-06.\n",
      " 147/3560 [>.............................] - ETA: 9:01 - loss: 1.2363 - accuracy: 0.5540\n",
      "Batch 00148: setting learning rate to 5.226666666666666e-06.\n",
      " 148/3560 [>.............................] - ETA: 9:01 - loss: 1.2366 - accuracy: 0.5538\n",
      "Batch 00149: setting learning rate to 5.262222222222222e-06.\n",
      "\n",
      "Batch 00150: setting learning rate to 5.2977777777777775e-06.\n",
      " 150/3560 [>.............................] - ETA: 9:00 - loss: 1.2364 - accuracy: 0.5531\n",
      "Batch 00151: setting learning rate to 5.333333333333333e-06.\n",
      " 151/3560 [>.............................] - ETA: 8:58 - loss: 1.2364 - accuracy: 0.5522\n",
      "Batch 00152: setting learning rate to 5.368888888888889e-06.\n",
      " 152/3560 [>.............................] - ETA: 8:55 - loss: 1.2363 - accuracy: 0.5520\n",
      "Batch 00153: setting learning rate to 5.404444444444444e-06.\n",
      " 153/3560 [>.............................] - ETA: 8:53 - loss: 1.2376 - accuracy: 0.5513\n",
      "Batch 00154: setting learning rate to 5.44e-06.\n",
      " 154/3560 [>.............................] - ETA: 8:50 - loss: 1.2355 - accuracy: 0.5517\n",
      "Batch 00155: setting learning rate to 5.475555555555556e-06.\n",
      "\n",
      "Batch 00156: setting learning rate to 5.511111111111111e-06.\n",
      " 156/3560 [>.............................] - ETA: 8:48 - loss: 1.2358 - accuracy: 0.5519\n",
      "Batch 00157: setting learning rate to 5.546666666666666e-06.\n",
      " 157/3560 [>.............................] - ETA: 8:55 - loss: 1.2341 - accuracy: 0.5525\n",
      "Batch 00158: setting learning rate to 5.582222222222222e-06.\n",
      " 158/3560 [>.............................] - ETA: 8:52 - loss: 1.2317 - accuracy: 0.5526\n",
      "Batch 00159: setting learning rate to 5.617777777777778e-06.\n",
      " 159/3560 [>.............................] - ETA: 8:50 - loss: 1.2317 - accuracy: 0.5519\n",
      "Batch 00160: setting learning rate to 5.653333333333333e-06.\n",
      " 160/3560 [>.............................] - ETA: 8:48 - loss: 1.2313 - accuracy: 0.5521\n",
      "Batch 00161: setting learning rate to 5.6888888888888884e-06.\n",
      " 161/3560 [>.............................] - ETA: 8:46 - loss: 1.2315 - accuracy: 0.5516\n",
      "Batch 00162: setting learning rate to 5.724444444444445e-06.\n",
      " 162/3560 [>.............................] - ETA: 8:45 - loss: 1.2311 - accuracy: 0.5517\n",
      "Batch 00163: setting learning rate to 5.76e-06.\n",
      " 163/3560 [>.............................] - ETA: 8:43 - loss: 1.2299 - accuracy: 0.5516\n",
      "Batch 00164: setting learning rate to 5.795555555555555e-06.\n",
      " 164/3560 [>.............................] - ETA: 8:41 - loss: 1.2278 - accuracy: 0.5516\n",
      "Batch 00165: setting learning rate to 5.8311111111111105e-06.\n",
      "\n",
      "Batch 00166: setting learning rate to 5.866666666666667e-06.\n",
      " 166/3560 [>.............................] - ETA: 8:41 - loss: 1.2249 - accuracy: 0.5521\n",
      "Batch 00167: setting learning rate to 5.902222222222222e-06.\n",
      "\n",
      "Batch 00168: setting learning rate to 5.937777777777777e-06.\n",
      " 168/3560 [>.............................] - ETA: 8:42 - loss: 1.2215 - accuracy: 0.5526\n",
      "Batch 00169: setting learning rate to 5.9733333333333335e-06.\n",
      " 169/3560 [>.............................] - ETA: 8:40 - loss: 1.2210 - accuracy: 0.5523\n",
      "Batch 00170: setting learning rate to 6.008888888888889e-06.\n",
      " 170/3560 [>.............................] - ETA: 8:38 - loss: 1.2200 - accuracy: 0.5528\n",
      "Batch 00171: setting learning rate to 6.044444444444444e-06.\n",
      " 171/3560 [>.............................] - ETA: 8:36 - loss: 1.2176 - accuracy: 0.5535\n",
      "Batch 00172: setting learning rate to 6.079999999999999e-06.\n",
      " 172/3560 [>.............................] - ETA: 8:34 - loss: 1.2156 - accuracy: 0.5538\n",
      "Batch 00173: setting learning rate to 6.1155555555555555e-06.\n",
      " 173/3560 [>.............................] - ETA: 8:32 - loss: 1.2135 - accuracy: 0.5540\n",
      "Batch 00174: setting learning rate to 6.151111111111111e-06.\n",
      " 174/3560 [>.............................] - ETA: 8:32 - loss: 1.2137 - accuracy: 0.5539\n",
      "Batch 00175: setting learning rate to 6.186666666666666e-06.\n",
      " 175/3560 [>.............................] - ETA: 8:30 - loss: 1.2118 - accuracy: 0.5545\n",
      "Batch 00176: setting learning rate to 6.222222222222222e-06.\n",
      " 176/3560 [>.............................] - ETA: 8:29 - loss: 1.2099 - accuracy: 0.5556\n",
      "Batch 00177: setting learning rate to 6.257777777777778e-06.\n",
      " 177/3560 [>.............................] - ETA: 8:35 - loss: 1.2124 - accuracy: 0.5558\n",
      "Batch 00178: setting learning rate to 6.293333333333333e-06.\n",
      " 178/3560 [>.............................] - ETA: 8:33 - loss: 1.2110 - accuracy: 0.5569\n",
      "Batch 00179: setting learning rate to 6.328888888888888e-06.\n",
      " 179/3560 [>.............................] - ETA: 8:31 - loss: 1.2099 - accuracy: 0.5564\n",
      "Batch 00180: setting learning rate to 6.364444444444444e-06.\n",
      "\n",
      "Batch 00181: setting learning rate to 6.4e-06.\n",
      " 181/3560 [>.............................] - ETA: 8:28 - loss: 1.2100 - accuracy: 0.5561\n",
      "Batch 00182: setting learning rate to 6.435555555555555e-06.\n",
      " 182/3560 [>.............................] - ETA: 8:26 - loss: 1.2100 - accuracy: 0.5567\n",
      "Batch 00183: setting learning rate to 6.471111111111111e-06.\n",
      "\n",
      "Batch 00184: setting learning rate to 6.5066666666666665e-06.\n",
      " 184/3560 [>.............................] - ETA: 8:26 - loss: 1.2105 - accuracy: 0.5574\n",
      "Batch 00185: setting learning rate to 6.542222222222222e-06.\n",
      " 185/3560 [>.............................] - ETA: 8:24 - loss: 1.2132 - accuracy: 0.5581\n",
      "Batch 00186: setting learning rate to 6.577777777777777e-06.\n",
      "\n",
      "Batch 00187: setting learning rate to 6.613333333333333e-06.\n",
      " 187/3560 [>.............................] - ETA: 8:21 - loss: 1.2127 - accuracy: 0.5583\n",
      "Batch 00188: setting learning rate to 6.6488888888888886e-06.\n",
      " 188/3560 [>.............................] - ETA: 8:19 - loss: 1.2120 - accuracy: 0.5582\n",
      "Batch 00189: setting learning rate to 6.684444444444444e-06.\n",
      " 189/3560 [>.............................] - ETA: 8:17 - loss: 1.2130 - accuracy: 0.5577\n",
      "Batch 00190: setting learning rate to 6.72e-06.\n",
      " 190/3560 [>.............................] - ETA: 8:17 - loss: 1.2123 - accuracy: 0.5586\n",
      "Batch 00191: setting learning rate to 6.755555555555555e-06.\n",
      " 191/3560 [>.............................] - ETA: 8:17 - loss: 1.2106 - accuracy: 0.5586\n",
      "Batch 00192: setting learning rate to 6.791111111111111e-06.\n",
      " 192/3560 [>.............................] - ETA: 8:19 - loss: 1.2097 - accuracy: 0.5579\n",
      "Batch 00193: setting learning rate to 6.826666666666667e-06.\n",
      " 193/3560 [>.............................] - ETA: 8:19 - loss: 1.2103 - accuracy: 0.5573\n",
      "Batch 00194: setting learning rate to 6.862222222222222e-06.\n",
      " 194/3560 [>.............................] - ETA: 8:20 - loss: 1.2099 - accuracy: 0.5569\n",
      "Batch 00195: setting learning rate to 6.8977777777777774e-06.\n",
      " 195/3560 [>.............................] - ETA: 8:23 - loss: 1.2115 - accuracy: 0.5564\n",
      "Batch 00196: setting learning rate to 6.933333333333333e-06.\n",
      " 196/3560 [>.............................] - ETA: 8:21 - loss: 1.2114 - accuracy: 0.5563\n",
      "Batch 00197: setting learning rate to 6.968888888888889e-06.\n",
      " 197/3560 [>.............................] - ETA: 8:20 - loss: 1.2114 - accuracy: 0.5552\n",
      "Batch 00198: setting learning rate to 7.004444444444444e-06.\n",
      " 198/3560 [>.............................] - ETA: 8:18 - loss: 1.2113 - accuracy: 0.5549\n",
      "Batch 00199: setting learning rate to 7.0399999999999995e-06.\n",
      " 199/3560 [>.............................] - ETA: 8:16 - loss: 1.2126 - accuracy: 0.5551\n",
      "Batch 00200: setting learning rate to 7.075555555555556e-06.\n",
      " 200/3560 [>.............................] - ETA: 8:14 - loss: 1.2117 - accuracy: 0.5541\n",
      "Batch 00201: setting learning rate to 7.111111111111111e-06.\n",
      " 201/3560 [>.............................] - ETA: 8:17 - loss: 1.2098 - accuracy: 0.5546\n",
      "Batch 00202: setting learning rate to 7.146666666666666e-06.\n",
      " 202/3560 [>.............................] - ETA: 8:15 - loss: 1.2093 - accuracy: 0.5546\n",
      "Batch 00203: setting learning rate to 7.182222222222222e-06.\n",
      " 203/3560 [>.............................] - ETA: 8:14 - loss: 1.2083 - accuracy: 0.5546\n",
      "Batch 00204: setting learning rate to 7.217777777777778e-06.\n",
      " 204/3560 [>.............................] - ETA: 8:12 - loss: 1.2079 - accuracy: 0.5544\n",
      "Batch 00205: setting learning rate to 7.253333333333333e-06.\n",
      "\n",
      "Batch 00206: setting learning rate to 7.288888888888888e-06.\n",
      " 206/3560 [>.............................] - ETA: 8:08 - loss: 1.2097 - accuracy: 0.5543\n",
      "Batch 00207: setting learning rate to 7.3244444444444445e-06.\n",
      "\n",
      "Batch 00208: setting learning rate to 7.36e-06.\n",
      " 208/3560 [>.............................] - ETA: 8:12 - loss: 1.2083 - accuracy: 0.5541\n",
      "Batch 00209: setting learning rate to 7.395555555555555e-06.\n",
      " 209/3560 [>.............................] - ETA: 8:12 - loss: 1.2078 - accuracy: 0.5535\n",
      "Batch 00210: setting learning rate to 7.4311111111111105e-06.\n",
      " 210/3560 [>.............................] - ETA: 8:11 - loss: 1.2062 - accuracy: 0.5536\n",
      "Batch 00211: setting learning rate to 7.466666666666667e-06.\n",
      " 211/3560 [>.............................] - ETA: 8:09 - loss: 1.2068 - accuracy: 0.5538\n",
      "Batch 00212: setting learning rate to 7.502222222222222e-06.\n",
      "\n",
      "Batch 00213: setting learning rate to 7.537777777777777e-06.\n",
      " 213/3560 [>.............................] - ETA: 8:10 - loss: 1.2065 - accuracy: 0.5525\n",
      "Batch 00214: setting learning rate to 7.573333333333333e-06.\n",
      "\n",
      "Batch 00215: setting learning rate to 7.608888888888889e-06.\n",
      " 215/3560 [>.............................] - ETA: 8:07 - loss: 1.2051 - accuracy: 0.5526\n",
      "Batch 00216: setting learning rate to 7.644444444444445e-06.\n",
      " 216/3560 [>.............................] - ETA: 8:07 - loss: 1.2040 - accuracy: 0.5530\n",
      "Batch 00217: setting learning rate to 7.68e-06.\n",
      " 217/3560 [>.............................] - ETA: 8:05 - loss: 1.2054 - accuracy: 0.5527\n",
      "Batch 00218: setting learning rate to 7.715555555555555e-06.\n",
      " 218/3560 [>.............................] - ETA: 8:04 - loss: 1.2039 - accuracy: 0.5529\n",
      "Batch 00219: setting learning rate to 7.751111111111112e-06.\n",
      "\n",
      "Batch 00220: setting learning rate to 7.786666666666666e-06.\n",
      " 220/3560 [>.............................] - ETA: 8:01 - loss: 1.2035 - accuracy: 0.5526\n",
      "Batch 00221: setting learning rate to 7.822222222222222e-06.\n",
      " 221/3560 [>.............................] - ETA: 8:00 - loss: 1.2028 - accuracy: 0.5532\n",
      "Batch 00222: setting learning rate to 7.857777777777777e-06.\n",
      " 222/3560 [>.............................] - ETA: 7:58 - loss: 1.2014 - accuracy: 0.5535\n",
      "Batch 00223: setting learning rate to 7.893333333333333e-06.\n",
      " 223/3560 [>.............................] - ETA: 7:57 - loss: 1.2013 - accuracy: 0.5537\n",
      "Batch 00224: setting learning rate to 7.928888888888889e-06.\n",
      " 224/3560 [>.............................] - ETA: 8:01 - loss: 1.1994 - accuracy: 0.5540\n",
      "Batch 00225: setting learning rate to 7.964444444444443e-06.\n",
      " 225/3560 [>.............................] - ETA: 8:00 - loss: 1.1995 - accuracy: 0.5546\n",
      "Batch 00226: setting learning rate to 8e-06.\n",
      " 226/3560 [>.............................] - ETA: 8:02 - loss: 1.1998 - accuracy: 0.5546\n",
      "Batch 00227: setting learning rate to 8.035555555555556e-06.\n",
      " 227/3560 [>.............................] - ETA: 8:00 - loss: 1.1999 - accuracy: 0.5549\n",
      "Batch 00228: setting learning rate to 8.07111111111111e-06.\n",
      " 228/3560 [>.............................] - ETA: 8:00 - loss: 1.1993 - accuracy: 0.5548\n",
      "Batch 00229: setting learning rate to 8.106666666666666e-06.\n",
      "\n",
      "Batch 00230: setting learning rate to 8.142222222222223e-06.\n",
      " 230/3560 [>.............................] - ETA: 7:59 - loss: 1.1972 - accuracy: 0.5553\n",
      "Batch 00231: setting learning rate to 8.177777777777777e-06.\n",
      "\n",
      "Batch 00232: setting learning rate to 8.213333333333333e-06.\n",
      " 232/3560 [>.............................] - ETA: 7:56 - loss: 1.1956 - accuracy: 0.5560\n",
      "Batch 00233: setting learning rate to 8.24888888888889e-06.\n",
      " 233/3560 [>.............................] - ETA: 7:57 - loss: 1.1969 - accuracy: 0.5561\n",
      "Batch 00234: setting learning rate to 8.284444444444444e-06.\n",
      " 234/3560 [>.............................] - ETA: 7:56 - loss: 1.1966 - accuracy: 0.5568\n",
      "Batch 00235: setting learning rate to 8.32e-06.\n",
      "\n",
      "Batch 00236: setting learning rate to 8.355555555555554e-06.\n",
      " 236/3560 [>.............................] - ETA: 7:53 - loss: 1.1950 - accuracy: 0.5565\n",
      "Batch 00237: setting learning rate to 8.39111111111111e-06.\n",
      "\n",
      "Batch 00238: setting learning rate to 8.426666666666667e-06.\n",
      " 238/3560 [=>............................] - ETA: 7:52 - loss: 1.1928 - accuracy: 0.5569\n",
      "Batch 00239: setting learning rate to 8.462222222222221e-06.\n",
      "\n",
      "Batch 00240: setting learning rate to 8.497777777777777e-06.\n",
      " 240/3560 [=>............................] - ETA: 7:53 - loss: 1.1930 - accuracy: 0.5561\n",
      "Batch 00241: setting learning rate to 8.533333333333334e-06.\n",
      " 241/3560 [=>............................] - ETA: 7:56 - loss: 1.1927 - accuracy: 0.5563\n",
      "Batch 00242: setting learning rate to 8.568888888888888e-06.\n",
      " 242/3560 [=>............................] - ETA: 7:57 - loss: 1.1927 - accuracy: 0.5566\n",
      "Batch 00243: setting learning rate to 8.604444444444444e-06.\n",
      " 243/3560 [=>............................] - ETA: 7:56 - loss: 1.1914 - accuracy: 0.5567\n",
      "Batch 00244: setting learning rate to 8.64e-06.\n",
      " 244/3560 [=>............................] - ETA: 7:54 - loss: 1.1905 - accuracy: 0.5571\n",
      "Batch 00245: setting learning rate to 8.675555555555555e-06.\n",
      " 245/3560 [=>............................] - ETA: 7:53 - loss: 1.1903 - accuracy: 0.5569\n",
      "Batch 00246: setting learning rate to 8.711111111111111e-06.\n",
      " 246/3560 [=>............................] - ETA: 7:52 - loss: 1.1895 - accuracy: 0.5564\n",
      "Batch 00247: setting learning rate to 8.746666666666667e-06.\n",
      " 247/3560 [=>............................] - ETA: 7:50 - loss: 1.1893 - accuracy: 0.5566\n",
      "Batch 00248: setting learning rate to 8.782222222222222e-06.\n",
      " 248/3560 [=>............................] - ETA: 7:49 - loss: 1.1890 - accuracy: 0.5568\n",
      "Batch 00249: setting learning rate to 8.817777777777778e-06.\n",
      "\n",
      "Batch 00250: setting learning rate to 8.853333333333332e-06.\n",
      " 250/3560 [=>............................] - ETA: 7:46 - loss: 1.1869 - accuracy: 0.5573\n",
      "Batch 00251: setting learning rate to 8.888888888888888e-06.\n",
      " 251/3560 [=>............................] - ETA: 7:45 - loss: 1.1866 - accuracy: 0.5568\n",
      "Batch 00252: setting learning rate to 8.924444444444444e-06.\n",
      " 252/3560 [=>............................] - ETA: 7:44 - loss: 1.1859 - accuracy: 0.5570\n",
      "Batch 00253: setting learning rate to 8.959999999999999e-06.\n",
      " 253/3560 [=>............................] - ETA: 7:44 - loss: 1.1872 - accuracy: 0.5569\n",
      "Batch 00254: setting learning rate to 8.995555555555555e-06.\n",
      " 254/3560 [=>............................] - ETA: 7:42 - loss: 1.1872 - accuracy: 0.5563\n",
      "Batch 00255: setting learning rate to 9.031111111111111e-06.\n",
      " 255/3560 [=>............................] - ETA: 7:43 - loss: 1.1870 - accuracy: 0.5565\n",
      "Batch 00256: setting learning rate to 9.066666666666666e-06.\n",
      " 256/3560 [=>............................] - ETA: 7:43 - loss: 1.1871 - accuracy: 0.5559\n",
      "Batch 00257: setting learning rate to 9.102222222222222e-06.\n",
      " 257/3560 [=>............................] - ETA: 7:45 - loss: 1.1876 - accuracy: 0.5558\n",
      "Batch 00258: setting learning rate to 9.137777777777778e-06.\n",
      " 258/3560 [=>............................] - ETA: 7:45 - loss: 1.1877 - accuracy: 0.5561\n",
      "Batch 00259: setting learning rate to 9.173333333333332e-06.\n",
      " 259/3560 [=>............................] - ETA: 7:47 - loss: 1.1880 - accuracy: 0.5560\n",
      "Batch 00260: setting learning rate to 9.208888888888889e-06.\n",
      " 260/3560 [=>............................] - ETA: 7:46 - loss: 1.1879 - accuracy: 0.5564\n",
      "Batch 00261: setting learning rate to 9.244444444444445e-06.\n",
      " 261/3560 [=>............................] - ETA: 7:45 - loss: 1.1877 - accuracy: 0.5569\n",
      "Batch 00262: setting learning rate to 9.28e-06.\n",
      " 262/3560 [=>............................] - ETA: 7:44 - loss: 1.1870 - accuracy: 0.5570\n",
      "Batch 00263: setting learning rate to 9.315555555555555e-06.\n",
      " 263/3560 [=>............................] - ETA: 7:42 - loss: 1.1875 - accuracy: 0.5568\n",
      "Batch 00264: setting learning rate to 9.351111111111112e-06.\n",
      " 264/3560 [=>............................] - ETA: 7:41 - loss: 1.1874 - accuracy: 0.5565\n",
      "Batch 00265: setting learning rate to 9.386666666666666e-06.\n",
      " 265/3560 [=>............................] - ETA: 7:40 - loss: 1.1881 - accuracy: 0.5562\n",
      "Batch 00266: setting learning rate to 9.422222222222222e-06.\n",
      " 266/3560 [=>............................] - ETA: 7:39 - loss: 1.1879 - accuracy: 0.5558\n",
      "Batch 00267: setting learning rate to 9.457777777777777e-06.\n",
      " 267/3560 [=>............................] - ETA: 7:37 - loss: 1.1879 - accuracy: 0.5562\n",
      "Batch 00268: setting learning rate to 9.493333333333333e-06.\n",
      " 268/3560 [=>............................] - ETA: 7:39 - loss: 1.1868 - accuracy: 0.5562\n",
      "Batch 00269: setting learning rate to 9.528888888888889e-06.\n",
      " 269/3560 [=>............................] - ETA: 7:38 - loss: 1.1869 - accuracy: 0.5558\n",
      "Batch 00270: setting learning rate to 9.564444444444443e-06.\n",
      "\n",
      "Batch 00271: setting learning rate to 9.6e-06.\n",
      " 271/3560 [=>............................] - ETA: 7:36 - loss: 1.1863 - accuracy: 0.5558\n",
      "Batch 00272: setting learning rate to 9.635555555555556e-06.\n",
      " 272/3560 [=>............................] - ETA: 7:38 - loss: 1.1859 - accuracy: 0.5558"
     ]
    }
   ],
   "source": [
    "run_training(dropout = 0.5, lr_rate = 0.00001, architecture = 'mobilenet', batch = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "564DXmWLDMxa"
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "1qVdSNFxjg5-",
    "outputId": "c571704d-0a0e-4a65-a7f8-15fcb6b82127"
   },
   "outputs": [],
   "source": [
    "# run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-6Yocnrq2u4F"
   ],
   "name": "25_May.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
