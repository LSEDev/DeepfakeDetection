{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udRhv-d-2i8l"
   },
   "source": [
    "# Train preliminary models - Xception, ResNet, EfficentNet etc.\n",
    "       -- built for FF+ dataset with file structure as required by Keras' flow_from_directory method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "t7E1CjC9fqhq",
    "outputId": "abdef6eb-823c-4fb1-ad28-7521345d04fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 12 08:17:15 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P0    39W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# See available GPU RAM \n",
    "!nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !watch -n0.1 nvidia-smi # better way to see GPU utilisation\n",
    "# !htop # cpu threads and if they're all working\n",
    "# !pip3 install --no-cache-dir -I tensorflow==2.2 #Â use if no gpu is attached so code will run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Llx-HRnYiWQU",
    "outputId": "6e6a3556-fbb1-4972-b046-8586183f768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.1-dlenv_tfe\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "3DRA3QPDgLLR",
    "outputId": "ed171b89-378d-469d-a254-c291be71af4d"
   },
   "outputs": [],
   "source": [
    "# Required for EfficientNet\n",
    "# !pip install git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtO5vELz8i3-"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ7mWThq32JA"
   },
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture):\n",
    "    '''Builds a specified network with the selected dropout after the last dense layer.\n",
    "\n",
    "    Architectures that can be selected are:\n",
    "    vgg, xception, resnet50, mobilenet, efficientnet, densenet\n",
    "    \n",
    "    Optimiser is Adam, with a provided learning rate (lr_rate) and fixed\n",
    "    decay 1e-6, loss is traditionally categorical_crossentropy.'''\n",
    "\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "    if architecture=='xception':\n",
    "        from tensorflow.keras.applications.xception import Xception\n",
    "        conv_base = Xception(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture=='vgg':\n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='resnet50':\n",
    "        from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "        conv_base = ResNet50(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='mobilenet':\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture== 'efficientnet':\n",
    "        # EfficientNetB7 has the highest top-1 accuracy on imagenet\n",
    "        # among EfficientNextB{0:7}\n",
    "        from efficientnet.tfkeras import EfficientNetB0\n",
    "        conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "        \n",
    "    elif architecture== 'densenet':\n",
    "        from tensorflow.keras.applications.densenet import DenseNet201\n",
    "        conv_base = DenseNet201(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture not in ['vgg', 'xception', 'resnet50',\n",
    "                              'mobilenet', 'efficientnet', 'densenet']:\n",
    "        return \"An unknown network is specified\"\n",
    "    \n",
    "\n",
    "    outputconv_base = conv_base.output\n",
    "    t_flat = Flatten()(outputconv_base)\n",
    "    t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
    "    t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "    t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
    "    t_do = Dropout(dropout)(t_dense3)\n",
    "    predictions = Dense(2, activation= 'softmax')(t_do)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=predictions, name = 'model')\n",
    "\n",
    "    conv_base.trainable = False # freeze the convolutional base\n",
    "    \n",
    "    # # Code below trains all layers without using any pretrained weights\n",
    "    #for layer in conv_base.layers:\n",
    "    #  layer.trainable = True\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate= lr_rate, decay=1e-6)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_rate)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen_train = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "#             width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "#             height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            brightness_range=[0.6, 1.4],\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "    \n",
    "    datagen_test = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen_train.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    val_data = datagen_test.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(224,224), batch_size = batch)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary train time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights\n",
    "\n",
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed. '''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/mobilenet_new\"\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # if there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # pick out accuracies out of file names\n",
    "            acc = [el[len(el)-10 : len(el)-5] for el in all_weights]\n",
    "            # get index of the first maximum accuracy\n",
    "            optimal_index = acc.index(max(acc))\n",
    "            # get the name of the file with optimal weights, load corresponding weights\n",
    "            optimal_weights = all_weights[optimal_index]\n",
    "            print(\"Loading\", path_to_weights + '/' + optimal_weights)\n",
    "            model.load_weights(path_to_weights + '/' + optimal_weights)\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture):\n",
    "    '''Takes the latest saved weights and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/mobilenet_new_model.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/mobilenet_new'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/mobilenet_new')\n",
    "\n",
    "    # Save weights - below saves every epoch where there is improvement\n",
    "    # filepath=\"../all_faces_bucket/trained_models/weights/\" + architecture + \"/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/mobilenet_new/highest_val_acc.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/mobilenet_new.csv',\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    # Set learning rate config \n",
    "    sample_count = 60000 # number of training samples\n",
    "    epochs = 50 # total epochs - affects total steps (and hence speed of decay)\n",
    "    warmup_epoch = 3 # number of warmup epochs\n",
    "    batch_size = train_data.batch_size\n",
    "    learning_rate_base = 0.0002\n",
    "    total_steps = int(epochs * sample_count / batch_size)\n",
    "    warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "    \n",
    "    warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warmup_steps,\n",
    "                                        hold_base_rate_steps=2,\n",
    "                                        verbose=0)\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs, shuffle=True,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=[plot_losses, checkpoint, csv_logger, es, warm_up_lr],\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIUZirJoxsdx"
   },
   "source": [
    "## Unifying Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYFsNbZMqYTv"
   },
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture)\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in8HHH594qtA"
   },
   "source": [
    "## Train Various Model Architectures\n",
    "Note: Make sure CPUs have enough memory for each batch eg. 1 core with 3.75GB RAM cant take batches larger than 32.  \n",
    "8 CPUs with 30GB RAM typically works well for batches of 256.\n",
    "\n",
    "In this model: 2 cores/8GB and T4 used. \n",
    "\n",
    "Also note that while multiprocessing speeds up training, it interacts badly with Tensorflow and leads to deadlocks. To be on the safe side, set use_multiprocessing to False when training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz0ilB_F8F4g"
   },
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "colab_type": "code",
    "id": "_GZwsiNC7rkK",
    "outputId": "7b0348f4-e292-4cb7-af8f-49af0eda2638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Found 113928 images belonging to 2 classes.\n",
      "Found 21291 images belonging to 2 classes.\n",
      "Ensure class weights function corresponds to these class indices: {'authentic': 0, 'fake': 1}\n",
      "There are no weights stored. Training model from scratch:\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 445 steps, validate for 83 steps\n",
      "Epoch 1/50\n",
      "\n",
      "Batch 00001: setting learning rate to 0.0.\n",
      "  1/445 [..............................] - ETA: 2:44:22 - loss: 2.7090 - accuracy: 0.6641\n",
      "Batch 00002: setting learning rate to 2.844950213371266e-07.\n",
      "  2/445 [..............................] - ETA: 1:22:35 - loss: 5.6195 - accuracy: 0.4648\n",
      "Batch 00003: setting learning rate to 5.689900426742532e-07.\n",
      "  3/445 [..............................] - ETA: 55:19 - loss: 4.6535 - accuracy: 0.4323  \n",
      "Batch 00004: setting learning rate to 8.534850640113799e-07.\n",
      "  4/445 [..............................] - ETA: 41:39 - loss: 4.5969 - accuracy: 0.4980\n",
      "Batch 00005: setting learning rate to 1.1379800853485064e-06.\n",
      "  5/445 [..............................] - ETA: 33:29 - loss: 4.2996 - accuracy: 0.5195\n",
      "Batch 00006: setting learning rate to 1.422475106685633e-06.\n",
      "  6/445 [..............................] - ETA: 28:01 - loss: 3.9907 - accuracy: 0.5046\n",
      "Batch 00007: setting learning rate to 1.7069701280227598e-06.\n",
      "  7/445 [..............................] - ETA: 24:07 - loss: 3.7536 - accuracy: 0.4905\n",
      "Batch 00008: setting learning rate to 1.991465149359886e-06.\n",
      "  8/445 [..............................] - ETA: 21:10 - loss: 3.5417 - accuracy: 0.4961\n",
      "Batch 00009: setting learning rate to 2.275960170697013e-06.\n",
      "  9/445 [..............................] - ETA: 18:54 - loss: 3.3918 - accuracy: 0.5148\n",
      "Batch 00010: setting learning rate to 2.5604551920341396e-06.\n",
      " 10/445 [..............................] - ETA: 17:04 - loss: 3.2293 - accuracy: 0.5320\n",
      "Batch 00011: setting learning rate to 2.844950213371266e-06.\n",
      " 11/445 [..............................] - ETA: 15:34 - loss: 3.1279 - accuracy: 0.5380\n",
      "Batch 00012: setting learning rate to 3.1294452347083927e-06.\n",
      " 12/445 [..............................] - ETA: 14:19 - loss: 3.0615 - accuracy: 0.5329\n",
      "Batch 00013: setting learning rate to 3.4139402560455195e-06.\n",
      " 13/445 [..............................] - ETA: 13:16 - loss: 2.9577 - accuracy: 0.5312\n",
      "Batch 00014: setting learning rate to 3.698435277382646e-06.\n",
      " 14/445 [..............................] - ETA: 12:22 - loss: 2.8986 - accuracy: 0.5273\n",
      "Batch 00015: setting learning rate to 3.982930298719772e-06.\n",
      " 15/445 [>.............................] - ETA: 11:35 - loss: 2.8301 - accuracy: 0.5289\n",
      "Batch 00016: setting learning rate to 4.267425320056899e-06.\n",
      " 16/445 [>.............................] - ETA: 10:52 - loss: 2.7709 - accuracy: 0.5281\n",
      "Batch 00017: setting learning rate to 4.551920341394026e-06.\n",
      " 17/445 [>.............................] - ETA: 13:56 - loss: 2.7103 - accuracy: 0.5278\n",
      "Batch 00018: setting learning rate to 4.8364153627311525e-06.\n",
      " 18/445 [>.............................] - ETA: 13:33 - loss: 2.6565 - accuracy: 0.5224\n",
      "Batch 00019: setting learning rate to 5.120910384068279e-06.\n",
      " 19/445 [>.............................] - ETA: 12:54 - loss: 2.5911 - accuracy: 0.5245\n",
      "Batch 00020: setting learning rate to 5.405405405405405e-06.\n",
      " 20/445 [>.............................] - ETA: 12:38 - loss: 2.5394 - accuracy: 0.5297\n",
      "Batch 00021: setting learning rate to 5.689900426742532e-06.\n",
      " 21/445 [>.............................] - ETA: 12:03 - loss: 2.4876 - accuracy: 0.5342\n",
      "Batch 00022: setting learning rate to 5.974395448079659e-06.\n",
      " 22/445 [>.............................] - ETA: 11:36 - loss: 2.4420 - accuracy: 0.5350\n",
      "Batch 00023: setting learning rate to 6.2588904694167855e-06.\n",
      " 23/445 [>.............................] - ETA: 11:11 - loss: 2.3957 - accuracy: 0.5311\n",
      "Batch 00024: setting learning rate to 6.543385490753912e-06.\n",
      " 24/445 [>.............................] - ETA: 10:44 - loss: 2.3458 - accuracy: 0.5321\n",
      "Batch 00025: setting learning rate to 6.827880512091039e-06.\n",
      " 25/445 [>.............................] - ETA: 10:19 - loss: 2.3052 - accuracy: 0.5334\n",
      "Batch 00026: setting learning rate to 7.112375533428165e-06.\n",
      " 26/445 [>.............................] - ETA: 9:56 - loss: 2.2608 - accuracy: 0.5397 \n",
      "Batch 00027: setting learning rate to 7.396870554765292e-06.\n",
      " 27/445 [>.............................] - ETA: 9:39 - loss: 2.2291 - accuracy: 0.5437\n",
      "Batch 00028: setting learning rate to 7.681365576102418e-06.\n",
      " 28/445 [>.............................] - ETA: 9:19 - loss: 2.1989 - accuracy: 0.5460\n",
      "Batch 00029: setting learning rate to 7.965860597439544e-06.\n",
      " 29/445 [>.............................] - ETA: 9:01 - loss: 2.1712 - accuracy: 0.5426\n",
      "Batch 00030: setting learning rate to 8.250355618776672e-06.\n",
      " 30/445 [=>............................] - ETA: 8:44 - loss: 2.1453 - accuracy: 0.5389\n",
      "Batch 00031: setting learning rate to 8.534850640113798e-06.\n",
      " 31/445 [=>............................] - ETA: 8:27 - loss: 2.1180 - accuracy: 0.5362\n",
      "Batch 00032: setting learning rate to 8.819345661450926e-06.\n",
      " 32/445 [=>............................] - ETA: 8:12 - loss: 2.0884 - accuracy: 0.5349\n",
      "Batch 00033: setting learning rate to 9.103840682788051e-06.\n"
     ]
    }
   ],
   "source": [
    "run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "564DXmWLDMxa"
   },
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "1qVdSNFxjg5-",
    "outputId": "c571704d-0a0e-4a65-a7f8-15fcb6b82127"
   },
   "outputs": [],
   "source": [
    "run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'efficientnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet', batch = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'densenet')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-6Yocnrq2u4F"
   ],
   "name": "25_May.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
