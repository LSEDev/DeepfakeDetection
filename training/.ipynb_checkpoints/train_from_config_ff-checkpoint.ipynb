{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Train-from-specified-config-files\" data-toc-modified-id=\"Train-from-specified-config-files-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Train from specified config files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-model\" data-toc-modified-id=\"Build-model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Build model</a></span></li><li><span><a href=\"#Augment-data\" data-toc-modified-id=\"Augment-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Augment data</a></span></li><li><span><a href=\"#Auxiliary-train-time-functions\" data-toc-modified-id=\"Auxiliary-train-time-functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Auxiliary train time functions</a></span></li><li><span><a href=\"#Training-functions\" data-toc-modified-id=\"Training-functions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Training functions</a></span></li><li><span><a href=\"#Unifying-Function\" data-toc-modified-id=\"Unifying-Function-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Unifying Function</a></span></li><li><span><a href=\"#Run-training\" data-toc-modified-id=\"Run-training-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Run training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udRhv-d-2i8l"
   },
   "source": [
    "# Train from specified config files\n",
    "       -- built for FF+ dataset with file structure as required by Keras' flow_from_directory method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "t7E1CjC9fqhq",
    "outputId": "abdef6eb-823c-4fb1-ad28-7521345d04fc"
   },
   "outputs": [],
   "source": [
    "# See available GPU RAM \n",
    "# !nvidia-smi # can also be run from linux shell while GPU is training\n",
    "# !nvidia-smi dmon # this will stream memory utilisation\n",
    "# !watch -n0.1 nvidia-smi # better way to see GPU utilisation\n",
    "# !htop # cpu threads and if they're all working\n",
    "# !pip3 install --no-cache-dir -I tensorflow==2.2 #Â use if no gpu is attached so code will run \n",
    "# !sudo kill -9 PID # clear GPU memory where 9 is PID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augmentation installations\n",
    "# !pip install CMake\n",
    "# !pip install dlib\n",
    "# !pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Llx-HRnYiWQU",
    "outputId": "6e6a3556-fbb1-4972-b046-8586183f768a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.1-dlenv_tfe\n"
     ]
    }
   ],
   "source": [
    "# This cell has the latest set up for AI Platform\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from numpy import expand_dims\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import *\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/augmentations')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/hyperparameters')\n",
    "sys.path.insert(1, '/home/jupyter/DeepFake-2019-20/visualisations')\n",
    "import VisualisationTools as plotting\n",
    "import hyper_utils as hp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(100)\n",
    "plot_losses = plotting.PlotLearning()\n",
    "os.chdir('/home/jupyter/DeepFake-2019-20')\n",
    "\n",
    "# # Augmentation libraries\n",
    "# import face_recognition\n",
    "# import cutout_augmentation as ca\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'efficientnet',\n",
       " 'epochs': 140,\n",
       " 'batch_size': 256,\n",
       " 'learning_rate_type': 'cosine_decay',\n",
       " 'learning_rate': 0.0006547675102413338,\n",
       " 'patience': 6,\n",
       " 'weight_initialisation': 'imagenet',\n",
       " 'optimiser': 'adam',\n",
       " 'momentum': 0.1,\n",
       " 'nesterov': True,\n",
       " 'label_smoothing': 0.04,\n",
       " 'dropout': 0.0,\n",
       " 'target_size': 224,\n",
       " 'class_weights': True,\n",
       " 'warmup_epochs': 7}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify config file\n",
    "config_number=0\n",
    "config_file='config{}'.format(config_number)\n",
    "def obtain_configs(number):\n",
    "    '''Extracts hyperparameters from config file given the config file number.'''\n",
    "    with open('./configs/config{}.json'.format(number)) as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = obtain_configs(config_number)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtO5vELz8i3-"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ7mWThq32JA"
   },
   "outputs": [],
   "source": [
    "def build_model(dropout, lr_rate, architecture, frozen_base=True):\n",
    "    '''Builds a specified network with the selected dropout after the last dense layer.\n",
    "\n",
    "    Architectures that can be selected are:\n",
    "    vgg, xception, resnet50, mobilenet, efficientnet, densenet\n",
    "    \n",
    "    Optimiser is Adam, with a provided learning rate (lr_rate) and fixed\n",
    "    decay 1e-6, loss is traditionally categorical_crossentropy.'''\n",
    "\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "    if architecture=='xception':\n",
    "        from tensorflow.keras.applications.xception import Xception\n",
    "        conv_base = Xception(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture=='vgg':\n",
    "        from tensorflow.keras.applications.vgg16 import VGG16\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='resnet50':\n",
    "        from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "        conv_base = ResNet50(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "      \n",
    "    elif architecture=='mobilenet':\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        conv_base = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture== 'efficientnet':\n",
    "        # EfficientNetB7 has the highest top-1 accuracy on imagenet\n",
    "        # among EfficientNextB{0:7}\n",
    "        !pip install git+https://github.com/qubvel/efficientnet\n",
    "        from efficientnet.tfkeras import EfficientNetB0\n",
    "        conv_base = EfficientNetB0(weights='noisy-student', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "        \n",
    "    elif architecture== 'densenet':\n",
    "        from tensorflow.keras.applications.densenet import DenseNet201\n",
    "        conv_base = DenseNet201(weights='imagenet', include_top=False,\n",
    "                        input_shape=(224,224,3))\n",
    "\n",
    "    elif architecture not in ['vgg', 'xception', 'resnet50',\n",
    "                              'mobilenet', 'efficientnet', 'densenet']:\n",
    "        return \"An unknown network is specified\"\n",
    "    \n",
    "\n",
    "    outputconv_base = conv_base.output\n",
    "    t_flat = Flatten()(outputconv_base)\n",
    "    t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
    "    t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "    t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
    "    t_do = Dropout(dropout)(t_dense3)\n",
    "    predictions = Dense(2, activation= 'softmax')(t_do)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=predictions, name = 'model')\n",
    "    \n",
    "    if frozen_base:\n",
    "        conv_base.trainable = False # freeze the convolutional base\n",
    "        \n",
    "    else: \n",
    "        conv_base.trainable = True\n",
    "\n",
    "\n",
    "    if params['optimiser']=='adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr_rate)\n",
    "    \n",
    "    elif params['optimiser']=='sgd' and params['nesterov']=='True':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr_rate, momentum=params['momentum'], nesterov=True)\n",
    "        \n",
    "    elif params['optimiser']=='sgd' and params['nesterov']=='False':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr_rate, momentum=params['momentum'], nesterov=False)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=params['label_smoothing']),\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(directory, batch):\n",
    "    '''Prepares train-time augmentation using given training and validations data)\n",
    "    \n",
    "    Returns train_data, val_data'''\n",
    "\n",
    "    datagen_train = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=True,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=True,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "#             width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "#             height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            brightness_range=[0.6, 1.4],\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            )\n",
    "    \n",
    "    datagen_test = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "    # Classes give the folders storing the two different categories\n",
    "    train_data = datagen_train.flow_from_directory(directory + '/train',\n",
    "                                             target_size=(params['target_size'], params['target_size']), batch_size = batch)\n",
    "    \n",
    "    val_data = datagen_test.flow_from_directory(directory + '/validation',\n",
    "                                             target_size=(params['target_size'], params['target_size']), batch_size = batch)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell which previews data\n",
    "t,v = augment_data('../all_faces_disk/home/jupyter/forensics_split', 8)\n",
    "b=8\n",
    "c=0\n",
    "while True:\n",
    "    fig, axe = plt.subplots(1,b, figsize=(28,10))\n",
    "    for j,i in enumerate(t.next()[0]):\n",
    "        axe[j].imshow(i, interpolation=\"nearest\")\n",
    "        axe[j].set_xticks([])\n",
    "        axe[j].set_yticks([])\n",
    "    plt.show()\n",
    "    c+=1\n",
    "    if c==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary train time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(train_data):\n",
    "    '''Calculates class weights that weight the data based on the imbalance.\n",
    "    Allows for better analysis in the case of imbalanced data - has no effect\n",
    "    if data is balanced since the weights are then equal for each class.\n",
    "    Use the generator obtained from the flow_from_directory method to obtain\n",
    "    the class_weights.\n",
    "    \n",
    "    Input:\n",
    "    train_data: the generator obtained during augmentation\n",
    "    \n",
    "    Returns a dictionary with class weights, required format for training'''\n",
    "    \n",
    "    # Calculate class weights which are required to fully balance the classes\n",
    "    # Compares frequencies of appearence for each distinct label\n",
    "    \n",
    "    # The line of code below can be used on a generator to find the index labels\n",
    "    print('Ensure class weights function corresponds to these class indices:',\n",
    "          train_data.class_indices)\n",
    "    \n",
    "    counter = Counter(train_data.classes)                          \n",
    "    max_val = float(max(counter.values()))       \n",
    "    class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "    return class_weights\n",
    "\n",
    "def load_model_weights(model, architecture):\n",
    "    '''An alternative to training if there are already some generated weights\n",
    "    \n",
    "    Takes a built model (and its architecture type) and loads the weights\n",
    "    with the highest validation accuracy.\n",
    "    \n",
    "    If there are no saved weights, a message is printed.'''\n",
    "\n",
    "    path_to_weights = \"../all_faces_bucket/trained_models/weights/{}\".format(config_file)\n",
    "    # get all the weights file names in a list\n",
    "    if os.path.exists(path_to_weights):\n",
    "        all_weights = sorted(os.listdir(path_to_weights + '/'))\n",
    "    # If there is at least one file\n",
    "        if len(all_weights) >= 1:\n",
    "            # Use weights from highest val acc\n",
    "            model.load_weights(path_to_weights + '/' + 'highest_val_acc.hdf5')\n",
    "            \n",
    "        else: # otherwise warn that no weights were loaded\n",
    "            print(\"There are no weights stored. Training model from scratch:\")   \n",
    "    \n",
    "    else: # otherwise warn that no weights were loaded\n",
    "        print(\"There are no weights stored. Training model from scratch:\")   \n",
    "        \n",
    "def save_model_from_best_weights(dropout, lr_rate, architecture, frozen_base):\n",
    "    '''Takes the weights with the highest val accuracy and saves the corresponding model.'''\n",
    "    model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    load_model_weights(model, architecture)\n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate):\n",
    "    '''Trains a provided model.\n",
    "    Takes 6 arguments:\n",
    "    \n",
    "    1. model: a built model with an architecture specified in the build function\n",
    "    2. train_data: augmented data obtained from the augment_data function\n",
    "    3. val_data: validation data obtained from the augment_data function\n",
    "    4. epochs -- number of epochs\n",
    "    5. class weights -- a dictionary with weights (equal for balanced data so\n",
    "    no negative impact)\n",
    "    6. architecture: can choose vgg, xception, resnet50, mobilenet or efficientnet\n",
    "    7. lr_rate: initial learning rate\n",
    "    '''\n",
    "    \n",
    "    # Make a trained_models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models')\n",
    "    \n",
    "    # Make a weights folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights')\n",
    "        \n",
    "    # Make a weights folder for the architecture if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/weights/{}'.format(config_file)):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/weights/{}'.format(config_file))\n",
    "\n",
    "    # Below saves on file - the weights with the highest validation accuracy\n",
    "    filepath=\"../all_faces_bucket/trained_models/weights/{}/highest_val_acc.hdf5\".format(config_file)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # Make a folder to store training accuracies if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/training_accuracies'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/training_accuracies')\n",
    "    \n",
    "    # Callback to save training accuracies after each epoch\n",
    "    csv_logger = CSVLogger('../all_faces_bucket/trained_models/training_accuracies/{}.csv'.format(config_file),\n",
    "                           separator=',', append=True)\n",
    "    \n",
    "    # Stop after 3 epochs if val_accuracy doesn't improve\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=params['patience'])\n",
    "                          \n",
    "    # Load previous weights from training if there are any\n",
    "    load_model_weights(model, architecture)\n",
    "    \n",
    "    if params['learning_rate_type']=='cosine_decay':\n",
    "        # Set learning rate config \n",
    "        sample_count = train_data.n # number of training samples\n",
    "        epochs = epochs # total epochs - affects total steps (and hence speed of decay)\n",
    "        warmup_epoch = params['warmup_epochs'] # number of warmup epochs\n",
    "        batch_size = train_data.batch_size\n",
    "        learning_rate_base = lr_rate\n",
    "        total_steps = int(epochs * sample_count / batch_size)\n",
    "\n",
    "        warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "\n",
    "        warm_up_lr = hp.WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                            total_steps=total_steps,\n",
    "                                            warmup_learning_rate=0.0,\n",
    "                                            warmup_steps=warmup_steps,\n",
    "                                            hold_base_rate_steps=2,\n",
    "                                            verbose=0)\n",
    "\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es, warm_up_lr]\n",
    "        \n",
    "    elif params['learning_rate_type']=='constant':\n",
    "        cb = [plot_losses, checkpoint, csv_logger, es]\n",
    "        \n",
    "    elif params['learning_rate_type']=='increasing':\n",
    "        pass\n",
    "\n",
    "    history = model.fit(train_data, epochs=epochs,\n",
    "              steps_per_epoch = train_data.n//train_data.batch_size,\n",
    "              validation_data = val_data, \n",
    "              validation_steps = val_data.n//val_data.batch_size,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=cb,\n",
    "              initial_epoch=0,                    # start training epoch - useful if continuing training\n",
    "              verbose=1,\n",
    "              max_queue_size=100,                # maximum size for the generator queue\n",
    "              workers=16,                        # maximum number of processes to spin up when using process-based threading\n",
    "              use_multiprocessing=False)\n",
    "    \n",
    "    # Make a saved models folder if it doesn't exist\n",
    "    if not os.path.exists('../all_faces_bucket/trained_models/saved_models'):\n",
    "        os.makedirs('../all_faces_bucket/trained_models/saved_models')\n",
    "        \n",
    "    model.save_weights('../all_faces_bucket/trained_models/weights/{}/lastepoch.hdf5'.format(config_file)) \n",
    "    model.save('../all_faces_bucket/trained_models/saved_models/{}.h5'.format(config_file))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIUZirJoxsdx"
   },
   "source": [
    "## Unifying Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYFsNbZMqYTv"
   },
   "outputs": [],
   "source": [
    "def run_training(dropout = 0.5, lr_rate = 0.0001, architecture = 'vgg', \n",
    "                 batch = 32, epochs = 50, frozen_base=True):\n",
    "\n",
    "    '''Builds a model based on the specified architecture, augments training\n",
    "    data (reserving a fraction for validation), then computes class weights to\n",
    "    balance data and trains the model.\n",
    "    \n",
    "    Inputs:\n",
    "    1. dropout  -- for the model\n",
    "    2. lr_rate\n",
    "    3. architecture -- a choice of vgg, resnet50, mobilenet, xception and efficientnet\n",
    "    4. batch -- batch size\n",
    "    5. epochs -- number of epochs\n",
    "    '''\n",
    "\n",
    "    # Build a model, augment data, get class_weights and train the model\n",
    "    # Strategy scope allows us to leverage multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope(): # Allows for parallel GPUs\n",
    "        model = build_model(dropout, lr_rate, architecture, frozen_base)\n",
    "    train_data, val_data = augment_data('../all_faces_disk/home/jupyter/forensics_split', batch)\n",
    "    class_weights = calculate_class_weights(train_data)\n",
    "    if params['class_weights']=='True':\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, class_weights, architecture, lr_rate)\n",
    "    else:\n",
    "        trained_model = train_model(model, train_data, val_data, epochs, None, architecture, lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in8HHH594qtA"
   },
   "source": [
    "## Run training \n",
    "Note: Make sure CPUs have enough memory for each batch eg. 1 core with 3.75GB RAM cant take batches larger than 32.  \n",
    "8 CPUs with 30GB RAM typically works well for batches of 256.\n",
    "\n",
    "In this model: 4 cores/8GB and T4 used - took approx 8 hours to train top dense layers and fine tune. \n",
    "\n",
    "Also note that while multiprocessing speeds up training, it interacts badly with Tensorflow and leads to deadlocks. To be on the safe side, set use_multiprocessing to False when training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "colab_type": "code",
    "id": "_GZwsiNC7rkK",
    "outputId": "7b0348f4-e292-4cb7-af8f-49af0eda2638"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train dense layers with base frozen\n",
    "run_training(dropout = params['dropout'], lr_rate = params['learning_rate'], \n",
    "             architecture = params['architecture'], batch = params['batch_size'],\n",
    "             epochs = params['epochs'], frozen_base=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all weights to fine-tine model\n",
    "# NBNB this will use same learming rate type as specified in params unless you change it \n",
    "run_training(dropout = params['dropout'], lr_rate = params['learning_rate']/10, \n",
    "             architecture = params['architecture'], batch = params['batch_size'],\n",
    "             epochs = params['epochs'], frozen_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to bucket from highest val acc weights\n",
    "# save_model_from_best_weights(dropout = 0.5, lr_rate = 0.0002, architecture = 'mobilenet', frozen_base=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-6Yocnrq2u4F"
   ],
   "name": "25_May.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
